[2022-12-16T13:11:28.848+0000] {processor.py:154} INFO - Started process (PID=207) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:11:28.863+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:11:28.866+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:11:28.866+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:11:30.474+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:11:31.394+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:11:31.393+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:11:32.138+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:11:32.137+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:11:32.555+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.731 seconds
[2022-12-16T13:11:42.847+0000] {processor.py:154} INFO - Started process (PID=218) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:11:42.850+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:11:42.865+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:11:42.864+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:11:42.983+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:11:44.363+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:11:44.362+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:11:44.497+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:11:44.496+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:11:44.730+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.899 seconds
[2022-12-16T13:11:55.129+0000] {processor.py:154} INFO - Started process (PID=237) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:11:55.135+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:11:55.145+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:11:55.144+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:11:55.393+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:11:56.555+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:11:56.554+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:11:56.809+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:11:56.808+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:11:57.057+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.962 seconds
[2022-12-16T13:12:07.199+0000] {processor.py:154} INFO - Started process (PID=249) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:12:07.283+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:12:07.288+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:12:07.287+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:12:07.373+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:12:07.610+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:12:07.609+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:12:07.786+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:12:07.785+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:12:08.111+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.930 seconds
[2022-12-16T13:12:18.527+0000] {processor.py:154} INFO - Started process (PID=257) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:12:18.540+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:12:18.544+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:12:18.543+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:12:18.689+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:12:19.128+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:12:19.127+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:12:20.041+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:12:20.040+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:12:20.345+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.844 seconds
[2022-12-16T13:12:31.117+0000] {processor.py:154} INFO - Started process (PID=267) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:12:31.136+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:12:31.146+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:12:31.145+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:12:31.379+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:12:32.497+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T13:12:43.272+0000] {processor.py:154} INFO - Started process (PID=285) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:12:43.276+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:12:43.280+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:12:43.279+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:12:43.818+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:12:44.294+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:12:44.293+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:12:44.449+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:12:44.448+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:12:44.589+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.376 seconds
[2022-12-16T13:12:55.172+0000] {processor.py:154} INFO - Started process (PID=295) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:12:55.207+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:12:55.252+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:12:55.210+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:12:55.506+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:12:57.000+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:12:56.999+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:12:57.299+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:12:57.290+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:12:57.684+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.634 seconds
[2022-12-16T13:13:08.430+0000] {processor.py:154} INFO - Started process (PID=302) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:13:08.441+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:13:08.455+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:13:08.450+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:13:08.819+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:13:10.157+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:13:10.156+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:13:10.458+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:13:10.457+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:13:11.204+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.813 seconds
[2022-12-16T13:13:22.182+0000] {processor.py:154} INFO - Started process (PID=320) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:13:22.209+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:13:22.260+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:13:22.234+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:13:23.034+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:13:25.190+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:13:25.189+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:13:25.902+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:13:25.901+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:13:26.899+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 4.815 seconds
[2022-12-16T13:13:37.371+0000] {processor.py:154} INFO - Started process (PID=332) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:13:37.378+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:13:37.403+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:13:37.402+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:13:37.753+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:13:38.463+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:13:38.451+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:13:38.743+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:13:38.742+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:13:39.021+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.690 seconds
[2022-12-16T13:13:49.512+0000] {processor.py:154} INFO - Started process (PID=343) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:13:49.516+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:13:49.520+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:13:49.519+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:13:49.638+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:13:50.150+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:13:50.146+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:13:50.364+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:13:50.363+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:13:50.609+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.117 seconds
[2022-12-16T13:14:00.765+0000] {processor.py:154} INFO - Started process (PID=350) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:14:00.827+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:14:00.832+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:14:00.831+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:14:00.962+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:14:01.253+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:14:01.252+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:14:01.432+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:14:01.431+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:14:01.688+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.939 seconds
[2022-12-16T13:14:12.881+0000] {processor.py:154} INFO - Started process (PID=368) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:14:12.906+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:14:12.965+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:14:12.964+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:14:13.490+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:14:14.632+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:14:14.630+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:14:15.312+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:14:15.306+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:14:15.792+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.089 seconds
[2022-12-16T13:14:26.104+0000] {processor.py:154} INFO - Started process (PID=381) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:14:26.107+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:14:26.112+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:14:26.111+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:14:26.275+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:14:27.216+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:14:27.214+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:14:27.817+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:14:27.816+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:14:28.182+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.095 seconds
[2022-12-16T13:14:38.641+0000] {processor.py:154} INFO - Started process (PID=390) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:14:38.668+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:14:38.677+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:14:38.676+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:14:38.994+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:14:39.528+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:14:39.526+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:14:39.905+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:14:39.904+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:14:40.228+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.656 seconds
[2022-12-16T13:14:50.760+0000] {processor.py:154} INFO - Started process (PID=401) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:14:50.784+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:14:50.804+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:14:50.803+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:14:51.142+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:14:52.069+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:14:52.068+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:14:52.233+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:14:52.232+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:14:52.429+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.705 seconds
[2022-12-16T13:15:03.170+0000] {processor.py:154} INFO - Started process (PID=418) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:15:03.197+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:15:03.214+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:15:03.213+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:15:03.612+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:15:04.353+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:15:04.352+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:15:04.849+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:15:04.848+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:15:05.212+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.133 seconds
[2022-12-16T13:15:15.645+0000] {processor.py:154} INFO - Started process (PID=428) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:15:15.649+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:15:15.653+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:15:15.652+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:15:15.837+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:15:16.802+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:15:16.801+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:15:17.230+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:15:17.229+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:15:17.619+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.990 seconds
[2022-12-16T13:15:28.011+0000] {processor.py:154} INFO - Started process (PID=435) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:15:28.016+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:15:28.025+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:15:28.024+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:15:28.239+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:15:29.379+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:15:29.378+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:15:29.585+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:15:29.584+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:15:29.849+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.855 seconds
[2022-12-16T13:15:40.500+0000] {processor.py:154} INFO - Started process (PID=452) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:15:40.531+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:15:40.539+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:15:40.534+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:15:40.722+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:15:41.057+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:15:41.056+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:15:41.244+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:15:41.243+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:15:41.374+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.889 seconds
[2022-12-16T13:15:52.065+0000] {processor.py:154} INFO - Started process (PID=463) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:15:52.077+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:15:52.082+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:15:52.081+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:15:52.161+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:15:52.921+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:15:52.921+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:15:53.050+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:15:53.049+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:15:53.172+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.125 seconds
[2022-12-16T13:16:03.433+0000] {processor.py:154} INFO - Started process (PID=473) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:16:03.479+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:16:03.484+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:16:03.483+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:16:03.565+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:16:04.112+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T13:16:14.396+0000] {processor.py:154} INFO - Started process (PID=483) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:16:14.399+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:16:14.403+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:16:14.402+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:16:14.482+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:16:15.173+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:16:15.172+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:16:15.308+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:16:15.308+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:16:15.445+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.064 seconds
[2022-12-16T13:16:25.832+0000] {processor.py:154} INFO - Started process (PID=501) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:16:25.843+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:16:25.851+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:16:25.846+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:16:25.957+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:16:26.192+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:16:26.191+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:16:26.349+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:16:26.348+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:16:26.502+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.689 seconds
[2022-12-16T13:16:36.913+0000] {processor.py:154} INFO - Started process (PID=511) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:16:36.943+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:16:36.947+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:16:36.946+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:16:37.130+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:16:37.394+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:16:37.393+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:16:37.538+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:16:37.537+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:16:37.650+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.766 seconds
[2022-12-16T13:16:48.297+0000] {processor.py:154} INFO - Started process (PID=521) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:16:48.301+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:16:48.305+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:16:48.304+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:16:48.403+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:16:48.758+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:16:48.757+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:16:48.893+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:16:48.892+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:16:49.031+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.750 seconds
[2022-12-16T13:16:59.447+0000] {processor.py:154} INFO - Started process (PID=538) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:16:59.496+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:16:59.501+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:16:59.499+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:16:59.720+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:17:00.520+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:17:00.519+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:17:00.678+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:17:00.677+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:17:00.790+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.436 seconds
[2022-12-16T13:17:11.271+0000] {processor.py:154} INFO - Started process (PID=549) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:17:11.275+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:17:11.279+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:17:11.278+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:17:11.363+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:17:12.321+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:17:12.321+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:17:12.449+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:17:12.448+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:17:12.566+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.309 seconds
[2022-12-16T13:17:23.482+0000] {processor.py:154} INFO - Started process (PID=559) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:17:23.506+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:17:23.511+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:17:23.509+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:17:23.591+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:17:23.799+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:17:23.798+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:17:23.937+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:17:23.936+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:17:24.046+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.577 seconds
[2022-12-16T13:17:34.374+0000] {processor.py:154} INFO - Started process (PID=569) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:17:34.378+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:17:34.390+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:17:34.389+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:17:34.485+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:17:34.769+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:17:34.768+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:17:34.899+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:17:34.898+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:17:35.154+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.794 seconds
[2022-12-16T13:17:45.901+0000] {processor.py:154} INFO - Started process (PID=587) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:17:45.928+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:17:45.944+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:17:45.931+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:17:46.061+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:17:46.859+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T13:17:57.313+0000] {processor.py:154} INFO - Started process (PID=597) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:17:57.335+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:17:57.343+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:17:57.338+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:17:57.436+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:17:57.777+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:17:57.776+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:17:57.927+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:17:57.926+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:17:58.045+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.748 seconds
[2022-12-16T13:18:08.553+0000] {processor.py:154} INFO - Started process (PID=607) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:18:08.576+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:18:08.586+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:18:08.579+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:18:08.696+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:18:08.984+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:18:08.983+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:18:09.126+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:18:09.125+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:18:09.268+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.776 seconds
[2022-12-16T13:18:19.535+0000] {processor.py:154} INFO - Started process (PID=617) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:18:19.628+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:18:19.632+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:18:19.631+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:18:19.828+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:18:20.442+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:18:20.441+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:18:21.341+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:18:21.341+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:18:22.009+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.489 seconds
[2022-12-16T13:18:32.545+0000] {processor.py:154} INFO - Started process (PID=635) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:18:32.548+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:18:32.556+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:18:32.551+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:18:32.682+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:18:33.648+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:18:33.641+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:18:34.211+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:18:34.210+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:18:34.532+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.018 seconds
[2022-12-16T13:18:45.234+0000] {processor.py:154} INFO - Started process (PID=645) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:18:45.238+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:18:45.246+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:18:45.245+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:18:45.608+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:18:46.522+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:18:46.521+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:18:47.277+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:18:47.259+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:18:47.679+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.479 seconds
[2022-12-16T13:18:58.316+0000] {processor.py:154} INFO - Started process (PID=658) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:18:58.337+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:18:58.351+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:18:58.350+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:18:58.901+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:19:00.169+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:19:00.168+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:19:00.542+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:19:00.541+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:19:00.929+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.641 seconds
[2022-12-16T13:19:11.530+0000] {processor.py:154} INFO - Started process (PID=676) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:19:11.546+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:19:11.573+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:19:11.572+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:19:12.380+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:19:13.824+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:19:13.822+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:19:14.666+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:19:14.658+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:19:15.213+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.752 seconds
[2022-12-16T13:19:26.162+0000] {processor.py:154} INFO - Started process (PID=686) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:19:26.195+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:19:26.222+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:19:26.198+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:19:26.785+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:19:28.283+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:19:28.282+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:19:28.506+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:19:28.505+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:19:28.691+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.604 seconds
[2022-12-16T13:19:39.327+0000] {processor.py:154} INFO - Started process (PID=696) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:19:39.331+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:19:39.338+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:19:39.334+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:19:39.566+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:19:40.162+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:19:40.161+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:19:40.585+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:19:40.584+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:19:40.762+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.508 seconds
[2022-12-16T13:19:51.218+0000] {processor.py:154} INFO - Started process (PID=706) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:19:51.236+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:19:51.241+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:19:51.240+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:19:51.572+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:19:52.052+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:19:52.052+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:19:52.333+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:19:52.331+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:19:52.710+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.554 seconds
[2022-12-16T13:20:03.181+0000] {processor.py:154} INFO - Started process (PID=722) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:20:03.222+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:20:03.230+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:20:03.229+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:20:03.699+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:20:05.499+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:20:05.498+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:20:06.272+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:20:06.250+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:20:06.821+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.703 seconds
[2022-12-16T13:20:17.538+0000] {processor.py:154} INFO - Started process (PID=734) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:20:17.564+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:20:17.568+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:20:17.567+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:20:17.944+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:20:18.781+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:20:18.780+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:20:19.006+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:20:19.005+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:20:19.221+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.724 seconds
[2022-12-16T13:20:29.749+0000] {processor.py:154} INFO - Started process (PID=744) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:20:29.813+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:20:29.817+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:20:29.815+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:20:30.188+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:20:31.394+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:20:31.393+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:20:31.946+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:20:31.945+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:20:32.249+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.567 seconds
[2022-12-16T13:20:42.802+0000] {processor.py:154} INFO - Started process (PID=761) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:20:42.850+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:20:42.862+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:20:42.861+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:20:43.356+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:20:44.805+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:20:44.804+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:20:45.115+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:20:45.114+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:20:45.649+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.946 seconds
[2022-12-16T13:20:55.977+0000] {processor.py:154} INFO - Started process (PID=772) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:20:55.998+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:20:56.011+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:20:56.009+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:20:56.262+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:20:57.588+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:20:57.585+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:20:57.972+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:20:57.971+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:20:58.179+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.233 seconds
[2022-12-16T13:21:08.758+0000] {processor.py:154} INFO - Started process (PID=784) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:21:08.776+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:21:08.790+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:21:08.789+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:21:08.991+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:21:10.384+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:21:10.382+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:21:10.642+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:21:10.638+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:21:10.887+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.173 seconds
[2022-12-16T13:21:21.362+0000] {processor.py:154} INFO - Started process (PID=794) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:21:21.507+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:21:21.524+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:21:21.515+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:21:21.927+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:21:22.375+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:21:22.370+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:21:22.664+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:21:22.663+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:21:23.108+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.785 seconds
[2022-12-16T13:21:33.620+0000] {processor.py:154} INFO - Started process (PID=812) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:21:33.632+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:21:33.646+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:21:33.645+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:21:33.967+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:21:34.967+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:21:34.966+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:21:35.285+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:21:35.284+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:21:35.618+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.080 seconds
[2022-12-16T13:21:46.514+0000] {processor.py:154} INFO - Started process (PID=822) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:21:46.522+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:21:46.534+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:21:46.533+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:21:46.869+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:21:48.292+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:21:48.291+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:21:48.870+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:21:48.869+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:21:49.208+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.764 seconds
[2022-12-16T13:21:59.625+0000] {processor.py:154} INFO - Started process (PID=832) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:21:59.631+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:21:59.636+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:21:59.634+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:21:59.739+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:22:00.097+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:22:00.095+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:22:00.375+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:22:00.374+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:22:00.530+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.926 seconds
[2022-12-16T13:22:11.012+0000] {processor.py:154} INFO - Started process (PID=842) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:22:11.037+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:22:11.041+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:22:11.040+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:22:11.152+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:22:11.395+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:22:11.394+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:22:11.560+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:22:11.559+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:22:11.686+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.691 seconds
[2022-12-16T13:22:22.062+0000] {processor.py:154} INFO - Started process (PID=859) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:22:22.184+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:22:22.190+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:22:22.187+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:22:22.446+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:22:23.062+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:22:23.061+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:22:23.344+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:22:23.338+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:22:23.644+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.606 seconds
[2022-12-16T13:22:34.066+0000] {processor.py:154} INFO - Started process (PID=870) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:22:34.105+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:22:34.115+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:22:34.114+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:22:34.325+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:22:34.843+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:22:34.842+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:22:34.999+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:22:34.998+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:22:35.164+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.121 seconds
[2022-12-16T13:22:45.670+0000] {processor.py:154} INFO - Started process (PID=880) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:22:45.707+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:22:45.712+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:22:45.710+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:22:45.886+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:22:46.306+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:22:46.305+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:22:46.475+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:22:46.474+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:22:46.619+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.989 seconds
[2022-12-16T13:22:57.142+0000] {processor.py:154} INFO - Started process (PID=890) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:22:57.240+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:22:57.248+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:22:57.242+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:22:57.444+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:22:57.750+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:22:57.749+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:22:57.946+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:22:57.945+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:22:58.069+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.975 seconds
[2022-12-16T13:23:08.594+0000] {processor.py:154} INFO - Started process (PID=908) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:23:08.632+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:23:08.645+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:23:08.635+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:23:09.045+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:23:10.130+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:23:10.129+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:23:10.814+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:23:10.814+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:23:11.114+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.546 seconds
[2022-12-16T13:23:21.445+0000] {processor.py:154} INFO - Started process (PID=918) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:23:21.448+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:23:21.460+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:23:21.459+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:23:21.820+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:23:23.253+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:23:23.251+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:23:23.446+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:23:23.445+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:23:23.683+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.276 seconds
[2022-12-16T13:23:34.787+0000] {processor.py:154} INFO - Started process (PID=928) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:23:34.837+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:23:34.843+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:23:34.842+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:23:35.005+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:23:35.314+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:23:35.313+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:23:35.495+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:23:35.494+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:23:35.796+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.030 seconds
[2022-12-16T13:23:46.202+0000] {processor.py:154} INFO - Started process (PID=938) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:23:46.255+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:23:46.268+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:23:46.267+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:23:46.610+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:23:47.242+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:23:47.241+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:23:47.409+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:23:47.408+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:23:47.560+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.372 seconds
[2022-12-16T13:23:58.316+0000] {processor.py:154} INFO - Started process (PID=956) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:23:58.366+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:23:58.386+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:23:58.385+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:23:59.067+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:24:00.929+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:24:00.928+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:24:01.450+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:24:01.449+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:24:02.185+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 4.027 seconds
[2022-12-16T13:24:12.405+0000] {processor.py:154} INFO - Started process (PID=966) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:24:12.410+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:24:12.414+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:24:12.413+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:24:12.545+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:24:12.844+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:24:12.843+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:24:13.169+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:24:13.168+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:24:13.399+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.011 seconds
[2022-12-16T13:24:23.773+0000] {processor.py:154} INFO - Started process (PID=976) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:24:23.776+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:24:23.780+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:24:23.779+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:24:23.869+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:24:24.144+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:24:24.143+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:24:24.273+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:24:24.272+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:24:24.421+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.663 seconds
[2022-12-16T13:24:34.909+0000] {processor.py:154} INFO - Started process (PID=993) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:24:34.931+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:24:34.944+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:24:34.943+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:24:35.345+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:24:35.955+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:24:35.954+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:24:36.312+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:24:36.298+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:24:36.690+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.863 seconds
[2022-12-16T13:24:47.481+0000] {processor.py:154} INFO - Started process (PID=1004) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:24:47.495+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:24:47.509+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:24:47.507+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:24:47.797+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:24:48.751+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T13:24:59.082+0000] {processor.py:154} INFO - Started process (PID=1014) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:24:59.100+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:24:59.105+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:24:59.103+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:24:59.191+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:24:59.430+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:24:59.429+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:24:59.570+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:24:59.562+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:24:59.843+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.773 seconds
[2022-12-16T13:25:10.381+0000] {processor.py:154} INFO - Started process (PID=1024) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:25:10.427+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:25:10.448+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:25:10.430+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:25:10.640+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:25:11.154+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:25:11.153+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:25:11.348+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:25:11.347+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:25:11.525+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.209 seconds
[2022-12-16T13:25:22.022+0000] {processor.py:154} INFO - Started process (PID=1041) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:25:22.064+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:25:22.073+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:25:22.072+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:25:22.379+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:25:22.921+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:25:22.920+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:25:23.185+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:25:23.184+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:25:23.344+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.406 seconds
[2022-12-16T13:25:34.281+0000] {processor.py:154} INFO - Started process (PID=1051) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:25:34.284+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:25:34.288+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:25:34.287+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:25:34.427+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:25:35.180+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T13:25:45.572+0000] {processor.py:154} INFO - Started process (PID=1061) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:25:45.624+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:25:45.628+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:25:45.626+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:25:45.961+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:25:46.964+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:25:46.962+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:25:47.176+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:25:47.162+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:25:47.383+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.842 seconds
[2022-12-16T13:25:57.602+0000] {processor.py:154} INFO - Started process (PID=1071) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:25:57.606+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:25:57.610+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:25:57.609+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:25:57.695+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:25:57.946+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:25:57.945+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:25:58.123+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:25:58.122+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:25:58.269+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.682 seconds
[2022-12-16T13:26:08.767+0000] {processor.py:154} INFO - Started process (PID=1088) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:26:08.771+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:26:08.783+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:26:08.778+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:26:08.920+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:26:10.771+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:26:10.770+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:26:11.048+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:26:11.047+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:26:11.165+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.418 seconds
[2022-12-16T13:26:21.501+0000] {processor.py:154} INFO - Started process (PID=1098) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:26:21.504+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:26:21.509+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:26:21.508+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:26:21.596+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:26:21.804+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:26:21.802+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:26:21.938+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:26:21.937+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:26:22.082+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.595 seconds
[2022-12-16T13:26:32.277+0000] {processor.py:154} INFO - Started process (PID=1108) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:26:32.294+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:26:32.318+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:26:32.317+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:26:32.634+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:26:33.069+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:26:33.068+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:26:33.254+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:26:33.253+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:26:33.360+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.104 seconds
[2022-12-16T13:26:43.496+0000] {processor.py:154} INFO - Started process (PID=1118) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:26:43.545+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:26:43.549+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:26:43.548+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:26:43.648+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:26:44.933+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T13:26:55.587+0000] {processor.py:154} INFO - Started process (PID=1135) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:26:55.592+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:26:55.597+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:26:55.596+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:26:55.706+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:26:55.937+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:26:55.936+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:26:56.215+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:26:56.194+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:26:56.444+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.878 seconds
[2022-12-16T13:27:07.260+0000] {processor.py:154} INFO - Started process (PID=1145) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:27:07.265+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:27:07.269+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:27:07.268+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:27:07.421+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:27:08.520+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:27:08.519+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:27:08.766+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:27:08.765+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:27:08.996+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.766 seconds
[2022-12-16T13:27:19.107+0000] {processor.py:154} INFO - Started process (PID=1155) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:27:19.111+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:27:19.115+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:27:19.114+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:27:19.202+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:27:19.601+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:27:19.600+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:27:19.836+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:27:19.835+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:27:20.045+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.951 seconds
[2022-12-16T13:27:30.373+0000] {processor.py:154} INFO - Started process (PID=1173) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:27:30.393+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:27:30.398+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:27:30.397+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:27:30.515+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:27:30.765+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:27:30.763+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:27:31.005+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:27:31.004+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:27:31.289+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.931 seconds
[2022-12-16T13:27:41.826+0000] {processor.py:154} INFO - Started process (PID=1183) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:27:41.967+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:27:41.972+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:27:41.971+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:27:42.057+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:27:42.266+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:27:42.266+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:27:42.409+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:27:42.408+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:27:42.522+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.710 seconds
[2022-12-16T13:27:52.798+0000] {processor.py:154} INFO - Started process (PID=1193) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:27:52.802+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:27:52.806+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:27:52.805+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:27:52.893+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:27:53.105+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:27:53.104+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:27:53.265+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:27:53.264+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:27:53.398+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.613 seconds
[2022-12-16T13:28:03.665+0000] {processor.py:154} INFO - Started process (PID=1203) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:28:03.692+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:28:03.696+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:28:03.695+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:28:03.782+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:28:03.988+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:28:03.987+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:28:04.116+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:28:04.115+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:28:04.276+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.627 seconds
[2022-12-16T13:28:14.633+0000] {processor.py:154} INFO - Started process (PID=1221) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:28:14.720+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:28:14.730+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:28:14.728+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:28:14.871+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:28:15.892+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:28:15.890+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:28:16.132+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:28:16.131+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:28:16.369+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.762 seconds
[2022-12-16T13:28:26.971+0000] {processor.py:154} INFO - Started process (PID=1231) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:28:26.976+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:28:26.981+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:28:26.980+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:28:27.108+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:28:27.355+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:28:27.354+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:28:27.506+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:28:27.503+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:28:27.689+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.736 seconds
[2022-12-16T13:28:38.316+0000] {processor.py:154} INFO - Started process (PID=1241) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:28:38.319+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:28:38.323+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:28:38.322+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:28:38.511+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:28:38.980+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:28:38.978+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:28:39.157+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:28:39.156+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:28:39.351+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.051 seconds
[2022-12-16T13:28:49.821+0000] {processor.py:154} INFO - Started process (PID=1251) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:28:49.825+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:28:49.829+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:28:49.828+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:28:49.919+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:28:50.206+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:28:50.205+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:28:50.431+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:28:50.427+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:28:50.964+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.158 seconds
[2022-12-16T13:29:01.414+0000] {processor.py:154} INFO - Started process (PID=1269) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:29:01.437+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:29:01.445+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:29:01.444+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:29:01.693+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:29:02.178+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:29:02.178+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:29:02.434+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:29:02.433+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:29:02.760+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.405 seconds
[2022-12-16T13:29:13.484+0000] {processor.py:154} INFO - Started process (PID=1279) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:29:13.528+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:29:13.536+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:29:13.535+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:29:13.869+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:29:14.831+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:29:14.830+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:29:15.171+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:29:15.170+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:29:15.384+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.961 seconds
[2022-12-16T13:29:25.734+0000] {processor.py:154} INFO - Started process (PID=1289) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:29:25.765+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:29:25.769+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:29:25.768+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:29:25.881+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:29:26.398+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:29:26.396+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:29:26.552+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:29:26.552+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:29:26.705+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.033 seconds
[2022-12-16T13:29:37.192+0000] {processor.py:154} INFO - Started process (PID=1306) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:29:37.200+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:29:37.222+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:29:37.202+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:29:37.479+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:29:37.794+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:29:37.793+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:29:38.011+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:29:38.010+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:29:38.256+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.154 seconds
[2022-12-16T13:29:48.736+0000] {processor.py:154} INFO - Started process (PID=1316) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:29:48.855+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:29:48.878+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:29:48.858+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:29:49.045+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:29:50.666+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:29:50.665+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:29:50.809+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:29:50.808+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:29:50.916+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.217 seconds
[2022-12-16T13:30:01.265+0000] {processor.py:154} INFO - Started process (PID=1326) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:30:01.290+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:30:01.294+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:30:01.293+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:30:01.413+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:30:02.744+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:30:02.738+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:30:02.963+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:30:02.962+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:30:03.099+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.859 seconds
[2022-12-16T13:30:13.577+0000] {processor.py:154} INFO - Started process (PID=1336) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:30:13.602+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:30:13.606+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:30:13.605+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:30:13.732+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:30:14.378+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:30:14.377+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:30:14.700+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:30:14.699+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:30:14.832+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.298 seconds
[2022-12-16T13:30:25.188+0000] {processor.py:154} INFO - Started process (PID=1354) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:30:25.197+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:30:25.206+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:30:25.205+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:30:25.458+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:30:26.284+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:30:26.283+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:30:26.543+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:30:26.542+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:30:26.735+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.607 seconds
[2022-12-16T13:30:37.220+0000] {processor.py:154} INFO - Started process (PID=1364) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:30:37.224+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:30:37.229+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:30:37.228+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:30:37.399+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:30:37.644+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:30:37.643+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:30:37.773+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:30:37.772+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:30:37.917+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.711 seconds
[2022-12-16T13:30:48.236+0000] {processor.py:154} INFO - Started process (PID=1374) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:30:48.240+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:30:48.245+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:30:48.244+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:30:48.412+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:30:49.165+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:30:49.163+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:30:49.422+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:30:49.421+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:30:49.570+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.355 seconds
[2022-12-16T13:30:59.855+0000] {processor.py:154} INFO - Started process (PID=1384) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:30:59.873+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:30:59.877+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:30:59.876+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:30:59.978+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:31:00.300+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:31:00.298+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:31:00.465+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:31:00.464+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:31:00.638+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.798 seconds
[2022-12-16T13:31:10.940+0000] {processor.py:154} INFO - Started process (PID=1401) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:31:10.950+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:31:10.958+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:31:10.957+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:31:11.145+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:31:12.070+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:31:12.069+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:31:12.404+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:31:12.403+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:31:12.593+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.698 seconds
[2022-12-16T13:31:23.012+0000] {processor.py:154} INFO - Started process (PID=1411) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:31:23.021+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:31:23.032+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:31:23.026+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:31:23.325+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:31:23.765+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:31:23.764+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:31:24.048+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:31:24.047+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:31:24.604+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.647 seconds
[2022-12-16T13:31:34.956+0000] {processor.py:154} INFO - Started process (PID=1421) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:31:34.960+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:31:34.967+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:31:34.966+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:31:35.053+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:31:35.458+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:31:35.457+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:31:35.663+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:31:35.662+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:31:35.825+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.883 seconds
[2022-12-16T13:31:46.285+0000] {processor.py:154} INFO - Started process (PID=1438) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:31:46.289+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:31:46.293+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:31:46.292+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:31:46.568+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:31:47.188+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:31:47.187+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:31:47.446+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:31:47.445+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:31:47.674+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.405 seconds
[2022-12-16T13:31:58.121+0000] {processor.py:154} INFO - Started process (PID=1449) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:31:58.178+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:31:58.186+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:31:58.185+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:31:58.310+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:31:58.925+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:31:58.924+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:31:59.114+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:31:59.113+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:31:59.249+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.142 seconds
[2022-12-16T13:32:09.513+0000] {processor.py:154} INFO - Started process (PID=1459) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:32:09.516+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:32:09.521+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:32:09.520+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:32:09.613+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:32:09.866+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:32:09.865+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:32:10.031+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:32:10.030+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:32:10.183+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.686 seconds
[2022-12-16T13:32:20.508+0000] {processor.py:154} INFO - Started process (PID=1469) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:32:20.557+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:32:20.561+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:32:20.560+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:32:20.651+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:32:21.563+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:32:21.562+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:32:21.785+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:32:21.785+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:32:21.935+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.441 seconds
[2022-12-16T13:32:32.584+0000] {processor.py:154} INFO - Started process (PID=1486) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:32:32.604+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:32:32.648+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:32:32.637+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:32:32.914+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:32:33.501+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:32:33.500+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:32:33.723+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:32:33.722+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:32:34.020+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.496 seconds
[2022-12-16T13:32:44.335+0000] {processor.py:154} INFO - Started process (PID=1497) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:32:44.361+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:32:44.366+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:32:44.365+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:32:44.714+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:32:45.428+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:32:45.427+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:32:45.740+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:32:45.739+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:32:46.001+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.706 seconds
[2022-12-16T13:32:56.319+0000] {processor.py:154} INFO - Started process (PID=1507) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:32:56.323+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:32:56.327+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:32:56.326+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:32:56.437+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:32:57.901+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:32:57.900+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:32:58.088+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:32:58.087+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:32:58.283+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.978 seconds
[2022-12-16T13:33:08.715+0000] {processor.py:154} INFO - Started process (PID=1517) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:33:08.720+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:33:08.725+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:33:08.724+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:33:08.915+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:33:09.359+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:33:09.357+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:33:09.971+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:33:09.970+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:33:10.161+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.491 seconds
[2022-12-16T13:33:20.536+0000] {processor.py:154} INFO - Started process (PID=1532) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:33:20.540+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:33:20.550+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:33:20.549+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:33:20.989+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:33:22.622+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:33:22.621+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:33:23.022+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:33:23.020+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:33:23.378+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.890 seconds
[2022-12-16T13:33:33.860+0000] {processor.py:154} INFO - Started process (PID=1544) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:33:33.869+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:33:33.876+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:33:33.872+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:33:33.998+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:33:34.301+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:33:34.300+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:33:34.516+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:33:34.515+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:33:34.764+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.929 seconds
[2022-12-16T13:33:45.285+0000] {processor.py:154} INFO - Started process (PID=1552) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:33:45.326+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:33:45.342+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:33:45.340+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:33:45.560+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:33:46.260+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:33:46.259+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:33:46.467+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:33:46.466+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:33:46.641+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.448 seconds
[2022-12-16T13:33:56.987+0000] {processor.py:154} INFO - Started process (PID=1562) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:33:57.008+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:33:57.018+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:33:57.016+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:33:57.136+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:33:58.646+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T13:34:09.921+0000] {processor.py:154} INFO - Started process (PID=1582) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:34:09.976+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:34:09.985+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:34:09.984+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:34:10.420+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:34:11.280+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:34:11.279+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:34:11.704+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:34:11.698+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:34:11.967+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.166 seconds
[2022-12-16T13:34:22.270+0000] {processor.py:154} INFO - Started process (PID=1590) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:34:22.281+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:34:22.295+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:34:22.294+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:34:22.416+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:34:23.086+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:34:23.085+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:34:23.421+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:34:23.420+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:34:23.753+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.501 seconds
[2022-12-16T13:34:34.297+0000] {processor.py:154} INFO - Started process (PID=1599) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:34:34.308+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:34:34.322+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:34:34.321+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:34:34.850+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:34:35.742+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T13:34:46.624+0000] {processor.py:154} INFO - Started process (PID=1609) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:34:46.637+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:34:46.646+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:34:46.640+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:34:47.060+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:34:48.166+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:34:48.165+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:34:48.409+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:34:48.408+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:34:48.663+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.126 seconds
[2022-12-16T13:34:59.341+0000] {processor.py:154} INFO - Started process (PID=1628) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:34:59.372+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:34:59.380+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:34:59.379+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:34:59.657+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:35:00.224+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:35:00.223+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:35:00.751+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:35:00.742+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:35:01.182+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.896 seconds
[2022-12-16T13:35:11.441+0000] {processor.py:154} INFO - Started process (PID=1635) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:35:11.474+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:35:11.478+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:35:11.477+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:35:11.602+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:35:12.190+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:35:12.188+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:35:12.427+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:35:12.426+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:35:12.832+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.405 seconds
[2022-12-16T13:35:23.282+0000] {processor.py:154} INFO - Started process (PID=1645) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:35:23.291+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:35:23.296+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:35:23.294+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:35:23.408+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:35:23.655+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:35:23.654+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:35:23.849+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:35:23.848+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:35:23.991+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.729 seconds
[2022-12-16T13:35:34.340+0000] {processor.py:154} INFO - Started process (PID=1661) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:35:34.368+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:35:34.377+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:35:34.373+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:35:34.626+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:35:35.690+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:35:35.689+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:35:36.196+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:35:36.192+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:35:37.066+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.757 seconds
[2022-12-16T13:35:48.024+0000] {processor.py:154} INFO - Started process (PID=1672) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:35:48.031+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:35:48.037+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:35:48.036+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:35:48.310+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:35:49.420+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:35:49.418+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:35:49.785+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:35:49.784+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:35:49.904+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.944 seconds
[2022-12-16T13:36:00.074+0000] {processor.py:154} INFO - Started process (PID=1682) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:36:00.113+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:36:00.121+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:36:00.120+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:36:00.209+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:36:00.790+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:36:00.788+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:36:00.970+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:36:00.969+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:36:01.215+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.161 seconds
[2022-12-16T13:36:11.562+0000] {processor.py:154} INFO - Started process (PID=1692) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:36:11.605+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:36:11.609+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:36:11.608+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:36:11.713+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:36:13.452+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:36:13.451+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:36:13.842+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:36:13.841+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:36:14.053+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.506 seconds
[2022-12-16T13:36:24.557+0000] {processor.py:154} INFO - Started process (PID=1713) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:36:24.566+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:36:24.571+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:36:24.570+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:36:24.706+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:36:25.326+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:36:25.325+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:36:25.939+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:36:25.938+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:36:26.129+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.588 seconds
[2022-12-16T13:36:37.177+0000] {processor.py:154} INFO - Started process (PID=1723) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:36:37.194+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:36:37.203+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:36:37.202+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:36:37.493+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:36:39.807+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:36:39.806+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:36:40.291+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:36:40.278+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:36:40.720+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.613 seconds
[2022-12-16T13:36:51.420+0000] {processor.py:154} INFO - Started process (PID=1735) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:36:51.473+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:36:51.482+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:36:51.480+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:36:51.617+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:36:52.653+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:36:52.652+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:36:52.831+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:36:52.830+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:36:53.069+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.672 seconds
[2022-12-16T13:37:03.546+0000] {processor.py:154} INFO - Started process (PID=1752) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:37:03.565+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:37:03.573+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:37:03.572+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:37:03.779+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:37:05.137+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:37:05.136+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:37:05.415+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:37:05.414+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:37:05.677+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.168 seconds
[2022-12-16T13:37:16.022+0000] {processor.py:154} INFO - Started process (PID=1763) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:37:16.070+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:37:16.076+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:37:16.075+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:37:16.225+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:37:16.691+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:37:16.686+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:37:16.977+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:37:16.976+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:37:17.179+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.173 seconds
[2022-12-16T13:37:27.543+0000] {processor.py:154} INFO - Started process (PID=1773) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:37:27.548+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:37:27.556+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:37:27.555+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:37:27.663+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:37:28.560+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:37:28.559+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:37:28.707+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:37:28.706+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:37:28.828+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.307 seconds
[2022-12-16T13:37:39.145+0000] {processor.py:154} INFO - Started process (PID=1783) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:37:39.153+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:37:39.164+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:37:39.163+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:37:39.410+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:37:40.146+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T13:37:50.814+0000] {processor.py:154} INFO - Started process (PID=1801) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:37:50.836+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:37:50.842+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:37:50.840+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:37:51.075+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:37:51.626+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:37:51.625+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:37:51.898+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:37:51.898+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:37:52.181+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.427 seconds
[2022-12-16T13:38:02.741+0000] {processor.py:154} INFO - Started process (PID=1811) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:38:02.745+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:38:02.750+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:38:02.748+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:38:02.840+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:38:04.620+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:38:04.619+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:38:04.862+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:38:04.861+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:38:05.053+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.327 seconds
[2022-12-16T13:38:15.393+0000] {processor.py:154} INFO - Started process (PID=1821) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:38:15.538+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:38:15.543+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:38:15.542+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:38:15.754+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:38:16.741+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:38:16.739+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:38:16.942+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:38:16.941+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:38:17.481+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.118 seconds
[2022-12-16T13:38:27.833+0000] {processor.py:154} INFO - Started process (PID=1831) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:38:27.860+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:38:27.872+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:38:27.871+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:38:28.261+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:38:28.946+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:38:28.945+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:38:29.514+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:38:29.513+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:38:29.794+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.043 seconds
[2022-12-16T13:38:40.659+0000] {processor.py:154} INFO - Started process (PID=1849) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:38:40.677+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:38:40.688+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:38:40.687+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:38:41.178+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:38:41.621+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:38:41.620+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:38:41.882+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:38:41.881+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:38:42.031+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.499 seconds
[2022-12-16T13:38:52.483+0000] {processor.py:154} INFO - Started process (PID=1859) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:38:52.487+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:38:52.491+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:38:52.490+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:38:52.643+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:38:54.321+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T13:39:04.726+0000] {processor.py:154} INFO - Started process (PID=1869) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:39:04.753+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:39:04.758+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:39:04.757+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:39:04.935+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:39:05.572+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:39:05.571+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:39:05.726+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:39:05.725+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:39:05.838+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.129 seconds
[2022-12-16T13:39:16.179+0000] {processor.py:154} INFO - Started process (PID=1887) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:39:16.196+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:39:16.208+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:39:16.199+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:39:16.441+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:39:17.035+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:39:17.034+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:39:17.244+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:39:17.243+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:39:17.426+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.265 seconds
[2022-12-16T13:39:27.570+0000] {processor.py:154} INFO - Started process (PID=1897) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:39:27.581+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:39:27.590+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:39:27.589+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:39:27.744+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:39:28.005+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:39:28.005+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:39:28.171+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:39:28.170+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:39:28.378+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.833 seconds
[2022-12-16T13:39:38.985+0000] {processor.py:154} INFO - Started process (PID=1907) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:39:38.989+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:39:38.994+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:39:38.993+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:39:39.086+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:39:39.313+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:39:39.312+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:39:39.500+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:39:39.499+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:39:39.756+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.790 seconds
[2022-12-16T13:39:50.656+0000] {processor.py:154} INFO - Started process (PID=1917) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:39:50.671+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:39:50.680+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:39:50.679+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:39:50.796+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:39:51.689+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:39:51.688+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:39:51.847+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:39:51.846+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:39:52.444+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.830 seconds
[2022-12-16T13:40:02.875+0000] {processor.py:154} INFO - Started process (PID=1936) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:40:02.884+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:40:02.898+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:40:02.897+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:40:03.093+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:40:04.358+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T13:40:14.854+0000] {processor.py:154} INFO - Started process (PID=1946) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:40:14.858+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:40:14.862+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:40:14.861+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:40:14.973+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:40:15.395+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:40:15.394+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:40:15.719+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:40:15.690+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:40:16.180+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.342 seconds
[2022-12-16T13:40:26.912+0000] {processor.py:154} INFO - Started process (PID=1956) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:40:26.956+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:40:26.965+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:40:26.964+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:40:27.329+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:40:27.666+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:40:27.665+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:40:27.845+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:40:27.842+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:40:28.036+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.157 seconds
[2022-12-16T13:40:38.553+0000] {processor.py:154} INFO - Started process (PID=1973) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:40:38.565+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:40:38.582+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:40:38.581+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:40:38.782+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:40:39.736+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:40:39.730+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:40:40.035+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:40:40.022+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:40:40.549+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.057 seconds
[2022-12-16T13:40:51.300+0000] {processor.py:154} INFO - Started process (PID=1984) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:40:51.304+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:40:51.313+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:40:51.312+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:40:51.452+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:40:52.435+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:40:52.434+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:40:52.821+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:40:52.820+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:40:53.349+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.080 seconds
[2022-12-16T13:41:03.689+0000] {processor.py:154} INFO - Started process (PID=1994) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:41:03.698+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:41:03.713+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:41:03.712+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:41:03.886+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:41:05.369+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:41:05.367+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:41:05.671+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:41:05.670+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:41:05.925+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.269 seconds
[2022-12-16T13:41:16.280+0000] {processor.py:154} INFO - Started process (PID=2004) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:41:16.331+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:41:16.336+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:41:16.335+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:41:16.519+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:41:17.088+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:41:17.075+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:41:17.634+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:41:17.633+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:41:17.836+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.581 seconds
[2022-12-16T13:41:28.452+0000] {processor.py:154} INFO - Started process (PID=2022) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:41:28.486+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:41:28.498+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:41:28.497+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:41:28.821+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:41:31.200+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T13:41:41.909+0000] {processor.py:154} INFO - Started process (PID=2032) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:41:41.914+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:41:41.922+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:41:41.921+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:41:42.146+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:41:43.089+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:41:43.088+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:41:43.225+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:41:43.224+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:41:43.379+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.486 seconds
[2022-12-16T13:41:53.830+0000] {processor.py:154} INFO - Started process (PID=2042) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:41:53.854+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:41:53.858+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:41:53.857+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:41:54.046+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:41:54.387+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:41:54.386+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:41:54.869+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:41:54.868+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:41:55.166+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.388 seconds
[2022-12-16T13:42:05.636+0000] {processor.py:154} INFO - Started process (PID=2059) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:42:05.651+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:42:05.684+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:42:05.654+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:42:06.081+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:42:06.399+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:42:06.398+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:42:06.663+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:42:06.662+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:42:06.996+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.419 seconds
[2022-12-16T13:42:17.615+0000] {processor.py:154} INFO - Started process (PID=2070) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:42:17.621+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:42:17.629+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:42:17.628+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:42:17.764+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:42:18.759+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:42:18.758+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:42:18.995+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:42:18.994+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:42:19.243+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.654 seconds
[2022-12-16T13:42:29.616+0000] {processor.py:154} INFO - Started process (PID=2080) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:42:29.710+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:42:29.715+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:42:29.714+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:42:29.826+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:42:30.223+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:42:30.222+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:42:30.367+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:42:30.366+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:42:30.480+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.888 seconds
[2022-12-16T13:42:40.949+0000] {processor.py:154} INFO - Started process (PID=2090) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:42:41.010+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:42:41.017+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:42:41.016+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:42:41.322+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:42:41.703+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:42:41.702+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:42:41.898+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:42:41.897+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:42:42.076+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.168 seconds
[2022-12-16T13:42:52.321+0000] {processor.py:154} INFO - Started process (PID=2108) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:42:52.328+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:42:52.336+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:42:52.335+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:42:52.520+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:42:53.453+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:42:53.452+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:42:53.614+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:42:53.613+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:42:53.802+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.520 seconds
[2022-12-16T13:43:04.176+0000] {processor.py:154} INFO - Started process (PID=2118) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:43:04.180+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:43:04.186+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:43:04.184+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:43:04.366+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:43:04.591+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:43:04.590+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:43:04.728+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:43:04.727+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:43:04.873+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.712 seconds
[2022-12-16T13:43:15.235+0000] {processor.py:154} INFO - Started process (PID=2128) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:43:15.240+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:43:15.245+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:43:15.244+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:43:15.362+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:43:16.358+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T13:43:26.784+0000] {processor.py:154} INFO - Started process (PID=2138) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:43:26.824+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:43:26.828+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:43:26.827+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:43:26.991+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:43:27.278+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:43:27.277+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:43:27.424+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:43:27.423+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:43:27.580+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.815 seconds
[2022-12-16T13:43:38.095+0000] {processor.py:154} INFO - Started process (PID=2157) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:43:38.105+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:43:38.119+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:43:38.114+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:43:38.337+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:43:39.059+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:43:39.041+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:43:39.453+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:43:39.452+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:43:39.699+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.646 seconds
[2022-12-16T13:43:50.265+0000] {processor.py:154} INFO - Started process (PID=2167) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:43:50.270+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:43:50.278+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:43:50.276+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:43:50.415+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:43:51.401+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T13:44:01.970+0000] {processor.py:154} INFO - Started process (PID=2177) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:44:02.016+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:44:02.025+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:44:02.024+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:44:02.219+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:44:03.158+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:44:03.157+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:44:03.525+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:44:03.524+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:44:03.897+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.983 seconds
[2022-12-16T13:44:14.120+0000] {processor.py:154} INFO - Started process (PID=2187) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:44:14.124+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:44:14.128+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:44:14.127+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:44:14.221+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:44:15.496+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T13:44:26.290+0000] {processor.py:154} INFO - Started process (PID=2205) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:44:26.302+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:44:26.309+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:44:26.308+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:44:26.556+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:44:26.886+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:44:26.885+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:44:27.142+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:44:27.141+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:44:27.304+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.070 seconds
[2022-12-16T13:44:37.692+0000] {processor.py:154} INFO - Started process (PID=2215) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:44:37.732+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:44:37.742+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:44:37.739+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:44:37.882+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:44:39.382+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T13:44:49.795+0000] {processor.py:154} INFO - Started process (PID=2225) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:44:49.827+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:44:49.832+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:44:49.831+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:44:49.971+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:44:50.738+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:44:50.738+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:44:50.877+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:44:50.876+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:44:51.025+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.244 seconds
[2022-12-16T13:45:01.407+0000] {processor.py:154} INFO - Started process (PID=2243) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:45:01.466+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:45:01.484+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:45:01.470+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:45:01.604+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:45:01.849+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:45:01.849+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:45:02.047+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:45:02.038+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:45:02.240+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.851 seconds
[2022-12-16T13:45:12.545+0000] {processor.py:154} INFO - Started process (PID=2253) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:45:12.576+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:45:12.580+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:45:12.579+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:45:12.705+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:45:13.480+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T13:45:23.828+0000] {processor.py:154} INFO - Started process (PID=2263) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:45:23.832+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:45:23.835+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:45:23.834+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:45:23.950+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:45:24.680+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:45:24.679+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:45:24.937+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:45:24.936+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:45:25.141+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.340 seconds
[2022-12-16T13:45:35.490+0000] {processor.py:154} INFO - Started process (PID=2273) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:45:35.496+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:45:35.502+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:45:35.501+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:45:35.636+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:45:35.968+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:45:35.967+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:45:36.143+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:45:36.140+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:45:36.346+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.882 seconds
[2022-12-16T13:45:46.842+0000] {processor.py:154} INFO - Started process (PID=2291) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:45:46.870+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:45:46.889+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:45:46.878+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:45:47.027+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:45:47.441+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:45:47.440+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:45:47.911+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:45:47.900+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:45:48.229+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.409 seconds
[2022-12-16T13:45:58.636+0000] {processor.py:154} INFO - Started process (PID=2301) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:45:58.682+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:45:58.687+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:45:58.686+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:45:58.777+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:45:59.333+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:45:59.332+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:45:59.481+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:45:59.480+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:45:59.631+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.010 seconds
[2022-12-16T13:46:09.997+0000] {processor.py:154} INFO - Started process (PID=2311) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:46:10.106+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:46:10.112+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:46:10.110+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:46:10.362+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:46:11.348+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:46:11.347+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:46:11.608+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:46:11.607+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:46:11.805+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.824 seconds
[2022-12-16T13:46:22.705+0000] {processor.py:154} INFO - Started process (PID=2321) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:46:22.713+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:46:22.718+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:46:22.717+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:46:22.826+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:46:23.302+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:46:23.300+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:46:23.506+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:46:23.505+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:46:23.647+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.969 seconds
[2022-12-16T13:46:34.286+0000] {processor.py:154} INFO - Started process (PID=2338) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:46:34.296+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:46:34.301+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:46:34.300+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:46:34.565+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:46:35.130+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:46:35.129+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:46:35.454+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:46:35.454+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:46:35.657+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.400 seconds
[2022-12-16T13:46:45.819+0000] {processor.py:154} INFO - Started process (PID=2348) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:46:45.895+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:46:45.900+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:46:45.899+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:46:46.001+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:46:46.241+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:46:46.240+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:46:46.373+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:46:46.372+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:46:46.525+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.722 seconds
[2022-12-16T13:46:56.825+0000] {processor.py:154} INFO - Started process (PID=2358) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:46:56.836+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:46:56.848+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:46:56.847+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:46:57.029+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:46:57.832+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:46:57.831+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:46:57.985+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:46:57.984+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:46:58.114+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.316 seconds
[2022-12-16T13:47:08.570+0000] {processor.py:154} INFO - Started process (PID=2368) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:47:08.588+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:47:08.592+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:47:08.591+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:47:09.013+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:47:11.187+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:47:11.186+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:47:11.541+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:47:11.540+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:47:11.768+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.294 seconds
[2022-12-16T13:47:22.799+0000] {processor.py:154} INFO - Started process (PID=2386) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:47:22.848+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:47:22.857+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:47:22.856+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:47:23.286+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:47:24.866+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T13:47:35.509+0000] {processor.py:154} INFO - Started process (PID=2396) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:47:35.514+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:47:35.525+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:47:35.524+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:47:35.710+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:47:36.067+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:47:36.066+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:47:36.289+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:47:36.288+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:47:36.493+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.019 seconds
[2022-12-16T13:47:46.658+0000] {processor.py:154} INFO - Started process (PID=2406) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:47:46.712+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:47:46.717+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:47:46.715+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:47:46.884+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:47:47.590+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:47:47.589+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:47:47.931+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:47:47.930+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:47:48.137+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.500 seconds
[2022-12-16T13:47:59.316+0000] {processor.py:154} INFO - Started process (PID=2424) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:47:59.344+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:47:59.352+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:47:59.352+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:47:59.576+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:48:00.104+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:48:00.103+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:48:00.443+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:48:00.438+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:48:00.730+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.480 seconds
[2022-12-16T13:48:11.414+0000] {processor.py:154} INFO - Started process (PID=2435) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:48:11.464+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:48:11.468+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:48:11.467+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:48:11.747+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:48:12.580+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:48:12.575+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:48:12.902+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:48:12.901+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:48:13.164+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.791 seconds
[2022-12-16T13:48:23.753+0000] {processor.py:154} INFO - Started process (PID=2445) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:48:23.832+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:48:23.838+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:48:23.837+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:48:24.007+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:48:24.482+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:48:24.481+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:48:24.723+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:48:24.721+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:48:24.971+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.237 seconds
[2022-12-16T13:48:35.375+0000] {processor.py:154} INFO - Started process (PID=2455) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:48:35.379+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:48:35.384+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:48:35.383+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:48:35.497+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:48:35.933+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:48:35.932+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:48:36.109+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:48:36.108+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:48:36.254+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.895 seconds
[2022-12-16T13:48:46.982+0000] {processor.py:154} INFO - Started process (PID=2473) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:48:46.986+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:48:46.999+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:48:46.998+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:48:47.149+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:48:47.445+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:48:47.444+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:48:47.788+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:48:47.786+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:48:48.069+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.107 seconds
[2022-12-16T13:48:58.251+0000] {processor.py:154} INFO - Started process (PID=2483) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:48:58.279+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:48:58.283+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:48:58.282+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:48:58.379+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:48:58.630+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:48:58.629+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:48:58.801+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:48:58.800+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:48:58.977+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.742 seconds
[2022-12-16T13:49:09.446+0000] {processor.py:154} INFO - Started process (PID=2493) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:49:09.449+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:49:09.458+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:49:09.457+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:49:09.597+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:49:10.200+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:49:10.199+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:49:10.386+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:49:10.375+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:49:10.541+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.123 seconds
[2022-12-16T13:49:20.839+0000] {processor.py:154} INFO - Started process (PID=2503) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:49:20.856+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:49:20.871+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:49:20.870+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:49:21.053+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:49:21.860+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:49:21.853+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:49:22.052+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:49:22.050+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:49:22.188+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.388 seconds
[2022-12-16T13:49:32.834+0000] {processor.py:154} INFO - Started process (PID=2520) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:49:32.898+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:49:32.912+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:49:32.902+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:49:33.247+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:49:35.078+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:49:35.077+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:49:35.337+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:49:35.336+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:49:35.552+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.830 seconds
[2022-12-16T13:49:46.123+0000] {processor.py:154} INFO - Started process (PID=2531) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:49:46.127+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:49:46.132+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:49:46.131+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:49:46.219+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:49:46.948+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:49:46.947+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:49:47.130+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:49:47.129+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:49:47.276+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.168 seconds
[2022-12-16T13:49:57.670+0000] {processor.py:154} INFO - Started process (PID=2541) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:49:57.675+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:49:57.682+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:49:57.678+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:49:57.800+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:49:58.525+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:49:58.524+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:49:58.762+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:49:58.761+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:49:58.960+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.311 seconds
[2022-12-16T13:50:09.355+0000] {processor.py:154} INFO - Started process (PID=2551) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:50:09.361+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:50:09.370+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:50:09.369+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:50:09.522+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:50:10.124+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:50:10.123+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:50:10.306+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:50:10.306+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:50:10.490+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.170 seconds
[2022-12-16T13:50:20.774+0000] {processor.py:154} INFO - Started process (PID=2570) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:50:20.778+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:50:20.786+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:50:20.785+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:50:20.991+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:50:21.768+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:50:21.767+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:50:22.060+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:50:22.059+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:50:22.203+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.454 seconds
[2022-12-16T13:50:32.847+0000] {processor.py:154} INFO - Started process (PID=2580) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:50:32.854+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:50:32.863+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:50:32.862+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:50:32.997+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:50:33.752+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:50:33.751+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:50:34.145+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:50:34.144+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:50:34.445+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.624 seconds
[2022-12-16T13:50:44.770+0000] {processor.py:154} INFO - Started process (PID=2590) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:50:44.775+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:50:44.779+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:50:44.778+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:50:44.891+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:50:45.311+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:50:45.310+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:50:45.582+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:50:45.581+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:50:45.760+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.007 seconds
[2022-12-16T13:50:56.211+0000] {processor.py:154} INFO - Started process (PID=2600) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:50:56.217+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:50:56.221+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:50:56.220+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:50:56.376+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:50:56.962+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:50:56.961+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:50:57.437+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:50:57.436+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:50:57.591+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.413 seconds
[2022-12-16T13:51:08.203+0000] {processor.py:154} INFO - Started process (PID=2618) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:51:08.261+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:51:08.289+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:51:08.288+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:51:08.850+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:51:10.222+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T13:51:21.150+0000] {processor.py:154} INFO - Started process (PID=2628) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:51:21.161+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:51:21.177+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:51:21.176+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:51:21.556+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:51:22.285+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:51:22.284+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:51:22.455+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:51:22.454+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:51:22.647+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.580 seconds
[2022-12-16T13:51:33.190+0000] {processor.py:154} INFO - Started process (PID=2638) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:51:33.199+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:51:33.204+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:51:33.203+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:51:33.332+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:51:34.977+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T13:51:45.431+0000] {processor.py:154} INFO - Started process (PID=2648) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:51:45.435+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:51:45.451+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:51:45.442+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:51:45.562+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:51:46.548+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:51:46.542+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:51:46.914+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:51:46.913+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:51:47.636+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.221 seconds
[2022-12-16T13:51:58.498+0000] {processor.py:154} INFO - Started process (PID=2666) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:51:58.502+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:51:58.515+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:51:58.513+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:51:58.737+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:51:59.611+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:51:59.611+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:52:00.385+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:52:00.384+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:52:00.828+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.397 seconds
[2022-12-16T13:52:11.048+0000] {processor.py:154} INFO - Started process (PID=2676) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:52:11.052+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:52:11.061+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:52:11.060+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:52:11.205+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:52:12.221+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:52:12.220+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:52:12.718+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:52:12.714+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:52:13.097+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.090 seconds
[2022-12-16T13:52:23.549+0000] {processor.py:154} INFO - Started process (PID=2686) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:52:23.556+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:52:23.566+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:52:23.565+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:52:23.725+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:52:25.088+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:52:25.087+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:52:25.355+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:52:25.354+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:52:25.488+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.962 seconds
[2022-12-16T13:52:35.886+0000] {processor.py:154} INFO - Started process (PID=2702) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:52:35.898+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:52:35.906+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:52:35.905+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:52:36.082+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:52:36.874+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:52:36.873+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:52:37.047+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:52:37.046+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:52:37.288+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.477 seconds
[2022-12-16T13:52:47.927+0000] {processor.py:154} INFO - Started process (PID=2713) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:52:47.932+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:52:47.936+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:52:47.935+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:52:48.057+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:52:49.658+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:52:49.656+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:52:50.121+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:52:50.116+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:52:50.330+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.441 seconds
[2022-12-16T13:53:00.605+0000] {processor.py:154} INFO - Started process (PID=2723) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:53:00.648+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:53:00.653+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:53:00.652+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:53:00.802+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:53:02.009+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:53:02.008+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:53:02.503+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:53:02.502+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:53:02.865+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.276 seconds
[2022-12-16T13:53:13.240+0000] {processor.py:154} INFO - Started process (PID=2733) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:53:13.248+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:53:13.257+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:53:13.256+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:53:13.383+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:53:13.623+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:53:13.622+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:53:13.786+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:53:13.785+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:53:14.019+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.798 seconds
[2022-12-16T13:53:24.671+0000] {processor.py:154} INFO - Started process (PID=2751) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:53:24.681+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:53:24.691+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:53:24.689+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:53:24.940+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:53:26.365+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T13:53:37.501+0000] {processor.py:154} INFO - Started process (PID=2761) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:53:37.532+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:53:37.538+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:53:37.537+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:53:37.768+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:53:38.434+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:53:38.422+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:53:39.096+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:53:39.090+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:53:39.616+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.177 seconds
[2022-12-16T13:53:50.165+0000] {processor.py:154} INFO - Started process (PID=2771) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:53:50.186+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:53:50.194+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:53:50.192+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:53:50.388+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:53:50.722+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:53:50.721+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:53:50.946+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:53:50.945+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:53:51.186+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.042 seconds
[2022-12-16T13:54:01.838+0000] {processor.py:154} INFO - Started process (PID=2789) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:54:01.857+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:54:01.861+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:54:01.860+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:54:02.017+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:54:03.156+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:54:03.155+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:54:03.515+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:54:03.514+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:54:03.829+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.049 seconds
[2022-12-16T13:54:14.455+0000] {processor.py:154} INFO - Started process (PID=2800) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:54:14.458+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:54:14.463+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:54:14.462+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:54:14.550+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:54:14.775+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:54:14.774+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:54:14.924+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:54:14.923+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:54:15.070+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.633 seconds
[2022-12-16T13:54:25.470+0000] {processor.py:154} INFO - Started process (PID=2810) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:54:25.522+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:54:25.528+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:54:25.526+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:54:25.645+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:54:25.932+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:54:25.931+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:54:26.191+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:54:26.190+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:54:26.364+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.910 seconds
[2022-12-16T13:54:36.820+0000] {processor.py:154} INFO - Started process (PID=2820) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:54:36.829+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:54:36.840+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:54:36.839+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:54:37.081+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:54:38.197+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:54:38.196+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:54:38.564+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:54:38.563+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:54:39.024+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.254 seconds
[2022-12-16T13:54:49.527+0000] {processor.py:154} INFO - Started process (PID=2838) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:54:49.608+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:54:49.621+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:54:49.620+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:54:50.069+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:54:51.353+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:54:51.352+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:54:51.733+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:54:51.732+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:54:51.945+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.533 seconds
[2022-12-16T13:55:02.416+0000] {processor.py:154} INFO - Started process (PID=2848) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:55:02.428+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:55:02.439+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:55:02.436+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:55:02.647+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:55:03.473+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:55:03.472+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:55:03.885+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:55:03.884+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:55:04.200+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.804 seconds
[2022-12-16T13:55:14.403+0000] {processor.py:154} INFO - Started process (PID=2858) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:55:14.408+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:55:14.414+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:55:14.412+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:55:14.556+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:55:15.011+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:55:15.010+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:55:15.153+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:55:15.152+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:55:15.317+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.960 seconds
[2022-12-16T13:55:25.692+0000] {processor.py:154} INFO - Started process (PID=2868) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:55:25.743+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:55:25.748+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:55:25.747+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:55:25.882+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:55:26.290+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:55:26.289+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:55:26.544+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:55:26.543+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:55:27.268+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.594 seconds
[2022-12-16T13:55:37.765+0000] {processor.py:154} INFO - Started process (PID=2885) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:55:37.814+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:55:37.827+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:55:37.818+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:55:38.298+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:55:39.048+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:55:39.047+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:55:39.581+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:55:39.580+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:55:39.924+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.277 seconds
[2022-12-16T13:55:50.653+0000] {processor.py:154} INFO - Started process (PID=2895) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:55:50.666+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:55:50.675+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:55:50.674+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:55:50.850+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:55:52.370+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T13:56:02.979+0000] {processor.py:154} INFO - Started process (PID=2905) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:56:02.984+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:56:02.989+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:56:02.988+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:56:03.105+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:56:03.446+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:56:03.445+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:56:03.613+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:56:03.613+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:56:03.816+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.853 seconds
[2022-12-16T13:56:14.137+0000] {processor.py:154} INFO - Started process (PID=2922) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:56:14.146+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:56:14.151+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:56:14.149+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:56:14.334+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:56:16.268+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:56:16.259+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:56:16.752+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:56:16.751+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:56:17.045+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.938 seconds
[2022-12-16T13:56:27.591+0000] {processor.py:154} INFO - Started process (PID=2935) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:56:27.596+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:56:27.600+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:56:27.599+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:56:27.732+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:56:29.559+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T13:56:40.020+0000] {processor.py:154} INFO - Started process (PID=2946) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:56:40.080+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:56:40.086+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:56:40.085+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:56:40.350+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:56:40.742+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:56:40.741+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:56:40.908+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:56:40.906+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:56:41.244+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.245 seconds
[2022-12-16T13:56:51.718+0000] {processor.py:154} INFO - Started process (PID=2953) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:56:51.749+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:56:51.754+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:56:51.752+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:56:51.846+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:56:52.085+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:56:52.084+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:56:52.241+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:56:52.240+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:56:52.729+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.033 seconds
[2022-12-16T13:57:03.920+0000] {processor.py:154} INFO - Started process (PID=2970) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:57:03.969+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:57:03.983+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:57:03.977+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:57:04.439+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:57:07.226+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:57:07.225+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:57:07.818+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:57:07.817+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:57:07.986+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 4.113 seconds
[2022-12-16T13:57:18.516+0000] {processor.py:154} INFO - Started process (PID=2983) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:57:18.525+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:57:18.538+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:57:18.537+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:57:18.691+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:57:19.390+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:57:19.386+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:57:19.607+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:57:19.601+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:57:19.887+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.436 seconds
[2022-12-16T13:57:30.811+0000] {processor.py:154} INFO - Started process (PID=2993) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:57:30.836+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:57:30.841+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:57:30.840+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:57:31.002+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:57:31.535+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:57:31.534+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:57:31.851+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:57:31.850+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:57:32.138+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.365 seconds
[2022-12-16T13:57:42.582+0000] {processor.py:154} INFO - Started process (PID=3009) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:57:42.594+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:57:42.602+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:57:42.601+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:57:42.836+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:57:43.497+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:57:43.495+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:57:43.792+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:57:43.791+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:57:44.041+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.486 seconds
[2022-12-16T13:57:54.515+0000] {processor.py:154} INFO - Started process (PID=3021) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:57:54.521+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:57:54.524+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:57:54.524+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:57:54.624+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:57:55.067+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:57:55.066+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:57:55.208+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:57:55.207+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:57:55.323+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.827 seconds
[2022-12-16T13:58:05.659+0000] {processor.py:154} INFO - Started process (PID=3031) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:58:05.663+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:58:05.668+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:58:05.667+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:58:05.870+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:58:06.198+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:58:06.197+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:58:06.376+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:58:06.375+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:58:06.533+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.890 seconds
[2022-12-16T13:58:16.750+0000] {processor.py:154} INFO - Started process (PID=3041) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:58:16.754+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:58:16.759+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:58:16.757+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:58:16.882+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:58:17.195+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:58:17.194+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:58:17.366+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:58:17.365+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:58:17.634+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.902 seconds
[2022-12-16T13:58:28.101+0000] {processor.py:154} INFO - Started process (PID=3059) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:58:28.117+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:58:28.130+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:58:28.129+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:58:28.480+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:58:28.869+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:58:28.868+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:58:29.227+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:58:29.226+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:58:29.813+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.755 seconds
[2022-12-16T13:58:40.688+0000] {processor.py:154} INFO - Started process (PID=3069) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:58:40.693+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:58:40.712+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:58:40.706+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:58:40.877+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:58:41.210+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:58:41.209+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:58:41.350+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:58:41.349+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:58:41.499+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.868 seconds
[2022-12-16T13:58:51.837+0000] {processor.py:154} INFO - Started process (PID=3076) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:58:51.868+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:58:51.873+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:58:51.872+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:58:52.005+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:58:52.385+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:58:52.375+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:58:52.982+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:58:52.981+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:58:53.263+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.455 seconds
[2022-12-16T13:59:04.246+0000] {processor.py:154} INFO - Started process (PID=3095) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:59:04.267+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:59:04.271+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:59:04.270+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:59:04.610+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:59:05.700+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:59:05.699+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:59:06.048+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:59:06.047+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:59:06.290+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.093 seconds
[2022-12-16T13:59:16.687+0000] {processor.py:154} INFO - Started process (PID=3107) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:59:16.692+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:59:16.696+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:59:16.695+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:59:16.825+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:59:17.166+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:59:17.165+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:59:17.337+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:59:17.336+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:59:17.514+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.847 seconds
[2022-12-16T13:59:27.834+0000] {processor.py:154} INFO - Started process (PID=3117) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:59:27.839+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:59:27.843+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:59:27.842+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:59:28.055+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:59:29.206+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T13:59:39.577+0000] {processor.py:154} INFO - Started process (PID=3127) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:59:39.581+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:59:39.592+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:59:39.591+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:59:39.706+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:59:40.469+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:59:40.468+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:59:40.625+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:59:40.624+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:59:40.757+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.198 seconds
[2022-12-16T13:59:51.227+0000] {processor.py:154} INFO - Started process (PID=3145) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:59:51.261+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T13:59:51.274+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:59:51.272+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:59:51.763+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T13:59:52.864+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:59:52.862+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T13:59:53.217+0000] {logging_mixin.py:137} INFO - [2022-12-16T13:59:53.216+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T13:59:53.566+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.446 seconds
[2022-12-16T14:00:04.060+0000] {processor.py:154} INFO - Started process (PID=3155) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:00:04.076+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:00:04.091+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:00:04.089+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:00:04.485+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:00:05.530+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:00:05.528+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:00:05.892+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:00:05.887+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:00:06.282+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.317 seconds
[2022-12-16T14:00:16.494+0000] {processor.py:154} INFO - Started process (PID=3165) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:00:16.505+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:00:16.527+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:00:16.525+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:00:16.701+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:00:17.120+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:00:17.119+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:00:17.809+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:00:17.808+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:00:18.350+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.912 seconds
[2022-12-16T14:00:28.900+0000] {processor.py:154} INFO - Started process (PID=3174) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:00:28.922+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:00:28.953+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:00:28.940+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:00:29.269+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:00:29.689+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:00:29.689+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:00:29.964+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:00:29.963+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:00:30.129+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.449 seconds
[2022-12-16T14:00:40.421+0000] {processor.py:154} INFO - Started process (PID=3191) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:00:40.504+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:00:40.514+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:00:40.512+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:00:40.731+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:00:41.305+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:00:41.298+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:00:41.489+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:00:41.488+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:00:41.610+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.226 seconds
[2022-12-16T14:00:51.792+0000] {processor.py:154} INFO - Started process (PID=3200) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:00:51.795+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:00:51.804+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:00:51.803+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:00:51.969+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:00:52.746+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T14:01:03.228+0000] {processor.py:154} INFO - Started process (PID=3210) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:01:03.278+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:01:03.283+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:01:03.282+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:01:03.378+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:01:04.194+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:01:04.193+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:01:04.368+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:01:04.367+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:01:04.595+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.385 seconds
[2022-12-16T14:01:15.956+0000] {processor.py:154} INFO - Started process (PID=3231) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:01:15.964+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:01:15.981+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:01:15.967+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:01:16.382+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:01:17.394+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T14:01:28.375+0000] {processor.py:154} INFO - Started process (PID=3241) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:01:28.381+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:01:28.397+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:01:28.396+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:01:28.802+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:01:29.469+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:01:29.468+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:01:29.713+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:01:29.712+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:01:29.890+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.570 seconds
[2022-12-16T14:01:40.097+0000] {processor.py:154} INFO - Started process (PID=3248) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:01:40.101+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:01:40.106+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:01:40.105+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:01:40.226+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:01:40.516+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:01:40.514+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:01:40.704+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:01:40.703+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:01:40.943+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.864 seconds
[2022-12-16T14:01:51.996+0000] {processor.py:154} INFO - Started process (PID=3258) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:01:52.003+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:01:52.016+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:01:52.007+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:01:52.208+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:01:52.558+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:01:52.557+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:01:52.808+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:01:52.807+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:01:53.250+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.301 seconds
[2022-12-16T14:02:03.654+0000] {processor.py:154} INFO - Started process (PID=3275) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:02:03.679+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:02:03.694+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:02:03.692+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:02:04.008+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:02:04.685+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:02:04.684+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:02:05.128+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:02:05.123+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:02:05.688+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.102 seconds
[2022-12-16T14:02:16.586+0000] {processor.py:154} INFO - Started process (PID=3285) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:02:16.618+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:02:16.622+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:02:16.621+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:02:16.900+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:02:17.834+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:02:17.833+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:02:18.335+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:02:18.328+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:02:18.719+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.266 seconds
[2022-12-16T14:02:29.160+0000] {processor.py:154} INFO - Started process (PID=3295) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:02:29.164+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:02:29.168+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:02:29.167+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:02:29.263+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:02:29.811+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:02:29.810+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:02:30.002+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:02:30.001+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:02:30.308+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.164 seconds
[2022-12-16T14:02:40.881+0000] {processor.py:154} INFO - Started process (PID=3305) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:02:40.941+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:02:40.945+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:02:40.944+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:02:41.091+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:02:41.639+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:02:41.638+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:02:42.347+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:02:42.346+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:02:42.705+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.878 seconds
[2022-12-16T14:02:53.426+0000] {processor.py:154} INFO - Started process (PID=3326) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:02:53.516+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:02:53.526+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:02:53.525+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:02:53.868+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:02:55.529+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T14:03:06.213+0000] {processor.py:154} INFO - Started process (PID=3333) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:03:06.219+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:03:06.227+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:03:06.226+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:03:06.353+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:03:08.311+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:03:08.306+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:03:08.803+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:03:08.802+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:03:09.212+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.021 seconds
[2022-12-16T14:03:19.821+0000] {processor.py:154} INFO - Started process (PID=3346) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:03:19.856+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:03:19.871+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:03:19.870+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:03:20.221+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:03:22.083+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:03:22.082+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:03:22.366+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:03:22.365+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:03:22.645+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.843 seconds
[2022-12-16T14:03:33.755+0000] {processor.py:154} INFO - Started process (PID=3364) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:03:33.766+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:03:33.792+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:03:33.782+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:03:34.079+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:03:34.680+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:03:34.679+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:03:35.095+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:03:35.094+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:03:35.519+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.950 seconds
[2022-12-16T14:03:46.134+0000] {processor.py:154} INFO - Started process (PID=3374) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:03:46.138+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:03:46.148+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:03:46.143+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:03:46.297+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:03:46.886+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:03:46.885+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:03:47.026+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:03:47.025+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:03:47.166+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.050 seconds
[2022-12-16T14:03:57.584+0000] {processor.py:154} INFO - Started process (PID=3381) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:03:57.611+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:03:57.616+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:03:57.615+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:03:57.783+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:03:58.239+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:03:58.237+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:03:58.555+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:03:58.554+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:03:59.040+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.495 seconds
[2022-12-16T14:04:09.434+0000] {processor.py:154} INFO - Started process (PID=3391) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:04:09.466+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:04:09.479+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:04:09.474+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:04:09.755+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:04:10.178+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:04:10.177+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:04:10.398+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:04:10.397+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:04:10.596+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.207 seconds
[2022-12-16T14:04:21.284+0000] {processor.py:154} INFO - Started process (PID=3409) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:04:21.467+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:04:21.484+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:04:21.477+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:04:21.830+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:04:22.602+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:04:22.601+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:04:22.888+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:04:22.887+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:04:23.429+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.376 seconds
[2022-12-16T14:04:34.133+0000] {processor.py:154} INFO - Started process (PID=3419) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:04:34.177+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:04:34.182+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:04:34.180+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:04:34.319+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:04:34.686+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:04:34.685+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:04:34.963+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:04:34.962+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:04:35.138+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.042 seconds
[2022-12-16T14:04:45.463+0000] {processor.py:154} INFO - Started process (PID=3429) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:04:45.478+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:04:45.494+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:04:45.493+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:04:45.615+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:04:47.124+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T14:04:57.896+0000] {processor.py:154} INFO - Started process (PID=3442) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:04:57.973+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:04:58.007+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:04:57.999+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:04:58.339+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:04:58.700+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:04:58.699+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:04:58.881+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:04:58.880+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:04:59.128+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.247 seconds
[2022-12-16T14:05:09.828+0000] {processor.py:154} INFO - Started process (PID=3459) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:05:09.836+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:05:09.846+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:05:09.845+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:05:10.140+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:05:11.022+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:05:11.021+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:05:11.337+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:05:11.336+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:05:11.620+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.875 seconds
[2022-12-16T14:05:21.970+0000] {processor.py:154} INFO - Started process (PID=3466) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:05:22.023+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:05:22.027+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:05:22.026+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:05:22.181+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:05:23.534+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:05:23.533+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:05:24.129+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:05:24.128+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:05:24.820+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.894 seconds
[2022-12-16T14:05:35.534+0000] {processor.py:154} INFO - Started process (PID=3478) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:05:35.537+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:05:35.541+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:05:35.540+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:05:35.726+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:05:36.655+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:05:36.654+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:05:36.855+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:05:36.854+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:05:37.059+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.572 seconds
[2022-12-16T14:05:47.619+0000] {processor.py:154} INFO - Started process (PID=3498) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:05:47.650+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:05:47.654+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:05:47.653+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:05:48.042+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:05:49.153+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:05:49.152+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:05:49.858+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:05:49.846+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:05:50.420+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.852 seconds
[2022-12-16T14:06:01.027+0000] {processor.py:154} INFO - Started process (PID=3508) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:06:01.067+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:06:01.077+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:06:01.070+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:06:01.416+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:06:03.295+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:06:03.294+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:06:03.518+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:06:03.517+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:06:03.678+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.670 seconds
[2022-12-16T14:06:14.194+0000] {processor.py:154} INFO - Started process (PID=3518) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:06:14.209+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:06:14.230+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:06:14.222+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:06:14.570+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:06:15.202+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:06:15.201+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:06:15.382+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:06:15.381+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:06:15.532+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.380 seconds
[2022-12-16T14:06:25.861+0000] {processor.py:154} INFO - Started process (PID=3528) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:06:25.895+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:06:25.900+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:06:25.899+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:06:26.266+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:06:27.518+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:06:27.517+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:06:27.819+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:06:27.818+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:06:28.337+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.512 seconds
[2022-12-16T14:06:39.110+0000] {processor.py:154} INFO - Started process (PID=3548) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:06:39.125+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:06:39.142+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:06:39.134+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:06:39.398+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:06:40.726+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:06:40.725+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:06:41.019+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:06:41.018+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:06:41.357+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.319 seconds
[2022-12-16T14:06:51.808+0000] {processor.py:154} INFO - Started process (PID=3558) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:06:51.813+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:06:51.817+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:06:51.816+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:06:51.910+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:06:52.180+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:06:52.178+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:06:52.347+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:06:52.346+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:06:52.584+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.798 seconds
[2022-12-16T14:07:02.954+0000] {processor.py:154} INFO - Started process (PID=3568) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:07:02.961+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:07:02.973+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:07:02.972+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:07:03.205+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:07:03.500+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:07:03.499+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:07:03.709+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:07:03.708+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:07:03.915+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.000 seconds
[2022-12-16T14:07:14.534+0000] {processor.py:154} INFO - Started process (PID=3586) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:07:14.564+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:07:14.593+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:07:14.592+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:07:15.044+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:07:15.630+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:07:15.629+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:07:15.908+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:07:15.907+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:07:16.161+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.722 seconds
[2022-12-16T14:07:26.545+0000] {processor.py:154} INFO - Started process (PID=3596) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:07:26.560+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:07:26.565+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:07:26.564+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:07:26.705+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:07:27.282+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:07:27.281+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:07:27.721+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:07:27.720+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:07:27.890+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.423 seconds
[2022-12-16T14:07:38.367+0000] {processor.py:154} INFO - Started process (PID=3606) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:07:38.378+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:07:38.392+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:07:38.391+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:07:38.812+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:07:40.536+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:07:40.533+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:07:40.770+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:07:40.769+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:07:41.002+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.716 seconds
[2022-12-16T14:07:51.341+0000] {processor.py:154} INFO - Started process (PID=3616) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:07:51.382+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:07:51.398+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:07:51.397+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:07:51.550+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:07:51.783+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:07:51.782+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:07:51.925+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:07:51.924+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:07:52.065+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.765 seconds
[2022-12-16T14:08:02.314+0000] {processor.py:154} INFO - Started process (PID=3635) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:08:02.375+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:08:02.384+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:08:02.382+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:08:02.672+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:08:03.526+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:08:03.525+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:08:04.099+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:08:04.086+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:08:04.389+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.107 seconds
[2022-12-16T14:08:14.641+0000] {processor.py:154} INFO - Started process (PID=3645) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:08:14.646+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:08:14.650+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:08:14.649+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:08:14.774+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:08:15.236+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:08:15.235+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:08:15.533+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:08:15.531+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:08:16.056+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.439 seconds
[2022-12-16T14:08:26.528+0000] {processor.py:154} INFO - Started process (PID=3655) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:08:26.607+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:08:26.612+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:08:26.611+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:08:26.712+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:08:27.024+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:08:27.023+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:08:27.283+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:08:27.282+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:08:27.450+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.946 seconds
[2022-12-16T14:08:37.977+0000] {processor.py:154} INFO - Started process (PID=3665) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:08:38.000+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:08:38.033+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:08:38.031+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:08:38.519+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:08:38.990+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:08:38.989+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:08:39.334+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:08:39.325+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:08:39.579+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.632 seconds
[2022-12-16T14:08:50.326+0000] {processor.py:154} INFO - Started process (PID=3682) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:08:50.334+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:08:50.348+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:08:50.341+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:08:50.563+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:08:51.266+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:08:51.265+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:08:51.703+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:08:51.702+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:08:51.944+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.649 seconds
[2022-12-16T14:09:02.472+0000] {processor.py:154} INFO - Started process (PID=3692) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:09:02.477+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:09:02.482+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:09:02.481+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:09:02.605+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:09:04.214+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T14:09:14.691+0000] {processor.py:154} INFO - Started process (PID=3702) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:09:14.717+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:09:14.725+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:09:14.724+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:09:14.862+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:09:16.478+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:09:16.477+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:09:16.746+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:09:16.745+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:09:16.863+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.194 seconds
[2022-12-16T14:09:27.397+0000] {processor.py:154} INFO - Started process (PID=3718) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:09:27.417+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:09:27.422+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:09:27.421+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:09:27.660+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:09:28.810+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:09:28.809+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:09:29.102+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:09:29.101+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:09:29.420+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.062 seconds
[2022-12-16T14:09:40.364+0000] {processor.py:154} INFO - Started process (PID=3729) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:09:40.373+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:09:40.382+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:09:40.381+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:09:40.720+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:09:41.206+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:09:41.205+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:09:41.394+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:09:41.393+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:09:41.502+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.223 seconds
[2022-12-16T14:09:51.778+0000] {processor.py:154} INFO - Started process (PID=3739) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:09:51.788+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:09:51.797+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:09:51.796+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:09:52.115+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:09:52.679+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:09:52.678+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:09:52.876+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:09:52.875+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:09:53.087+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.362 seconds
[2022-12-16T14:10:03.377+0000] {processor.py:154} INFO - Started process (PID=3749) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:10:03.407+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:10:03.418+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:10:03.410+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:10:03.752+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:10:04.114+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:10:04.113+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:10:04.263+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:10:04.262+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:10:04.426+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.113 seconds
[2022-12-16T14:10:14.898+0000] {processor.py:154} INFO - Started process (PID=3767) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:10:14.937+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:10:14.957+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:10:14.949+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:10:15.112+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:10:16.196+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:10:16.182+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:10:16.557+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:10:16.556+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:10:16.766+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.900 seconds
[2022-12-16T14:10:27.222+0000] {processor.py:154} INFO - Started process (PID=3777) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:10:27.226+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:10:27.232+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:10:27.231+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:10:27.350+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:10:28.423+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T14:10:38.725+0000] {processor.py:154} INFO - Started process (PID=3787) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:10:38.762+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:10:38.766+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:10:38.765+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:10:38.938+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:10:39.835+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:10:39.833+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:10:40.210+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:10:40.209+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:10:40.564+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.881 seconds
[2022-12-16T14:10:51.016+0000] {processor.py:154} INFO - Started process (PID=3797) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:10:51.072+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:10:51.084+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:10:51.079+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:10:51.320+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:10:52.505+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T14:11:03.435+0000] {processor.py:154} INFO - Started process (PID=3815) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:11:03.445+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:11:03.457+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:11:03.456+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:11:03.800+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:11:04.723+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:11:04.722+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:11:05.027+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:11:05.026+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:11:05.199+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.813 seconds
[2022-12-16T14:11:15.906+0000] {processor.py:154} INFO - Started process (PID=3825) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:11:15.909+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:11:15.914+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:11:15.913+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:11:16.005+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:11:16.214+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:11:16.213+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:11:16.344+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:11:16.343+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:11:16.497+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.605 seconds
[2022-12-16T14:11:26.746+0000] {processor.py:154} INFO - Started process (PID=3835) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:11:26.775+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:11:26.782+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:11:26.781+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:11:26.871+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:11:27.222+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:11:27.221+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:11:27.350+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:11:27.349+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:11:27.474+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.742 seconds
[2022-12-16T14:12:11.800+0000] {processor.py:154} INFO - Started process (PID=169) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:12:11.812+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:12:11.828+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:12:11.826+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:12:12.117+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:12:13.045+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:12:13.036+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:12:13.231+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:12:13.230+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:12:13.354+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.579 seconds
[2022-12-16T14:12:23.735+0000] {processor.py:154} INFO - Started process (PID=179) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:12:23.785+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:12:23.790+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:12:23.789+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:12:23.874+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:12:24.193+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:12:24.192+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:12:24.348+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:12:24.347+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:12:24.488+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.767 seconds
[2022-12-16T14:12:35.176+0000] {processor.py:154} INFO - Started process (PID=189) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:12:35.180+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:12:35.187+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:12:35.183+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:12:35.269+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:12:35.700+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:12:35.699+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:12:35.970+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:12:35.961+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:12:36.086+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.923 seconds
[2022-12-16T14:12:46.341+0000] {processor.py:154} INFO - Started process (PID=199) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:12:46.368+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:12:46.373+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:12:46.370+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:12:46.465+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:12:47.423+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:12:47.422+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:12:47.654+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:12:47.653+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:12:48.212+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.888 seconds
[2022-12-16T14:12:59.356+0000] {processor.py:154} INFO - Started process (PID=220) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:12:59.360+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:12:59.364+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:12:59.363+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:12:59.509+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:13:00.756+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:13:00.755+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:13:01.127+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:13:01.118+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:13:01.754+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.425 seconds
[2022-12-16T14:13:12.249+0000] {processor.py:154} INFO - Started process (PID=230) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:13:12.269+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:13:12.273+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:13:12.272+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:13:12.500+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:13:13.271+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:13:13.270+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:13:13.410+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:13:13.409+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:13:13.538+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.318 seconds
[2022-12-16T14:13:23.821+0000] {processor.py:154} INFO - Started process (PID=240) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:13:23.843+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:13:23.848+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:13:23.847+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:13:23.942+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:13:24.518+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:13:24.517+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:13:24.797+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:13:24.796+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:13:24.928+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.121 seconds
[2022-12-16T14:13:35.409+0000] {processor.py:154} INFO - Started process (PID=258) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:13:35.444+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:13:35.448+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:13:35.447+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:13:35.681+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:13:36.293+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:13:36.292+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:13:36.521+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:13:36.520+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:13:36.655+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.313 seconds
[2022-12-16T14:13:47.424+0000] {processor.py:154} INFO - Started process (PID=268) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:13:47.459+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:13:47.463+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:13:47.462+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:13:47.635+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:13:48.338+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:13:48.337+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:13:48.775+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:13:48.774+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:13:49.239+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.858 seconds
[2022-12-16T14:13:59.382+0000] {processor.py:154} INFO - Started process (PID=278) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:13:59.421+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:13:59.434+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:13:59.424+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:13:59.552+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:14:01.264+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T14:14:11.653+0000] {processor.py:154} INFO - Started process (PID=290) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:14:11.660+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:14:11.664+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:14:11.663+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:14:11.755+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:14:11.983+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:14:11.982+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:14:12.169+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:14:12.168+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:14:12.456+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.818 seconds
[2022-12-16T14:14:22.726+0000] {processor.py:154} INFO - Started process (PID=307) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:14:22.732+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:14:22.741+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:14:22.737+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:14:22.974+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:14:23.601+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:14:23.601+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:14:23.752+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:14:23.752+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:14:23.958+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.253 seconds
[2022-12-16T14:14:34.289+0000] {processor.py:154} INFO - Started process (PID=315) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:14:34.292+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:14:34.297+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:14:34.295+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:14:34.498+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:14:34.833+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:14:34.832+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:14:34.974+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:14:34.973+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:14:35.108+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.833 seconds
[2022-12-16T14:14:45.477+0000] {processor.py:154} INFO - Started process (PID=327) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:14:45.504+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:14:45.516+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:14:45.515+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:14:45.707+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:14:45.965+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:14:45.964+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:14:46.094+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:14:46.093+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:14:46.235+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.794 seconds
[2022-12-16T14:14:56.717+0000] {processor.py:154} INFO - Started process (PID=343) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:14:56.728+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:14:56.732+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:14:56.731+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:14:56.863+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:14:57.301+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:14:57.300+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:14:57.493+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:14:57.492+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:14:57.663+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.977 seconds
[2022-12-16T14:15:08.134+0000] {processor.py:154} INFO - Started process (PID=353) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:15:08.153+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:15:08.156+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:15:08.155+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:15:08.266+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:15:08.518+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:15:08.517+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:15:08.686+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:15:08.685+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:15:08.940+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.824 seconds
[2022-12-16T14:15:19.673+0000] {processor.py:154} INFO - Started process (PID=363) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:15:19.724+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:15:19.728+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:15:19.726+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:15:19.996+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:15:20.367+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:15:20.366+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:15:20.519+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:15:20.518+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:15:20.637+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.035 seconds
[2022-12-16T14:15:30.975+0000] {processor.py:154} INFO - Started process (PID=373) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:15:30.978+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:15:30.983+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:15:30.982+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:15:31.082+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:15:31.377+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:15:31.377+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:15:31.509+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:15:31.508+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:15:31.645+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.684 seconds
[2022-12-16T14:15:42.407+0000] {processor.py:154} INFO - Started process (PID=388) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:15:42.464+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:15:42.472+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:15:42.471+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:15:42.610+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:15:42.938+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:15:42.937+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:15:43.182+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:15:43.181+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:15:43.334+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.968 seconds
[2022-12-16T14:15:53.658+0000] {processor.py:154} INFO - Started process (PID=398) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:15:53.764+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:15:53.772+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:15:53.771+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:15:53.918+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:15:54.524+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:15:54.523+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:15:54.690+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:15:54.689+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:15:54.850+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.219 seconds
[2022-12-16T14:16:05.147+0000] {processor.py:154} INFO - Started process (PID=408) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:16:05.177+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:16:05.181+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:16:05.180+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:16:05.369+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:16:05.684+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:16:05.680+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:16:05.876+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:16:05.875+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:16:06.021+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.888 seconds
[2022-12-16T14:16:16.334+0000] {processor.py:154} INFO - Started process (PID=424) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:16:16.388+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:16:16.397+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:16:16.396+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:16:16.875+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:16:17.870+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:16:17.869+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:16:18.055+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:16:18.054+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:16:18.247+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.929 seconds
[2022-12-16T14:16:28.393+0000] {processor.py:154} INFO - Started process (PID=435) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:16:28.443+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:16:28.448+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:16:28.446+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:16:28.604+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:16:29.217+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:16:29.216+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:16:29.356+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:16:29.356+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:16:29.483+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.104 seconds
[2022-12-16T14:16:39.750+0000] {processor.py:154} INFO - Started process (PID=445) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:16:39.754+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:16:39.758+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:16:39.757+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:16:39.863+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:16:40.289+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:16:40.288+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:16:40.521+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:16:40.520+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:16:40.706+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.970 seconds
[2022-12-16T14:16:50.870+0000] {processor.py:154} INFO - Started process (PID=455) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:16:50.927+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:16:50.932+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:16:50.930+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:16:51.030+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:16:51.241+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:16:51.241+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:16:51.376+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:16:51.374+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:16:51.525+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.670 seconds
[2022-12-16T14:17:02.478+0000] {processor.py:154} INFO - Started process (PID=473) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:17:02.488+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:17:02.501+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:17:02.500+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:17:02.664+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:17:02.905+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:17:02.904+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:17:03.133+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:17:03.132+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:17:03.368+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.920 seconds
[2022-12-16T14:17:13.967+0000] {processor.py:154} INFO - Started process (PID=483) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:17:14.018+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:17:14.023+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:17:14.022+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:17:14.115+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:17:14.320+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:17:14.319+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:17:14.455+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:17:14.454+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:17:14.583+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.627 seconds
[2022-12-16T14:17:24.874+0000] {processor.py:154} INFO - Started process (PID=493) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:17:24.878+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:17:24.882+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:17:24.881+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:17:24.969+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:17:25.170+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:17:25.169+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:17:25.309+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:17:25.308+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:17:25.432+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.572 seconds
[2022-12-16T14:17:36.175+0000] {processor.py:154} INFO - Started process (PID=503) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:17:36.214+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:17:36.218+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:17:36.217+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:17:36.301+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:17:36.686+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:17:36.685+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:17:36.831+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:17:36.830+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:17:36.965+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.805 seconds
[2022-12-16T14:17:47.990+0000] {processor.py:154} INFO - Started process (PID=521) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:17:47.994+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:17:48.016+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:17:48.002+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:17:48.148+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:17:48.434+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:17:48.433+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:17:48.606+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:17:48.605+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:17:48.849+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.875 seconds
[2022-12-16T14:17:59.320+0000] {processor.py:154} INFO - Started process (PID=531) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:17:59.383+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:17:59.388+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:17:59.387+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:17:59.474+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:17:59.771+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:17:59.770+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:17:59.933+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:17:59.931+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:18:00.103+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.807 seconds
[2022-12-16T14:18:10.419+0000] {processor.py:154} INFO - Started process (PID=541) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:18:10.432+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:18:10.444+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:18:10.437+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:18:10.562+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:18:10.798+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:18:10.797+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:18:10.931+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:18:10.930+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:18:11.042+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.642 seconds
[2022-12-16T14:18:21.513+0000] {processor.py:154} INFO - Started process (PID=559) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:18:21.534+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:18:21.561+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:18:21.560+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:18:21.701+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:18:22.324+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:18:22.323+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:18:22.489+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:18:22.488+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:18:22.681+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.194 seconds
[2022-12-16T14:18:33.105+0000] {processor.py:154} INFO - Started process (PID=569) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:18:33.127+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:18:33.132+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:18:33.131+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:18:33.226+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:18:33.968+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:18:33.967+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:18:34.096+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:18:34.095+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:18:34.230+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.139 seconds
[2022-12-16T14:18:44.430+0000] {processor.py:154} INFO - Started process (PID=579) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:18:44.451+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:18:44.456+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:18:44.455+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:18:44.538+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:18:44.746+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:18:44.745+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:18:44.875+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:18:44.874+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:18:45.017+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.602 seconds
[2022-12-16T14:18:55.736+0000] {processor.py:154} INFO - Started process (PID=589) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:18:55.765+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:18:55.769+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:18:55.768+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:18:55.853+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:18:56.067+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:18:56.066+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:18:56.195+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:18:56.194+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:18:56.328+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.606 seconds
[2022-12-16T14:19:07.125+0000] {processor.py:154} INFO - Started process (PID=607) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:19:07.156+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:19:07.160+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:19:07.159+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:19:07.430+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:19:07.866+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:19:07.865+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:19:08.035+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:19:08.034+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:19:08.182+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.136 seconds
[2022-12-16T14:19:18.600+0000] {processor.py:154} INFO - Started process (PID=617) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:19:18.635+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:19:18.644+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:19:18.638+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:19:18.744+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:19:18.953+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:19:18.952+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:19:19.080+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:19:19.079+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:19:19.230+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.670 seconds
[2022-12-16T14:19:29.519+0000] {processor.py:154} INFO - Started process (PID=627) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:19:29.539+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:19:29.545+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:19:29.544+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:19:29.656+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:19:30.048+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:19:30.046+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:19:30.174+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:19:30.173+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:19:30.310+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.813 seconds
[2022-12-16T14:19:40.774+0000] {processor.py:154} INFO - Started process (PID=637) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:19:40.812+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:19:40.825+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:19:40.824+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:19:40.959+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:19:41.261+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:19:41.261+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:19:41.389+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:19:41.388+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:19:41.515+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.769 seconds
[2022-12-16T14:19:52.143+0000] {processor.py:154} INFO - Started process (PID=655) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:19:52.287+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:19:52.339+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:19:52.338+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:19:52.487+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:19:53.336+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T14:20:03.757+0000] {processor.py:154} INFO - Started process (PID=665) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:20:03.811+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:20:03.815+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:20:03.814+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:20:04.242+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:20:05.374+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:20:05.373+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:20:05.594+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:20:05.593+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:20:05.913+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.189 seconds
[2022-12-16T14:20:16.254+0000] {processor.py:154} INFO - Started process (PID=675) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:20:16.259+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:20:16.266+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:20:16.266+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:20:16.385+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:20:16.611+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:20:16.610+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:20:16.753+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:20:16.752+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:20:16.883+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.645 seconds
[2022-12-16T14:20:27.670+0000] {processor.py:154} INFO - Started process (PID=692) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:20:27.740+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:20:27.744+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:20:27.743+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:20:28.131+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:20:28.745+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:20:28.744+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:20:28.967+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:20:28.966+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:20:29.235+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.637 seconds
[2022-12-16T14:20:39.971+0000] {processor.py:154} INFO - Started process (PID=702) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:20:39.989+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:20:39.993+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:20:39.992+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:20:40.085+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:20:41.364+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T14:20:51.561+0000] {processor.py:154} INFO - Started process (PID=712) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:20:51.611+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:20:51.616+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:20:51.615+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:20:51.710+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:20:51.924+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:20:51.922+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:20:52.185+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:20:52.184+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:20:52.340+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.793 seconds
[2022-12-16T14:21:02.641+0000] {processor.py:154} INFO - Started process (PID=722) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:21:02.692+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:21:02.696+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:21:02.695+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:21:02.977+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:21:04.613+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:21:04.611+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:21:04.782+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:21:04.781+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:21:04.957+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.330 seconds
[2022-12-16T14:21:15.577+0000] {processor.py:154} INFO - Started process (PID=740) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:21:15.584+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:21:15.588+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:21:15.587+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:21:15.703+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:21:16.106+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:21:16.105+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:21:16.242+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:21:16.241+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:21:16.362+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.823 seconds
[2022-12-16T14:21:26.589+0000] {processor.py:154} INFO - Started process (PID=750) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:21:26.592+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:21:26.596+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:21:26.596+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:21:26.678+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:21:26.996+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:21:26.995+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:21:27.135+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:21:27.134+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:21:27.254+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.681 seconds
[2022-12-16T14:21:37.526+0000] {processor.py:154} INFO - Started process (PID=760) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:21:37.529+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:21:37.534+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:21:37.533+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:21:37.627+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:21:37.854+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:21:37.854+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:21:37.980+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:21:37.979+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:21:38.091+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.578 seconds
[2022-12-16T14:21:48.446+0000] {processor.py:154} INFO - Started process (PID=777) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:21:48.489+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:21:48.510+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:21:48.498+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:21:48.714+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:21:49.427+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:21:49.426+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:21:49.571+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:21:49.570+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:21:49.721+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.307 seconds
[2022-12-16T14:22:00.757+0000] {processor.py:154} INFO - Started process (PID=788) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:22:00.760+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:22:00.764+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:22:00.763+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:22:00.849+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:22:02.117+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:22:02.116+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:22:02.274+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:22:02.273+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:22:02.383+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.641 seconds
[2022-12-16T14:22:12.708+0000] {processor.py:154} INFO - Started process (PID=798) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:22:12.898+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:22:12.905+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:22:12.904+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:22:13.001+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:22:13.707+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:22:13.703+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:22:13.835+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:22:13.834+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:22:13.947+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.255 seconds
[2022-12-16T14:22:24.325+0000] {processor.py:154} INFO - Started process (PID=808) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:22:24.355+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:22:24.359+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:22:24.358+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:22:24.452+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:22:24.681+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:22:24.680+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:22:24.812+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:22:24.811+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:22:24.941+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.631 seconds
[2022-12-16T14:22:35.824+0000] {processor.py:154} INFO - Started process (PID=825) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:22:35.832+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:22:35.844+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:22:35.843+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:22:35.973+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:22:36.748+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:22:36.747+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:22:37.005+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:22:37.004+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:22:37.185+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.399 seconds
[2022-12-16T14:22:47.601+0000] {processor.py:154} INFO - Started process (PID=835) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:22:47.629+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:22:47.637+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:22:47.636+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:22:47.751+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:22:48.933+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:22:48.932+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:22:49.073+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:22:49.071+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:22:49.573+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.988 seconds
[2022-12-16T14:22:59.825+0000] {processor.py:154} INFO - Started process (PID=845) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:22:59.845+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:22:59.852+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:22:59.851+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:22:59.943+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:23:01.451+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:23:01.450+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:23:01.581+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:23:01.580+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:23:01.719+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.908 seconds
[2022-12-16T14:23:12.282+0000] {processor.py:154} INFO - Started process (PID=862) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:23:12.286+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:23:12.302+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:23:12.300+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:23:12.493+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:23:13.139+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:23:13.138+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:23:13.381+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:23:13.380+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:23:13.772+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.507 seconds
[2022-12-16T14:23:24.035+0000] {processor.py:154} INFO - Started process (PID=872) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:23:24.044+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:23:24.048+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:23:24.047+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:23:24.140+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:23:24.371+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:23:24.370+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:23:24.517+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:23:24.516+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:23:24.656+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.635 seconds
[2022-12-16T14:23:34.963+0000] {processor.py:154} INFO - Started process (PID=882) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:23:34.967+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:23:34.979+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:23:34.974+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:23:35.108+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:23:35.346+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:23:35.345+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:23:35.586+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:23:35.585+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:23:35.691+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.744 seconds
[2022-12-16T14:23:46.538+0000] {processor.py:154} INFO - Started process (PID=892) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:23:46.705+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:23:46.710+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:23:46.709+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:23:46.792+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:23:47.361+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:23:47.360+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:23:47.556+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:23:47.556+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:23:47.688+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.164 seconds
[2022-12-16T14:23:58.061+0000] {processor.py:154} INFO - Started process (PID=910) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:23:58.073+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:23:58.086+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:23:58.085+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:23:58.318+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:23:58.722+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:23:58.721+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:23:58.998+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:23:58.997+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:23:59.228+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.202 seconds
[2022-12-16T14:24:09.602+0000] {processor.py:154} INFO - Started process (PID=920) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:24:09.631+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:24:09.639+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:24:09.638+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:24:09.723+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:24:09.935+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:24:09.934+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:24:10.092+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:24:10.091+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:24:10.233+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.645 seconds
[2022-12-16T14:24:20.610+0000] {processor.py:154} INFO - Started process (PID=930) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:24:20.667+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:24:20.671+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:24:20.670+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:24:20.779+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:24:21.049+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:24:21.048+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:24:21.197+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:24:21.196+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:24:21.306+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.709 seconds
[2022-12-16T14:24:31.937+0000] {processor.py:154} INFO - Started process (PID=946) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:24:32.036+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:24:32.045+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:24:32.044+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:24:32.274+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:24:32.962+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:24:32.961+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:24:33.635+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:24:33.633+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:24:33.961+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.040 seconds
[2022-12-16T14:24:44.303+0000] {processor.py:154} INFO - Started process (PID=957) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:24:44.341+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:24:44.350+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:24:44.348+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:24:44.451+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:24:45.037+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:24:45.036+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:24:45.214+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:24:45.213+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:24:45.352+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.068 seconds
[2022-12-16T14:24:55.653+0000] {processor.py:154} INFO - Started process (PID=967) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:24:55.657+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:24:55.661+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:24:55.660+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:24:55.751+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:24:56.030+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:24:56.029+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:24:56.183+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:24:56.182+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:24:56.294+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.666 seconds
[2022-12-16T14:25:06.577+0000] {processor.py:154} INFO - Started process (PID=977) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:25:06.607+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:25:06.612+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:25:06.611+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:25:06.696+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:25:07.424+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:25:07.423+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:25:07.555+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:25:07.554+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:25:07.697+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.134 seconds
[2022-12-16T14:25:18.195+0000] {processor.py:154} INFO - Started process (PID=994) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:25:18.241+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:25:18.256+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:25:18.251+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:25:18.468+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:25:19.178+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:25:19.176+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:25:19.516+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:25:19.515+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:25:19.946+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.778 seconds
[2022-12-16T14:25:30.180+0000] {processor.py:154} INFO - Started process (PID=1004) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:25:30.230+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:25:30.235+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:25:30.234+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:25:30.324+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:25:30.829+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:25:30.828+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:25:30.962+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:25:30.961+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:25:31.075+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.909 seconds
[2022-12-16T14:25:41.435+0000] {processor.py:154} INFO - Started process (PID=1014) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:25:41.477+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:25:41.481+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:25:41.480+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:25:41.571+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:25:42.760+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T14:25:53.681+0000] {processor.py:154} INFO - Started process (PID=1024) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:25:53.710+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:25:53.714+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:25:53.713+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:25:53.796+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:25:54.033+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:25:54.032+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:25:54.169+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:25:54.168+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:25:54.319+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.652 seconds
[2022-12-16T14:26:04.852+0000] {processor.py:154} INFO - Started process (PID=1041) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:26:04.880+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:26:04.885+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:26:04.883+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:26:04.975+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:26:05.259+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:26:05.258+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:26:05.503+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:26:05.502+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:26:05.619+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.781 seconds
[2022-12-16T14:26:16.306+0000] {processor.py:154} INFO - Started process (PID=1051) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:26:16.358+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:26:16.363+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:26:16.361+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:26:16.464+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:26:16.743+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:26:16.742+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:26:16.874+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:26:16.873+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:26:16.989+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.697 seconds
[2022-12-16T14:26:27.322+0000] {processor.py:154} INFO - Started process (PID=1061) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:26:27.326+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:26:27.330+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:26:27.329+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:26:27.426+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:26:27.634+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:26:27.633+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:26:27.760+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:26:27.759+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:26:27.908+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.601 seconds
[2022-12-16T14:26:38.340+0000] {processor.py:154} INFO - Started process (PID=1080) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:26:38.344+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:26:38.348+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:26:38.347+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:26:38.483+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:26:40.025+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:26:40.024+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:26:40.210+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:26:40.209+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:26:40.398+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.082 seconds
[2022-12-16T14:26:50.788+0000] {processor.py:154} INFO - Started process (PID=1090) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:26:50.792+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:26:50.796+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:26:50.795+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:26:50.877+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:26:51.106+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:26:51.105+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:26:51.271+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:26:51.270+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:26:51.410+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.639 seconds
[2022-12-16T14:27:01.815+0000] {processor.py:154} INFO - Started process (PID=1100) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:27:01.822+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:27:01.827+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:27:01.825+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:27:01.964+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:27:02.710+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:27:02.699+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:27:02.889+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:27:02.888+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:27:03.005+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.213 seconds
[2022-12-16T14:27:13.428+0000] {processor.py:154} INFO - Started process (PID=1110) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:27:13.458+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:27:13.462+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:27:13.461+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:27:13.545+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:27:15.191+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T14:27:25.909+0000] {processor.py:154} INFO - Started process (PID=1129) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:27:25.948+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:27:25.953+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:27:25.952+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:27:26.149+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:27:26.736+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:27:26.734+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:27:26.967+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:27:26.966+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:27:27.337+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.460 seconds
[2022-12-16T14:27:37.776+0000] {processor.py:154} INFO - Started process (PID=1139) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:27:37.779+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:27:37.783+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:27:37.782+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:27:37.863+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:27:38.070+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:27:38.069+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:27:38.197+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:27:38.196+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:27:38.332+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.570 seconds
[2022-12-16T14:27:48.973+0000] {processor.py:154} INFO - Started process (PID=1149) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:27:48.980+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:27:48.984+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:27:48.983+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:27:49.072+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:27:49.997+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T14:28:00.466+0000] {processor.py:154} INFO - Started process (PID=1166) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:28:00.502+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:28:00.518+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:28:00.517+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:28:00.621+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:28:00.872+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:28:00.871+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:28:01.026+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:28:01.026+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:28:01.172+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.733 seconds
[2022-12-16T14:28:11.922+0000] {processor.py:154} INFO - Started process (PID=1176) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:28:11.926+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:28:11.930+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:28:11.929+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:28:12.015+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:28:12.228+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:28:12.227+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:28:12.394+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:28:12.394+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:28:12.505+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.597 seconds
[2022-12-16T14:28:22.831+0000] {processor.py:154} INFO - Started process (PID=1186) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:28:22.839+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:28:22.845+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:28:22.843+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:28:23.029+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:28:23.371+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:28:23.370+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:28:23.535+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:28:23.534+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:28:23.652+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.844 seconds
[2022-12-16T14:28:34.022+0000] {processor.py:154} INFO - Started process (PID=1196) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:28:34.057+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:28:34.061+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:28:34.060+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:28:34.145+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:28:34.379+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:28:34.378+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:28:34.521+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:28:34.520+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:28:34.634+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.628 seconds
[2022-12-16T14:28:45.023+0000] {processor.py:154} INFO - Started process (PID=1215) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:28:45.027+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:28:45.043+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:28:45.030+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:28:45.197+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:28:45.482+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:28:45.481+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:28:45.804+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:28:45.803+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:28:46.042+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.058 seconds
[2022-12-16T14:28:56.235+0000] {processor.py:154} INFO - Started process (PID=1225) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:28:56.286+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:28:56.290+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:28:56.289+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:28:56.378+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:28:56.597+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:28:56.596+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:28:56.784+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:28:56.783+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:28:56.921+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.700 seconds
[2022-12-16T14:29:07.215+0000] {processor.py:154} INFO - Started process (PID=1235) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:29:07.218+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:29:07.222+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:29:07.222+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:29:07.327+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:29:07.615+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:29:07.614+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:29:07.752+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:29:07.751+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:29:07.887+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.686 seconds
[2022-12-16T14:29:18.881+0000] {processor.py:154} INFO - Started process (PID=1245) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:29:18.902+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:29:18.915+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:29:18.909+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:29:19.009+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:29:20.012+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T14:29:30.569+0000] {processor.py:154} INFO - Started process (PID=1264) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:29:30.669+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:29:30.674+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:29:30.673+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:29:30.800+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:29:31.065+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:29:31.064+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:29:31.390+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:29:31.389+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:29:31.709+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.187 seconds
[2022-12-16T14:29:42.749+0000] {processor.py:154} INFO - Started process (PID=1274) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:29:42.753+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:29:42.757+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:29:42.756+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:29:43.078+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:29:43.530+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:29:43.529+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:29:43.683+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:29:43.682+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:29:43.813+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.079 seconds
[2022-12-16T14:29:54.105+0000] {processor.py:154} INFO - Started process (PID=1284) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:29:54.127+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:29:54.131+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:29:54.130+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:29:54.220+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:29:54.439+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:29:54.438+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:29:54.607+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:29:54.606+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:29:54.799+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.709 seconds
[2022-12-16T14:30:05.085+0000] {processor.py:154} INFO - Started process (PID=1301) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:30:05.860+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:30:05.864+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:30:05.863+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:30:06.046+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:30:07.230+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:30:07.229+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:30:07.464+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:30:07.463+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:30:07.684+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.636 seconds
[2022-12-16T14:30:17.950+0000] {processor.py:154} INFO - Started process (PID=1311) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:30:17.953+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:30:17.957+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:30:17.956+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:30:18.042+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:30:18.246+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:30:18.245+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:30:18.385+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:30:18.385+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:30:18.530+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.595 seconds
[2022-12-16T14:30:28.839+0000] {processor.py:154} INFO - Started process (PID=1321) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:30:28.853+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:30:28.861+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:30:28.860+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:30:28.966+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:30:29.222+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:30:29.221+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:30:29.378+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:30:29.377+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:30:29.515+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.691 seconds
[2022-12-16T14:30:39.709+0000] {processor.py:154} INFO - Started process (PID=1331) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:30:39.713+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:30:39.718+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:30:39.717+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:30:39.805+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:30:40.028+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:30:40.027+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:30:40.185+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:30:40.184+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:30:40.299+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.605 seconds
[2022-12-16T14:30:51.137+0000] {processor.py:154} INFO - Started process (PID=1350) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:30:51.142+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:30:51.146+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:30:51.145+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:30:51.269+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:30:52.406+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:30:52.406+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:30:52.535+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:30:52.534+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:30:52.689+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.566 seconds
[2022-12-16T14:31:03.052+0000] {processor.py:154} INFO - Started process (PID=1360) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:31:03.055+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:31:03.060+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:31:03.059+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:31:03.144+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:31:04.498+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T14:31:15.502+0000] {processor.py:154} INFO - Started process (PID=1370) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:31:15.506+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:31:15.510+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:31:15.509+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:31:15.610+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:31:16.724+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T14:31:26.930+0000] {processor.py:154} INFO - Started process (PID=1388) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:31:26.934+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:31:26.945+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:31:26.940+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:31:27.051+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:31:27.303+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:31:27.301+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:31:27.459+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:31:27.458+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:31:27.601+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.683 seconds
[2022-12-16T14:31:38.052+0000] {processor.py:154} INFO - Started process (PID=1398) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:31:38.066+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:31:38.070+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:31:38.069+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:31:38.174+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:31:38.719+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:31:38.718+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:31:38.860+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:31:38.859+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:31:39.049+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.014 seconds
[2022-12-16T14:31:49.310+0000] {processor.py:154} INFO - Started process (PID=1408) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:31:49.332+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:31:49.336+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:31:49.335+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:31:49.418+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:31:49.900+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:31:49.900+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:31:50.023+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:31:50.022+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:31:50.186+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.889 seconds
[2022-12-16T14:32:00.498+0000] {processor.py:154} INFO - Started process (PID=1418) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:32:00.512+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:32:00.520+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:32:00.514+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:32:00.684+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:32:01.826+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T14:32:12.365+0000] {processor.py:154} INFO - Started process (PID=1436) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:32:12.413+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:32:12.418+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:32:12.417+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:32:12.501+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:32:14.052+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:32:14.051+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:32:14.210+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:32:14.209+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:32:14.317+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.968 seconds
[2022-12-16T14:32:24.639+0000] {processor.py:154} INFO - Started process (PID=1446) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:32:24.678+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:32:24.686+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:32:24.685+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:32:24.773+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:32:25.162+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:32:25.161+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:32:25.297+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:32:25.297+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:32:25.432+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.806 seconds
[2022-12-16T14:32:35.559+0000] {processor.py:154} INFO - Started process (PID=1456) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:32:35.562+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:32:35.567+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:32:35.565+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:32:35.653+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:32:36.326+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:32:36.325+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:32:36.453+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:32:36.452+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:32:36.633+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.088 seconds
[2022-12-16T14:32:47.209+0000] {processor.py:154} INFO - Started process (PID=1474) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:32:47.216+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:32:47.220+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:32:47.219+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:32:47.337+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:32:47.702+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:32:47.701+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:32:47.952+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:32:47.951+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:32:48.228+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.069 seconds
[2022-12-16T14:32:58.730+0000] {processor.py:154} INFO - Started process (PID=1484) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:32:58.787+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:32:58.791+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:32:58.790+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:32:58.907+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:32:59.119+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:32:59.118+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:32:59.266+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:32:59.265+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:32:59.370+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.658 seconds
[2022-12-16T14:33:09.622+0000] {processor.py:154} INFO - Started process (PID=1494) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:33:09.625+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:33:09.633+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:33:09.632+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:33:09.763+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:33:09.979+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:33:09.978+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:33:10.133+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:33:10.132+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:33:10.275+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.668 seconds
[2022-12-16T14:33:20.584+0000] {processor.py:154} INFO - Started process (PID=1504) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:33:20.588+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:33:20.592+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:33:20.591+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:33:20.680+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:33:20.969+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:33:20.969+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:33:21.110+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:33:21.109+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:33:21.223+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.653 seconds
[2022-12-16T14:33:31.818+0000] {processor.py:154} INFO - Started process (PID=1523) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:33:31.882+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:33:31.887+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:33:31.886+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:33:32.028+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:33:32.868+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:33:32.867+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:33:33.035+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:33:33.034+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:33:33.186+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.418 seconds
[2022-12-16T14:33:43.577+0000] {processor.py:154} INFO - Started process (PID=1533) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:33:43.606+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:33:43.612+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:33:43.610+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:33:43.720+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:33:44.532+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:33:44.531+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:33:44.690+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:33:44.689+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:33:44.948+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.403 seconds
[2022-12-16T14:33:55.717+0000] {processor.py:154} INFO - Started process (PID=1543) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:33:55.728+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:33:55.732+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:33:55.731+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:33:55.817+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:33:56.331+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:33:56.330+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:33:56.468+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:33:56.467+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:33:56.603+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.900 seconds
[2022-12-16T14:34:07.317+0000] {processor.py:154} INFO - Started process (PID=1562) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:34:07.340+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:34:07.350+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:34:07.349+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:34:07.452+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:34:09.038+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T14:34:19.859+0000] {processor.py:154} INFO - Started process (PID=1572) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:34:19.916+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:34:19.924+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:34:19.923+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:34:20.004+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:34:20.229+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:34:20.228+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:34:20.380+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:34:20.379+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:34:20.501+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.655 seconds
[2022-12-16T14:34:30.769+0000] {processor.py:154} INFO - Started process (PID=1582) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:34:30.853+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:34:30.858+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:34:30.857+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:34:30.955+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:34:32.119+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T14:34:42.507+0000] {processor.py:154} INFO - Started process (PID=1592) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:34:42.520+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:34:42.525+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:34:42.524+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:34:42.627+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:34:42.887+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:34:42.886+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:34:43.020+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:34:43.019+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:34:43.164+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.672 seconds
[2022-12-16T14:34:53.952+0000] {processor.py:154} INFO - Started process (PID=1611) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:34:53.956+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:34:53.961+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:34:53.960+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:34:54.158+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:34:54.533+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:34:54.532+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:34:54.794+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:34:54.794+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:34:55.090+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.163 seconds
[2022-12-16T14:35:05.537+0000] {processor.py:154} INFO - Started process (PID=1621) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:35:05.568+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:35:05.572+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:35:05.571+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:35:05.685+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:35:07.452+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:35:07.451+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:35:07.599+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:35:07.598+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:35:07.769+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.251 seconds
[2022-12-16T14:35:18.096+0000] {processor.py:154} INFO - Started process (PID=1631) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:35:18.144+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:35:18.149+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:35:18.147+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:35:18.232+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:35:19.274+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T14:35:29.712+0000] {processor.py:154} INFO - Started process (PID=1649) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:35:29.733+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:35:29.741+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:35:29.740+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:35:29.903+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:35:31.033+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:35:31.032+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:35:31.195+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:35:31.194+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:35:31.358+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.685 seconds
[2022-12-16T14:35:41.605+0000] {processor.py:154} INFO - Started process (PID=1660) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:35:41.610+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:35:41.614+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:35:41.613+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:35:41.756+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:35:42.006+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:35:42.005+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:35:42.234+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:35:42.233+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:35:42.390+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.804 seconds
[2022-12-16T14:35:52.752+0000] {processor.py:154} INFO - Started process (PID=1670) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:35:52.784+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:35:52.788+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:35:52.786+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:35:52.879+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:35:53.384+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:35:53.383+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:35:53.532+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:35:53.531+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:35:53.709+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.972 seconds
[2022-12-16T14:36:04.059+0000] {processor.py:154} INFO - Started process (PID=1680) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:36:04.063+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:36:04.071+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:36:04.070+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:36:04.198+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:36:04.686+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:36:04.685+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:36:04.820+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:36:04.819+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:36:04.949+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.916 seconds
[2022-12-16T14:36:15.395+0000] {processor.py:154} INFO - Started process (PID=1697) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:36:15.418+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:36:15.421+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:36:15.420+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:36:15.551+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:36:16.142+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:36:16.141+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:36:16.405+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:36:16.405+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:36:16.631+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.255 seconds
[2022-12-16T14:36:27.077+0000] {processor.py:154} INFO - Started process (PID=1707) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:36:27.094+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:36:27.102+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:36:27.101+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:36:27.195+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:36:27.446+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:36:27.445+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:36:27.603+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:36:27.602+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:36:27.743+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.680 seconds
[2022-12-16T14:36:38.238+0000] {processor.py:154} INFO - Started process (PID=1717) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:36:38.277+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:36:38.281+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:36:38.280+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:36:38.384+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:36:38.685+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:36:38.684+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:36:38.921+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:36:38.920+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:36:39.129+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.963 seconds
[2022-12-16T14:36:49.510+0000] {processor.py:154} INFO - Started process (PID=1727) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:36:49.546+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:36:49.559+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:36:49.554+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:36:49.908+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:36:50.342+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:36:50.341+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:36:50.579+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:36:50.578+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:36:50.730+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.271 seconds
[2022-12-16T14:37:01.486+0000] {processor.py:154} INFO - Started process (PID=1744) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:37:01.549+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:37:01.599+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:37:01.578+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:37:02.164+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:37:03.284+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T14:37:13.664+0000] {processor.py:154} INFO - Started process (PID=1754) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:37:13.690+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:37:13.694+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:37:13.693+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:37:13.781+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:37:14.016+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:37:14.015+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:37:14.184+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:37:14.183+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:37:14.312+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.660 seconds
[2022-12-16T14:37:24.534+0000] {processor.py:154} INFO - Started process (PID=1764) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:37:24.594+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:37:24.599+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:37:24.597+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:37:24.704+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:37:25.009+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:37:25.008+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:37:25.146+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:37:25.145+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:37:25.972+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.452 seconds
[2022-12-16T14:37:36.201+0000] {processor.py:154} INFO - Started process (PID=1774) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:37:36.211+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:37:36.215+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:37:36.214+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:37:36.344+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:37:37.582+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:37:37.581+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:37:38.385+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:37:38.384+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:37:38.590+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.418 seconds
[2022-12-16T14:37:49.224+0000] {processor.py:154} INFO - Started process (PID=1792) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:37:49.233+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:37:49.237+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:37:49.236+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:37:49.513+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:37:50.603+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:37:50.589+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:37:51.183+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:37:51.182+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:37:51.506+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.354 seconds
[2022-12-16T14:38:01.821+0000] {processor.py:154} INFO - Started process (PID=1802) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:38:01.844+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:38:01.848+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:38:01.847+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:38:02.001+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:38:02.270+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:38:02.269+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:38:02.670+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:38:02.669+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:38:02.842+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.063 seconds
[2022-12-16T14:38:12.979+0000] {processor.py:154} INFO - Started process (PID=1812) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:38:13.002+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:38:13.007+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:38:13.006+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:38:13.094+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:38:13.636+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:38:13.636+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:38:13.803+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:38:13.802+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:38:14.220+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.255 seconds
[2022-12-16T14:38:24.637+0000] {processor.py:154} INFO - Started process (PID=1830) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:38:24.653+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:38:24.657+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:38:24.656+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:38:24.772+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:38:25.061+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:38:25.060+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:38:25.475+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:38:25.474+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:38:25.932+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.314 seconds
[2022-12-16T14:38:36.418+0000] {processor.py:154} INFO - Started process (PID=1840) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:38:36.437+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:38:36.441+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:38:36.441+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:38:36.599+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:38:36.972+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:38:36.968+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:38:37.296+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:38:37.295+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:38:37.552+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.158 seconds
[2022-12-16T14:38:47.963+0000] {processor.py:154} INFO - Started process (PID=1850) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:38:47.983+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:38:47.987+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:38:47.986+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:38:48.076+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:38:48.683+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:38:48.682+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:38:49.023+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:38:49.022+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:38:49.355+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.409 seconds
[2022-12-16T14:39:00.404+0000] {processor.py:154} INFO - Started process (PID=1860) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:39:00.407+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:39:00.412+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:39:00.411+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:39:00.508+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:39:01.116+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:39:01.115+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:39:01.254+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:39:01.253+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:39:01.434+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.044 seconds
[2022-12-16T14:39:12.078+0000] {processor.py:154} INFO - Started process (PID=1878) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:39:12.115+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:39:12.127+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:39:12.122+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:39:12.355+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:39:12.890+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:39:12.889+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:39:13.063+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:39:13.062+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:39:13.432+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.388 seconds
[2022-12-16T14:39:23.959+0000] {processor.py:154} INFO - Started process (PID=1888) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:39:23.963+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:39:23.976+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:39:23.976+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:39:24.133+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:39:24.504+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:39:24.503+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:39:24.637+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:39:24.636+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:39:24.843+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.899 seconds
[2022-12-16T14:39:35.369+0000] {processor.py:154} INFO - Started process (PID=1898) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:39:35.417+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:39:35.421+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:39:35.420+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:39:35.513+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:39:35.737+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:39:35.737+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:39:35.888+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:39:35.888+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:39:36.458+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.102 seconds
[2022-12-16T14:39:46.913+0000] {processor.py:154} INFO - Started process (PID=1915) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:39:46.941+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:39:46.945+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:39:46.944+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:39:47.149+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:39:47.435+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:39:47.434+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:39:47.611+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:39:47.610+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:39:48.090+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.200 seconds
[2022-12-16T14:39:58.952+0000] {processor.py:154} INFO - Started process (PID=1925) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:39:58.976+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:39:59.000+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:39:58.988+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:39:59.141+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:40:00.054+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:40:00.053+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:40:00.527+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:40:00.526+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:40:00.849+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.990 seconds
[2022-12-16T14:40:11.152+0000] {processor.py:154} INFO - Started process (PID=1935) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:40:11.156+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:40:11.161+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:40:11.160+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:40:11.256+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:40:11.661+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:40:11.660+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:40:11.837+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:40:11.836+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:40:11.987+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.849 seconds
[2022-12-16T14:40:22.529+0000] {processor.py:154} INFO - Started process (PID=1945) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:40:22.542+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:40:22.548+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:40:22.546+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:40:22.694+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:40:23.345+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:40:23.344+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:40:23.586+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:40:23.585+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:40:23.976+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.490 seconds
[2022-12-16T14:40:34.434+0000] {processor.py:154} INFO - Started process (PID=1962) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:40:34.752+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:40:34.765+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:40:34.764+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:40:34.919+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:40:35.394+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:40:35.393+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:40:35.835+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:40:35.834+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:40:36.242+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.831 seconds
[2022-12-16T14:40:46.783+0000] {processor.py:154} INFO - Started process (PID=1972) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:40:46.830+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:40:46.835+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:40:46.834+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:40:46.920+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:40:47.374+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:40:47.373+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:40:47.664+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:40:47.663+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:40:47.800+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.030 seconds
[2022-12-16T14:40:58.080+0000] {processor.py:154} INFO - Started process (PID=1982) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:40:58.084+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:40:58.088+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:40:58.087+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:40:58.175+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:40:59.086+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:40:59.085+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:40:59.422+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:40:59.421+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:40:59.628+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.563 seconds
[2022-12-16T14:41:10.132+0000] {processor.py:154} INFO - Started process (PID=2000) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:41:10.187+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:41:10.196+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:41:10.195+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:41:10.427+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:41:11.594+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:41:11.593+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:41:11.850+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:41:11.849+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:41:12.098+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.982 seconds
[2022-12-16T14:41:22.567+0000] {processor.py:154} INFO - Started process (PID=2010) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:41:22.570+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:41:22.574+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:41:22.573+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:41:22.666+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:41:22.870+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:41:22.869+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:41:23.022+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:41:23.021+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:41:23.414+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.861 seconds
[2022-12-16T14:41:33.742+0000] {processor.py:154} INFO - Started process (PID=2020) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:41:33.746+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:41:33.750+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:41:33.749+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:41:33.832+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:41:34.037+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:41:34.036+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:41:34.168+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:41:34.167+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:41:34.458+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.728 seconds
[2022-12-16T14:41:44.972+0000] {processor.py:154} INFO - Started process (PID=2030) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:41:44.983+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:41:44.988+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:41:44.986+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:41:45.095+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:41:46.332+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:41:46.331+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:41:46.610+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:41:46.609+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:41:46.846+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.889 seconds
[2022-12-16T14:41:57.417+0000] {processor.py:154} INFO - Started process (PID=2048) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:41:57.433+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:41:57.439+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:41:57.438+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:41:57.748+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:41:58.661+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:41:58.660+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:41:58.843+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:41:58.842+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:41:59.068+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.669 seconds
[2022-12-16T14:42:09.443+0000] {processor.py:154} INFO - Started process (PID=2058) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:42:09.500+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:42:09.505+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:42:09.504+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:42:09.623+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:42:10.326+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:42:10.325+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:42:10.526+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:42:10.526+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:42:10.739+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.315 seconds
[2022-12-16T14:42:13.730+0000] {processor.py:154} INFO - Started process (PID=2071) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:42:13.772+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:42:13.776+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:42:13.775+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:42:13.956+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:42:14.222+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:42:14.218+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:42:14.418+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:42:14.417+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:42:14.528+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.825 seconds
[2022-12-16T14:42:25.024+0000] {processor.py:154} INFO - Started process (PID=2081) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:42:25.041+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:42:25.046+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:42:25.044+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:42:25.152+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:42:25.295+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:42:25.294+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:42:25.418+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:42:25.417+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:42:25.543+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.542 seconds
[2022-12-16T14:42:36.348+0000] {processor.py:154} INFO - Started process (PID=2100) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:42:36.388+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:42:36.392+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:42:36.391+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:42:36.585+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:42:36.811+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:42:36.810+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:42:36.972+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:42:36.971+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:42:37.187+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.868 seconds
[2022-12-16T14:42:47.671+0000] {processor.py:154} INFO - Started process (PID=2110) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:42:47.682+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:42:47.695+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:42:47.694+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:42:47.822+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:42:48.388+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:42:48.387+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:42:48.532+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:42:48.531+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:42:48.743+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.101 seconds
[2022-12-16T14:42:59.011+0000] {processor.py:154} INFO - Started process (PID=2117) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:42:59.060+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:42:59.065+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:42:59.064+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:42:59.153+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:42:59.293+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:42:59.292+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:42:59.403+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:42:59.402+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:42:59.540+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.543 seconds
[2022-12-16T14:43:09.922+0000] {processor.py:154} INFO - Started process (PID=2133) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:43:10.005+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:43:10.021+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:43:10.020+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:43:10.171+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:43:10.348+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:43:10.347+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:43:10.608+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:43:10.606+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:43:11.014+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.134 seconds
[2022-12-16T14:43:21.676+0000] {processor.py:154} INFO - Started process (PID=2145) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:43:21.707+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:43:21.718+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:43:21.717+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:43:21.822+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:43:22.131+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:43:22.130+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:43:22.250+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:43:22.249+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:43:22.366+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.729 seconds
[2022-12-16T14:43:33.148+0000] {processor.py:154} INFO - Started process (PID=2155) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:43:33.152+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:43:33.156+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:43:33.155+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:43:33.240+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:43:33.379+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:43:33.378+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:43:33.492+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:43:33.491+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:43:33.624+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.490 seconds
[2022-12-16T14:43:43.900+0000] {processor.py:154} INFO - Started process (PID=2165) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:43:43.944+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:43:43.949+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:43:43.948+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:43:44.037+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:43:44.168+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:43:44.167+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:43:44.297+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:43:44.296+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:43:44.445+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.560 seconds
[2022-12-16T14:43:54.868+0000] {processor.py:154} INFO - Started process (PID=2183) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:43:54.871+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:43:54.883+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:43:54.878+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:43:54.997+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:43:55.154+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:43:55.153+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:43:55.298+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:43:55.297+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:43:55.436+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.581 seconds
[2022-12-16T14:44:05.904+0000] {processor.py:154} INFO - Started process (PID=2193) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:44:05.908+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:44:05.913+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:44:05.912+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:44:06.024+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:44:06.193+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:44:06.193+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:44:06.307+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:44:06.306+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:44:06.426+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.538 seconds
[2022-12-16T14:44:16.778+0000] {processor.py:154} INFO - Started process (PID=2203) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:44:16.821+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:44:16.825+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:44:16.824+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:44:16.924+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:44:17.803+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:44:17.795+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:44:18.048+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:44:18.043+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:44:18.278+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.516 seconds
[2022-12-16T14:44:28.478+0000] {processor.py:154} INFO - Started process (PID=2213) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:44:28.482+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:44:28.486+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:44:28.485+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:44:28.575+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:44:28.812+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:44:28.811+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:44:28.930+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:44:28.929+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:44:29.038+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.574 seconds
[2022-12-16T14:44:39.700+0000] {processor.py:154} INFO - Started process (PID=2231) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:44:39.720+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:44:39.740+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:44:39.739+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:44:39.951+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:44:40.202+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:44:40.201+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:44:40.364+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:44:40.363+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:44:40.589+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.942 seconds
[2022-12-16T14:44:51.048+0000] {processor.py:154} INFO - Started process (PID=2241) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:44:51.053+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:44:51.058+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:44:51.057+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:44:51.190+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:44:52.017+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T14:45:02.165+0000] {processor.py:154} INFO - Started process (PID=2251) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:45:02.169+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:45:02.176+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:45:02.175+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:45:02.265+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:45:02.475+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:45:02.474+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:45:02.613+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:45:02.613+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:45:02.735+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.584 seconds
[2022-12-16T14:45:13.317+0000] {processor.py:154} INFO - Started process (PID=2269) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:45:13.344+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:45:13.357+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:45:13.356+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:45:13.630+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:45:13.930+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:45:13.929+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:45:14.380+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:45:14.366+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:45:14.872+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.599 seconds
[2022-12-16T14:45:25.092+0000] {processor.py:154} INFO - Started process (PID=2280) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:45:25.096+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:45:25.100+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:45:25.098+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:45:25.189+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:45:25.346+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:45:25.345+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:45:25.485+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:45:25.482+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:45:25.699+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.622 seconds
[2022-12-16T14:45:35.889+0000] {processor.py:154} INFO - Started process (PID=2290) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:45:35.896+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:45:35.904+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:45:35.903+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:45:36.040+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:45:36.548+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:45:36.547+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:45:36.675+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:45:36.674+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:45:36.807+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.932 seconds
[2022-12-16T14:45:46.958+0000] {processor.py:154} INFO - Started process (PID=2300) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:45:46.962+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:45:46.966+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:45:46.965+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:45:47.053+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:45:47.638+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:45:47.636+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:45:47.859+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:45:47.858+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:45:48.033+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.089 seconds
[2022-12-16T14:45:58.466+0000] {processor.py:154} INFO - Started process (PID=2318) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:45:58.511+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:45:58.515+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:45:58.514+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:45:58.673+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:45:59.469+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T14:46:10.032+0000] {processor.py:154} INFO - Started process (PID=2328) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:46:10.077+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:46:10.081+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:46:10.080+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:46:10.226+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:46:10.425+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:46:10.424+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:46:10.751+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:46:10.750+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:46:11.019+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.003 seconds
[2022-12-16T14:46:21.446+0000] {processor.py:154} INFO - Started process (PID=2338) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:46:21.501+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:46:21.508+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:46:21.507+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:46:21.598+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:46:22.311+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:46:22.310+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:46:22.442+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:46:22.441+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:46:22.576+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.150 seconds
[2022-12-16T14:46:32.892+0000] {processor.py:154} INFO - Started process (PID=2348) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:46:32.936+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:46:32.940+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:46:32.939+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:46:33.027+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:46:33.175+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:46:33.174+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:46:33.289+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:46:33.288+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:46:33.421+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.542 seconds
[2022-12-16T14:46:44.233+0000] {processor.py:154} INFO - Started process (PID=2366) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:46:44.261+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:46:44.264+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:46:44.263+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:46:44.352+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:46:44.518+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:46:44.518+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:46:44.649+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:46:44.648+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:46:44.792+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.571 seconds
[2022-12-16T14:46:55.177+0000] {processor.py:154} INFO - Started process (PID=2376) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:46:55.253+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:46:55.258+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:46:55.257+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:46:55.355+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:46:55.528+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:46:55.527+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:46:55.644+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:46:55.643+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:46:55.751+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.588 seconds
[2022-12-16T14:47:06.115+0000] {processor.py:154} INFO - Started process (PID=2386) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:47:06.135+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:47:06.140+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:47:06.139+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:47:06.348+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:47:06.927+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T14:47:17.180+0000] {processor.py:154} INFO - Started process (PID=2404) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:47:17.205+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:47:17.213+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:47:17.212+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:47:17.352+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:47:18.835+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:47:18.834+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:47:19.273+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:47:19.272+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:47:19.688+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.540 seconds
[2022-12-16T14:47:30.103+0000] {processor.py:154} INFO - Started process (PID=2414) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:47:30.110+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:47:30.114+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:47:30.113+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:47:30.220+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:47:30.972+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:47:30.966+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:47:31.161+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:47:31.161+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:47:31.281+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.196 seconds
[2022-12-16T14:47:41.629+0000] {processor.py:154} INFO - Started process (PID=2424) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:47:41.633+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:47:41.636+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:47:41.635+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:47:41.732+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:47:42.046+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:47:42.045+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:47:42.295+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:47:42.294+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:47:42.558+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.943 seconds
[2022-12-16T14:47:52.911+0000] {processor.py:154} INFO - Started process (PID=2434) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:47:52.961+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:47:52.965+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:47:52.964+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:47:53.055+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:47:53.350+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:47:53.349+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:47:53.722+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:47:53.721+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:47:54.177+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.279 seconds
[2022-12-16T14:48:04.510+0000] {processor.py:154} INFO - Started process (PID=2451) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:48:04.534+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:48:04.549+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:48:04.542+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:48:04.726+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:48:06.156+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T14:48:16.661+0000] {processor.py:154} INFO - Started process (PID=2461) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:48:16.665+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:48:16.669+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:48:16.668+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:48:16.753+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:48:16.888+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:48:16.887+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:48:16.999+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:48:16.998+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:48:17.151+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.504 seconds
[2022-12-16T14:48:27.544+0000] {processor.py:154} INFO - Started process (PID=2471) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:48:27.548+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:48:27.552+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:48:27.551+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:48:27.641+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:48:27.886+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:48:27.885+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:48:28.017+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:48:28.016+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:48:28.140+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.611 seconds
[2022-12-16T14:48:39.240+0000] {processor.py:154} INFO - Started process (PID=2489) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:48:39.244+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:48:39.256+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:48:39.255+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:48:39.653+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:48:40.044+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:48:40.043+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:48:40.261+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:48:40.260+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:48:40.493+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.285 seconds
[2022-12-16T14:48:50.966+0000] {processor.py:154} INFO - Started process (PID=2499) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:48:50.971+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:48:50.978+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:48:50.977+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:48:51.087+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:48:51.240+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:48:51.239+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:48:51.359+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:48:51.358+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:48:51.485+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.535 seconds
[2022-12-16T14:49:01.758+0000] {processor.py:154} INFO - Started process (PID=2509) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:49:01.817+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:49:01.821+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:49:01.820+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:49:01.912+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:49:02.077+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:49:02.076+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:49:02.220+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:49:02.218+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:49:02.334+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.592 seconds
[2022-12-16T14:49:12.923+0000] {processor.py:154} INFO - Started process (PID=2519) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:49:12.927+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:49:12.931+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:49:12.930+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:49:13.016+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:49:13.150+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:49:13.149+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:49:13.268+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:49:13.267+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:49:13.405+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.495 seconds
[2022-12-16T14:49:23.935+0000] {processor.py:154} INFO - Started process (PID=2539) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:49:23.968+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:49:23.993+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:49:23.992+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:49:24.183+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:49:24.569+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:49:24.568+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:49:25.248+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:49:25.246+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:49:25.561+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.668 seconds
[2022-12-16T14:49:35.758+0000] {processor.py:154} INFO - Started process (PID=2549) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:49:35.802+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:49:35.806+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:49:35.805+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:49:35.889+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:49:36.408+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:49:36.407+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:49:36.528+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:49:36.527+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:49:36.667+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.922 seconds
[2022-12-16T14:49:46.978+0000] {processor.py:154} INFO - Started process (PID=2559) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:49:46.982+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:49:46.987+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:49:46.986+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:49:47.089+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:49:47.225+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:49:47.224+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:49:47.337+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:49:47.337+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:49:47.446+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.484 seconds
[2022-12-16T14:49:57.788+0000] {processor.py:154} INFO - Started process (PID=2569) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:49:57.791+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:49:57.795+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:49:57.794+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:49:57.876+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:49:58.366+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:49:58.365+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:49:58.486+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:49:58.485+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:49:58.826+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.056 seconds
[2022-12-16T14:50:09.259+0000] {processor.py:154} INFO - Started process (PID=2587) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:50:09.263+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:50:09.267+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:50:09.266+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:50:09.475+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:50:09.778+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:50:09.777+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:50:10.004+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:50:10.002+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:50:10.297+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.067 seconds
[2022-12-16T14:50:20.713+0000] {processor.py:154} INFO - Started process (PID=2597) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:50:20.717+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:50:20.721+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:50:20.720+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:50:20.818+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:50:20.962+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:50:20.961+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:50:21.097+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:50:21.096+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:50:21.207+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.509 seconds
[2022-12-16T14:50:31.666+0000] {processor.py:154} INFO - Started process (PID=2607) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:50:31.674+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:50:31.678+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:50:31.677+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:50:31.780+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:50:32.036+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:50:32.028+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:50:32.251+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:50:32.230+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:50:32.417+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.782 seconds
[2022-12-16T14:50:42.695+0000] {processor.py:154} INFO - Started process (PID=2623) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:50:42.704+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:50:42.708+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:50:42.707+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:50:42.812+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:50:42.965+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:50:42.964+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:50:43.094+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:50:43.093+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:50:43.306+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.626 seconds
[2022-12-16T14:50:53.891+0000] {processor.py:154} INFO - Started process (PID=2634) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:50:53.895+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:50:53.898+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:50:53.898+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:50:53.988+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:50:55.147+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:50:55.146+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:50:55.485+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:50:55.484+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:50:55.738+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.861 seconds
[2022-12-16T14:51:06.117+0000] {processor.py:154} INFO - Started process (PID=2644) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:51:06.121+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:51:06.125+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:51:06.124+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:51:06.211+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:51:06.387+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:51:06.386+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:51:06.498+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:51:06.497+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:51:06.607+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.503 seconds
[2022-12-16T14:51:16.904+0000] {processor.py:154} INFO - Started process (PID=2654) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:51:16.908+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:51:16.912+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:51:16.910+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:51:17.000+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:51:17.198+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:51:17.197+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:51:17.322+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:51:17.321+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:51:17.427+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.541 seconds
[2022-12-16T14:51:27.628+0000] {processor.py:154} INFO - Started process (PID=2672) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:51:27.632+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:51:27.641+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:51:27.640+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:51:27.866+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:51:28.078+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:51:28.077+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:51:28.314+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:51:28.312+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:51:28.458+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.844 seconds
[2022-12-16T14:51:39.088+0000] {processor.py:154} INFO - Started process (PID=2682) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:51:39.092+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:51:39.096+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:51:39.095+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:51:39.220+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:51:39.386+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:51:39.385+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:51:39.499+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:51:39.498+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:51:39.612+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.547 seconds
[2022-12-16T14:51:50.329+0000] {processor.py:154} INFO - Started process (PID=2692) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:51:50.333+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:51:50.337+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:51:50.336+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:51:50.435+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:51:50.926+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:51:50.925+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:51:51.052+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:51:51.051+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:51:51.175+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.860 seconds
[2022-12-16T14:52:01.544+0000] {processor.py:154} INFO - Started process (PID=2702) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:52:01.548+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:52:01.553+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:52:01.551+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:52:01.696+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:52:01.964+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:52:01.964+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:52:02.633+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:52:02.631+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:52:02.851+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.325 seconds
[2022-12-16T14:52:13.412+0000] {processor.py:154} INFO - Started process (PID=2720) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:52:13.448+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:52:13.452+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:52:13.451+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:52:13.552+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:52:13.777+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:52:13.776+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:52:13.987+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:52:13.974+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:52:14.244+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.860 seconds
[2022-12-16T14:52:24.465+0000] {processor.py:154} INFO - Started process (PID=2730) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:52:24.489+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:52:24.506+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:52:24.505+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:52:24.628+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:52:24.785+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:52:24.784+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:52:24.954+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:52:24.953+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:52:25.067+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.647 seconds
[2022-12-16T14:52:35.435+0000] {processor.py:154} INFO - Started process (PID=2740) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:52:35.465+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:52:35.470+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:52:35.469+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:52:35.822+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:52:36.398+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:52:36.397+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:52:36.715+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:52:36.712+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:52:37.232+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.818 seconds
[2022-12-16T14:52:47.623+0000] {processor.py:154} INFO - Started process (PID=2757) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:52:47.676+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:52:47.684+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:52:47.679+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:52:47.889+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:52:48.446+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:52:48.445+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:52:48.681+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:52:48.680+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:52:49.121+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.552 seconds
[2022-12-16T14:52:59.688+0000] {processor.py:154} INFO - Started process (PID=2768) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:52:59.691+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:52:59.696+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:52:59.695+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:52:59.815+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:53:00.020+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:53:00.014+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:53:00.350+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:53:00.349+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:53:00.566+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.893 seconds
[2022-12-16T14:53:10.999+0000] {processor.py:154} INFO - Started process (PID=2778) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:53:11.022+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:53:11.026+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:53:11.025+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:53:11.114+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:53:11.271+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:53:11.270+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:53:11.400+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:53:11.399+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:53:11.532+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.550 seconds
[2022-12-16T14:53:21.818+0000] {processor.py:154} INFO - Started process (PID=2788) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:53:21.822+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:53:21.826+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:53:21.825+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:53:21.916+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:53:22.057+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:53:22.056+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:53:22.188+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:53:22.187+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:53:22.311+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.509 seconds
[2022-12-16T14:53:32.704+0000] {processor.py:154} INFO - Started process (PID=2805) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:53:32.752+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:53:32.761+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:53:32.760+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:53:32.887+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:53:34.112+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T14:53:44.560+0000] {processor.py:154} INFO - Started process (PID=2815) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:53:44.564+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:53:44.568+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:53:44.567+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:53:44.660+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:53:44.809+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:53:44.808+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:53:44.930+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:53:44.929+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:53:45.110+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.567 seconds
[2022-12-16T14:53:55.433+0000] {processor.py:154} INFO - Started process (PID=2825) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:53:55.468+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:53:55.476+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:53:55.471+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:53:55.597+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:53:55.765+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:53:55.764+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:53:55.919+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:53:55.918+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:53:56.083+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.665 seconds
[2022-12-16T14:54:06.413+0000] {processor.py:154} INFO - Started process (PID=2835) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:54:06.423+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:54:06.427+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:54:06.426+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:54:06.529+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:54:06.966+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:54:06.965+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:54:07.101+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:54:07.100+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:54:07.239+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.844 seconds
[2022-12-16T14:54:17.790+0000] {processor.py:154} INFO - Started process (PID=2853) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:54:17.840+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:54:17.857+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:54:17.842+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:54:18.035+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:54:19.352+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T14:54:29.989+0000] {processor.py:154} INFO - Started process (PID=2863) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:54:29.992+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:54:29.997+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:54:29.996+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:54:30.086+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:54:30.379+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:54:30.378+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:54:30.538+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:54:30.537+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:54:30.650+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.676 seconds
[2022-12-16T14:54:41.107+0000] {processor.py:154} INFO - Started process (PID=2873) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:54:41.116+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:54:41.132+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:54:41.132+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:54:41.426+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:54:42.917+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:54:42.916+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:54:43.140+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:54:43.140+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:54:43.574+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.500 seconds
[2022-12-16T14:54:54.074+0000] {processor.py:154} INFO - Started process (PID=2890) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:54:54.084+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:54:54.090+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:54:54.089+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:54:54.317+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:54:54.882+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:54:54.881+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:54:55.152+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:54:55.143+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:54:55.378+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.341 seconds
[2022-12-16T14:55:05.591+0000] {processor.py:154} INFO - Started process (PID=2900) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:55:05.595+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:55:05.602+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:55:05.601+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:55:05.697+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:55:05.860+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:55:05.860+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:55:05.974+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:55:05.973+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:55:06.091+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.514 seconds
[2022-12-16T14:55:16.347+0000] {processor.py:154} INFO - Started process (PID=2910) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:55:16.400+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:55:16.409+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:55:16.408+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:55:16.491+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:55:17.237+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:55:17.236+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:55:17.358+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:55:17.357+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:55:17.494+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.161 seconds
[2022-12-16T14:55:27.800+0000] {processor.py:154} INFO - Started process (PID=2920) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:55:27.846+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:55:27.851+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:55:27.850+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:55:27.994+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:55:28.274+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:55:28.273+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:55:28.399+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:55:28.394+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:55:28.588+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.806 seconds
[2022-12-16T14:55:38.853+0000] {processor.py:154} INFO - Started process (PID=2938) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:55:38.905+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:55:38.920+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:55:38.919+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:55:39.054+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:55:39.294+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:55:39.292+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:55:39.436+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:55:39.434+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:55:39.549+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.726 seconds
[2022-12-16T14:55:49.678+0000] {processor.py:154} INFO - Started process (PID=2948) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:55:49.698+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:55:49.703+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:55:49.702+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:55:49.822+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:55:49.979+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:55:49.978+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:55:50.096+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:55:50.095+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:55:50.208+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.545 seconds
[2022-12-16T14:56:00.506+0000] {processor.py:154} INFO - Started process (PID=2958) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:56:00.515+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:56:00.523+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:56:00.522+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:56:00.629+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:56:00.769+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:56:00.768+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:56:00.881+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:56:00.880+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:56:00.991+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.501 seconds
[2022-12-16T14:56:11.440+0000] {processor.py:154} INFO - Started process (PID=2974) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:56:11.455+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:56:11.487+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:56:11.466+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:56:11.879+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:56:12.438+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:56:12.437+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:56:12.586+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:56:12.585+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:56:12.726+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.369 seconds
[2022-12-16T14:56:23.184+0000] {processor.py:154} INFO - Started process (PID=2986) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:56:23.201+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:56:23.205+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:56:23.204+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:56:23.294+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:56:24.203+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:56:24.202+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:56:24.324+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:56:24.322+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:56:24.434+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.265 seconds
[2022-12-16T14:56:34.718+0000] {processor.py:154} INFO - Started process (PID=2996) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:56:34.722+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:56:34.726+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:56:34.725+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:56:34.845+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:56:35.560+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:56:35.559+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:56:35.702+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:56:35.698+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:56:35.862+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.157 seconds
[2022-12-16T14:56:46.277+0000] {processor.py:154} INFO - Started process (PID=3006) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:56:46.281+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:56:46.285+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:56:46.284+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:56:46.404+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:56:47.360+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:56:47.358+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:56:47.647+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:56:47.641+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:56:47.821+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.577 seconds
[2022-12-16T14:56:58.325+0000] {processor.py:154} INFO - Started process (PID=3023) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:56:58.337+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:56:58.345+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:56:58.344+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:56:58.501+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:56:58.967+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:56:58.966+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:56:59.405+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:56:59.404+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:56:59.701+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.391 seconds
[2022-12-16T14:57:10.610+0000] {processor.py:154} INFO - Started process (PID=3033) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:57:10.614+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:57:10.622+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:57:10.621+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:57:10.740+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:57:12.121+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:57:12.120+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:57:12.235+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:57:12.234+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:57:12.346+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.753 seconds
[2022-12-16T14:57:22.882+0000] {processor.py:154} INFO - Started process (PID=3043) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:57:22.907+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:57:22.912+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:57:22.910+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:57:23.040+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:57:24.158+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T14:57:34.557+0000] {processor.py:154} INFO - Started process (PID=3060) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:57:34.587+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:57:34.597+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:57:34.596+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:57:34.742+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:57:34.922+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:57:34.921+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:57:35.055+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:57:35.054+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:57:35.198+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.666 seconds
[2022-12-16T14:57:45.362+0000] {processor.py:154} INFO - Started process (PID=3071) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:57:45.366+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:57:45.373+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:57:45.372+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:57:45.455+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:57:46.021+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:57:46.020+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:57:46.134+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:57:46.133+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:57:46.274+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.926 seconds
[2022-12-16T14:57:56.768+0000] {processor.py:154} INFO - Started process (PID=3081) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:57:56.783+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:57:56.787+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:57:56.786+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:57:56.949+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:57:57.816+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T14:58:08.134+0000] {processor.py:154} INFO - Started process (PID=3091) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:58:08.137+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:58:08.142+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:58:08.141+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:58:08.226+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:58:08.359+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:58:08.358+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:58:08.470+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:58:08.469+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:58:08.602+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.482 seconds
[2022-12-16T14:58:19.012+0000] {processor.py:154} INFO - Started process (PID=3110) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:58:19.021+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:58:19.029+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:58:19.028+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:58:19.164+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:58:19.875+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:58:19.874+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:58:20.094+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:58:20.094+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:58:20.230+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.233 seconds
[2022-12-16T14:58:30.552+0000] {processor.py:154} INFO - Started process (PID=3120) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:58:30.599+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:58:30.604+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:58:30.603+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:58:30.687+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:58:30.842+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:58:30.841+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:58:31.198+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:58:31.194+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:58:31.381+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.843 seconds
[2022-12-16T14:58:41.661+0000] {processor.py:154} INFO - Started process (PID=3130) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:58:41.665+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:58:41.669+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:58:41.668+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:58:41.750+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:58:42.277+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T14:58:52.970+0000] {processor.py:154} INFO - Started process (PID=3140) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:58:52.989+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:58:52.996+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:58:52.995+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:58:53.118+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:58:54.106+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T14:59:04.492+0000] {processor.py:154} INFO - Started process (PID=3157) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:59:04.496+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:59:04.500+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:59:04.499+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:59:04.679+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:59:04.870+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:59:04.869+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:59:05.142+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:59:05.141+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:59:05.332+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.875 seconds
[2022-12-16T14:59:15.693+0000] {processor.py:154} INFO - Started process (PID=3167) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:59:15.696+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:59:15.701+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:59:15.700+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:59:15.784+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:59:15.920+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:59:15.919+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:59:16.030+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:59:16.029+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:59:16.140+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.463 seconds
[2022-12-16T14:59:26.929+0000] {processor.py:154} INFO - Started process (PID=3177) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:59:26.956+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:59:26.961+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:59:26.960+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:59:27.043+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:59:27.176+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:59:27.175+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:59:27.287+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:59:27.286+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:59:27.398+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.483 seconds
[2022-12-16T14:59:37.697+0000] {processor.py:154} INFO - Started process (PID=3194) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:59:37.701+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:59:37.706+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:59:37.705+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:59:37.814+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:59:37.990+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:59:37.989+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:59:38.118+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:59:38.117+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:59:38.312+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.631 seconds
[2022-12-16T14:59:48.786+0000] {processor.py:154} INFO - Started process (PID=3204) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:59:48.789+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:59:48.794+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:59:48.793+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:59:48.899+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:59:49.070+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:59:49.069+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T14:59:49.178+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:59:49.178+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T14:59:49.298+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.527 seconds
[2022-12-16T14:59:59.651+0000] {processor.py:154} INFO - Started process (PID=3214) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:59:59.655+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T14:59:59.659+0000] {logging_mixin.py:137} INFO - [2022-12-16T14:59:59.658+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T14:59:59.750+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:00:00.609+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T15:00:11.080+0000] {processor.py:154} INFO - Started process (PID=3224) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:00:11.186+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:00:11.197+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:00:11.196+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:00:11.589+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:00:11.771+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:00:11.770+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:00:11.886+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:00:11.885+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:00:12.007+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.961 seconds
[2022-12-16T15:00:22.423+0000] {processor.py:154} INFO - Started process (PID=3243) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:00:22.458+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:00:22.462+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:00:22.461+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:00:22.633+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:00:22.934+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:00:22.933+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:00:23.091+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:00:23.090+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:00:23.216+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.809 seconds
[2022-12-16T15:00:33.712+0000] {processor.py:154} INFO - Started process (PID=3253) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:00:33.736+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:00:33.740+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:00:33.739+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:00:33.835+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:00:34.450+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:00:34.449+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:00:34.560+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:00:34.559+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:00:34.695+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.997 seconds
[2022-12-16T15:00:44.856+0000] {processor.py:154} INFO - Started process (PID=3263) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:00:44.864+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:00:44.870+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:00:44.869+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:00:44.996+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:00:45.570+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:00:45.569+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:00:45.784+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:00:45.783+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:00:45.926+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.092 seconds
[2022-12-16T15:00:56.518+0000] {processor.py:154} INFO - Started process (PID=3273) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:00:56.544+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:00:56.569+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:00:56.568+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:00:57.039+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:00:57.574+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:00:57.573+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:00:57.992+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:00:57.984+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:00:58.377+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.943 seconds
[2022-12-16T15:01:08.730+0000] {processor.py:154} INFO - Started process (PID=3291) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:01:08.772+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:01:08.781+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:01:08.780+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:01:08.929+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:01:09.732+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:01:09.731+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:01:09.853+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:01:09.852+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:01:09.961+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.247 seconds
[2022-12-16T15:01:20.286+0000] {processor.py:154} INFO - Started process (PID=3301) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:01:20.290+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:01:20.301+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:01:20.300+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:01:20.397+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:01:21.185+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T15:01:31.368+0000] {processor.py:154} INFO - Started process (PID=3311) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:01:31.399+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:01:31.403+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:01:31.402+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:01:31.568+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:01:31.861+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:01:31.860+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:01:31.980+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:01:31.979+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:01:32.702+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.353 seconds
[2022-12-16T15:01:43.170+0000] {processor.py:154} INFO - Started process (PID=3330) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:01:43.216+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:01:43.226+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:01:43.225+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:01:43.454+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:01:44.239+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:01:44.230+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:01:44.493+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:01:44.492+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:01:44.749+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.650 seconds
[2022-12-16T15:01:55.295+0000] {processor.py:154} INFO - Started process (PID=3340) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:01:55.300+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:01:55.306+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:01:55.305+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:01:55.414+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:01:56.140+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:01:56.139+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:01:56.327+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:01:56.326+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:01:56.529+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.248 seconds
[2022-12-16T15:02:06.718+0000] {processor.py:154} INFO - Started process (PID=3350) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:02:06.722+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:02:06.729+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:02:06.729+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:02:06.817+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:02:07.042+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:02:07.041+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:02:07.221+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:02:07.220+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:02:07.326+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.622 seconds
[2022-12-16T15:02:18.169+0000] {processor.py:154} INFO - Started process (PID=3360) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:02:18.182+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:02:18.191+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:02:18.190+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:02:18.527+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:02:18.945+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:02:18.944+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:02:19.155+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:02:19.154+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:02:19.314+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.198 seconds
[2022-12-16T15:02:29.969+0000] {processor.py:154} INFO - Started process (PID=3378) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:02:30.016+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:02:30.031+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:02:30.030+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:02:30.338+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:02:30.660+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:02:30.659+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:02:30.938+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:02:30.937+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:02:31.171+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.270 seconds
[2022-12-16T15:02:41.636+0000] {processor.py:154} INFO - Started process (PID=3388) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:02:41.659+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:02:41.670+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:02:41.662+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:02:41.833+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:02:42.257+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:02:42.256+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:02:42.401+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:02:42.400+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:02:42.613+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.052 seconds
[2022-12-16T15:02:53.001+0000] {processor.py:154} INFO - Started process (PID=3398) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:02:53.004+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:02:53.009+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:02:53.008+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:02:53.100+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:02:53.350+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:02:53.349+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:02:53.509+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:02:53.508+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:02:53.629+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.643 seconds
[2022-12-16T15:03:04.420+0000] {processor.py:154} INFO - Started process (PID=3415) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:03:04.424+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:03:04.444+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:03:04.443+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:03:04.887+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:03:05.213+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:03:05.202+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:03:05.466+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:03:05.465+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:03:05.702+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.430 seconds
[2022-12-16T15:03:16.432+0000] {processor.py:154} INFO - Started process (PID=3426) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:03:16.436+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:03:16.440+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:03:16.439+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:03:16.658+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:03:17.129+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:03:17.128+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:03:17.283+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:03:17.282+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:03:17.408+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.021 seconds
[2022-12-16T15:03:27.680+0000] {processor.py:154} INFO - Started process (PID=3436) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:03:27.683+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:03:27.686+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:03:27.685+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:03:27.786+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:03:28.090+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:03:28.089+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:03:28.359+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:03:28.358+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:03:28.529+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.865 seconds
[2022-12-16T15:03:38.752+0000] {processor.py:154} INFO - Started process (PID=3446) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:03:38.780+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:03:38.788+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:03:38.787+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:03:38.919+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:03:39.435+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:03:39.426+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:03:39.554+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:03:39.553+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:03:39.762+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.028 seconds
[2022-12-16T15:03:50.157+0000] {processor.py:154} INFO - Started process (PID=3464) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:03:50.162+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:03:50.184+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:03:50.171+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:03:50.314+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:03:50.774+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:03:50.772+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:03:50.960+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:03:50.959+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:03:51.149+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.008 seconds
[2022-12-16T15:04:01.437+0000] {processor.py:154} INFO - Started process (PID=3474) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:04:01.441+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:04:01.445+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:04:01.444+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:04:01.561+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:04:01.987+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:04:01.986+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:04:02.158+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:04:02.157+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:04:02.275+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.855 seconds
[2022-12-16T15:04:12.589+0000] {processor.py:154} INFO - Started process (PID=3484) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:04:12.592+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:04:12.597+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:04:12.596+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:04:12.684+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:04:12.856+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:04:12.855+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:04:12.975+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:04:12.974+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:04:13.113+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.539 seconds
[2022-12-16T15:04:23.412+0000] {processor.py:154} INFO - Started process (PID=3494) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:04:23.423+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:04:23.427+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:04:23.426+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:04:23.570+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:04:23.736+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:04:23.736+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:04:23.850+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:04:23.849+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:04:23.975+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.590 seconds
[2022-12-16T15:04:34.176+0000] {processor.py:154} INFO - Started process (PID=3512) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:04:34.223+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:04:34.230+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:04:34.229+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:04:34.393+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:04:34.550+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:04:34.549+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:04:34.708+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:04:34.699+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:04:35.003+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.858 seconds
[2022-12-16T15:04:45.421+0000] {processor.py:154} INFO - Started process (PID=3522) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:04:45.458+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:04:45.461+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:04:45.461+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:04:45.575+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:04:45.755+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:04:45.754+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:04:46.037+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:04:46.034+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:04:46.191+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.785 seconds
[2022-12-16T15:04:56.393+0000] {processor.py:154} INFO - Started process (PID=3532) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:04:56.447+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:04:56.451+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:04:56.450+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:04:56.559+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:04:56.758+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:04:56.757+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:04:56.885+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:04:56.884+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:04:56.994+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.620 seconds
[2022-12-16T15:05:07.355+0000] {processor.py:154} INFO - Started process (PID=3549) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:05:07.384+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:05:07.392+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:05:07.387+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:05:07.543+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:05:08.032+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:05:08.031+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:05:08.358+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:05:08.357+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:05:08.904+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.564 seconds
[2022-12-16T15:05:20.044+0000] {processor.py:154} INFO - Started process (PID=3560) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:05:20.083+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:05:20.162+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:05:20.138+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:05:20.745+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:05:20.964+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:05:20.963+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:05:21.116+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:05:21.115+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:05:21.282+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.371 seconds
[2022-12-16T15:05:31.802+0000] {processor.py:154} INFO - Started process (PID=3570) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:05:31.826+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:05:31.834+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:05:31.829+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:05:32.046+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:05:32.718+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:05:32.717+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:05:32.949+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:05:32.948+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:05:33.081+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.338 seconds
[2022-12-16T15:05:43.565+0000] {processor.py:154} INFO - Started process (PID=3580) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:05:43.616+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:05:43.628+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:05:43.619+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:05:43.783+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:05:44.046+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:05:44.045+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:05:44.176+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:05:44.175+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:05:44.377+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.870 seconds
[2022-12-16T15:05:55.362+0000] {processor.py:154} INFO - Started process (PID=3598) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:05:55.377+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:05:55.385+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:05:55.384+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:05:55.914+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:05:56.554+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:05:56.553+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:05:56.799+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:05:56.798+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:05:57.041+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.718 seconds
[2022-12-16T15:06:07.694+0000] {processor.py:154} INFO - Started process (PID=3608) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:06:07.698+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:06:07.706+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:06:07.705+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:06:07.865+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:06:08.142+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:06:08.141+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:06:08.322+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:06:08.321+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:06:08.491+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.812 seconds
[2022-12-16T15:06:19.282+0000] {processor.py:154} INFO - Started process (PID=3618) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:06:19.327+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:06:19.332+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:06:19.331+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:06:19.419+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:06:20.810+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:06:20.809+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:06:20.922+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:06:20.921+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:06:21.063+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.794 seconds
[2022-12-16T15:06:31.788+0000] {processor.py:154} INFO - Started process (PID=3628) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:06:31.813+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:06:31.825+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:06:31.824+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:06:31.967+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:06:33.028+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:06:33.027+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:06:33.141+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:06:33.140+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:06:33.263+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.500 seconds
[2022-12-16T15:06:43.901+0000] {processor.py:154} INFO - Started process (PID=3646) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:06:43.957+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:06:43.961+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:06:43.960+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:06:44.276+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:06:45.146+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:06:45.146+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:06:45.258+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:06:45.257+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:06:45.392+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.539 seconds
[2022-12-16T15:06:56.430+0000] {processor.py:154} INFO - Started process (PID=3656) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:06:56.455+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:06:56.460+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:06:56.459+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:06:56.542+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:06:56.676+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:06:56.675+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:06:56.790+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:06:56.789+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:06:56.917+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.503 seconds
[2022-12-16T15:07:07.249+0000] {processor.py:154} INFO - Started process (PID=3666) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:07:07.282+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:07:07.285+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:07:07.285+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:07:07.376+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:07:08.250+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:07:08.249+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:07:08.516+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:07:08.515+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:07:08.715+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.481 seconds
[2022-12-16T15:07:19.460+0000] {processor.py:154} INFO - Started process (PID=3684) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:07:19.495+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:07:19.513+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:07:19.512+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:07:19.735+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:07:20.339+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:07:20.338+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:07:20.538+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:07:20.538+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:07:20.679+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.302 seconds
[2022-12-16T15:07:30.953+0000] {processor.py:154} INFO - Started process (PID=3694) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:07:30.982+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:07:30.996+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:07:30.995+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:07:31.094+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:07:31.661+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:07:31.660+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:07:31.774+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:07:31.774+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:07:31.932+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.996 seconds
[2022-12-16T15:07:42.137+0000] {processor.py:154} INFO - Started process (PID=3704) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:07:42.209+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:07:42.214+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:07:42.212+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:07:42.298+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:07:43.005+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:07:43.004+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:07:43.116+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:07:43.115+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:07:43.247+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.125 seconds
[2022-12-16T15:07:53.631+0000] {processor.py:154} INFO - Started process (PID=3714) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:07:53.653+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:07:53.658+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:07:53.657+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:07:53.773+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:07:53.927+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:07:53.926+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:07:54.055+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:07:54.054+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:07:54.187+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.626 seconds
[2022-12-16T15:08:04.653+0000] {processor.py:154} INFO - Started process (PID=3732) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:08:04.679+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:08:04.691+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:08:04.690+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:08:04.794+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:08:04.959+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:08:04.958+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:08:05.281+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:08:05.280+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:08:05.697+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.081 seconds
[2022-12-16T15:08:16.109+0000] {processor.py:154} INFO - Started process (PID=3742) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:08:16.159+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:08:16.164+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:08:16.163+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:08:16.247+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:08:16.381+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:08:16.380+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:08:16.501+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:08:16.500+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:08:16.782+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.687 seconds
[2022-12-16T15:08:27.299+0000] {processor.py:154} INFO - Started process (PID=3752) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:08:27.317+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:08:27.322+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:08:27.321+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:08:27.406+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:08:27.536+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:08:27.535+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:08:27.655+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:08:27.654+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:08:27.803+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.518 seconds
[2022-12-16T15:08:38.104+0000] {processor.py:154} INFO - Started process (PID=3769) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:08:38.112+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:08:38.124+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:08:38.123+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:08:38.297+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:08:38.525+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:08:38.524+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:08:38.821+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:08:38.820+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:08:39.033+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.953 seconds
[2022-12-16T15:08:49.593+0000] {processor.py:154} INFO - Started process (PID=3780) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:08:49.622+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:08:49.627+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:08:49.626+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:08:49.753+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:08:49.974+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:08:49.973+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:08:50.110+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:08:50.109+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:08:50.218+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.637 seconds
[2022-12-16T15:09:00.772+0000] {processor.py:154} INFO - Started process (PID=3790) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:09:00.826+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:09:00.830+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:09:00.829+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:09:00.913+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:09:02.360+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T15:09:12.645+0000] {processor.py:154} INFO - Started process (PID=3800) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:09:12.693+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:09:12.697+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:09:12.696+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:09:12.778+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:09:12.914+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:09:12.914+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:09:13.027+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:09:13.026+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:09:13.153+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.525 seconds
[2022-12-16T15:09:23.632+0000] {processor.py:154} INFO - Started process (PID=3817) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:09:24.217+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:09:24.221+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:09:24.220+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:09:24.348+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:09:25.753+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:09:25.752+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:09:26.082+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:09:26.081+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:09:26.594+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.978 seconds
[2022-12-16T15:09:37.222+0000] {processor.py:154} INFO - Started process (PID=3827) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:09:37.246+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:09:37.250+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:09:37.249+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:09:37.387+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:09:37.545+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:09:37.544+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:09:37.668+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:09:37.667+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:09:37.791+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.589 seconds
[2022-12-16T15:09:48.178+0000] {processor.py:154} INFO - Started process (PID=3837) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:09:48.204+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:09:48.210+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:09:48.208+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:09:48.296+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:09:48.751+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:09:48.750+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:09:48.864+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:09:48.863+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:09:48.990+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.824 seconds
[2022-12-16T15:09:59.169+0000] {processor.py:154} INFO - Started process (PID=3847) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:09:59.220+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:09:59.225+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:09:59.224+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:09:59.310+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:09:59.459+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:09:59.458+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:09:59.583+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:09:59.582+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:09:59.725+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.570 seconds
[2022-12-16T15:10:10.245+0000] {processor.py:154} INFO - Started process (PID=3865) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:10:10.289+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:10:10.322+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:10:10.319+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:10:10.508+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:10:11.114+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:10:11.113+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:10:11.411+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:10:11.410+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:10:11.761+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.532 seconds
[2022-12-16T15:10:22.741+0000] {processor.py:154} INFO - Started process (PID=3875) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:10:22.776+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:10:22.780+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:10:22.779+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:10:22.947+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:10:24.626+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:10:24.625+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:10:24.772+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:10:24.772+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:10:24.930+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.225 seconds
[2022-12-16T15:10:35.354+0000] {processor.py:154} INFO - Started process (PID=3885) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:10:35.407+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:10:35.412+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:10:35.410+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:10:35.491+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:10:35.619+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:10:35.618+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:10:35.730+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:10:35.729+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:10:35.871+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.534 seconds
[2022-12-16T15:10:46.668+0000] {processor.py:154} INFO - Started process (PID=3902) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:10:46.698+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:10:46.706+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:10:46.705+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:10:46.844+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:10:47.126+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:10:47.125+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:10:47.257+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:10:47.257+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:10:47.383+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.730 seconds
[2022-12-16T15:10:57.759+0000] {processor.py:154} INFO - Started process (PID=3912) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:10:57.956+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:10:57.964+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:10:57.963+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:10:58.045+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:10:58.174+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:10:58.173+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:10:58.286+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:10:58.286+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:10:58.406+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.662 seconds
[2022-12-16T15:11:09.240+0000] {processor.py:154} INFO - Started process (PID=3922) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:11:09.291+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:11:09.295+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:11:09.294+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:11:09.404+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:11:10.282+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:11:10.281+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:11:10.441+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:11:10.440+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:11:10.635+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.445 seconds
[2022-12-16T15:11:20.814+0000] {processor.py:154} INFO - Started process (PID=3932) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:11:20.860+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:11:20.865+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:11:20.864+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:11:20.947+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:11:22.077+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T15:11:32.608+0000] {processor.py:154} INFO - Started process (PID=3950) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:11:32.639+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:11:32.643+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:11:32.642+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:11:32.756+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:11:33.555+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:11:33.554+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:11:33.749+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:11:33.748+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:11:34.184+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.592 seconds
[2022-12-16T15:11:44.771+0000] {processor.py:154} INFO - Started process (PID=3960) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:11:44.793+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:11:44.801+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:11:44.800+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:11:44.962+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:11:45.245+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:11:45.244+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:11:45.397+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:11:45.396+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:11:45.529+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.836 seconds
[2022-12-16T15:11:55.785+0000] {processor.py:154} INFO - Started process (PID=3970) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:11:55.807+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:11:55.812+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:11:55.811+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:11:55.903+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:11:57.283+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:11:57.282+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:11:57.397+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:11:57.396+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:11:57.545+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.779 seconds
[2022-12-16T15:12:08.159+0000] {processor.py:154} INFO - Started process (PID=3989) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:12:08.163+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:12:08.176+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:12:08.175+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:12:08.290+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:12:08.457+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:12:08.456+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:12:08.619+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:12:08.618+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:12:08.769+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.625 seconds
[2022-12-16T15:12:15.848+0000] {processor.py:154} INFO - Started process (PID=4007) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:12:15.877+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:12:15.885+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:12:15.884+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:12:16.091+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:12:16.232+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:12:16.231+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:12:16.798+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:12:16.797+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:12:17.296+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.467 seconds
[2022-12-16T15:12:27.714+0000] {processor.py:154} INFO - Started process (PID=4017) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:12:27.766+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:12:27.770+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:12:27.769+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:12:27.914+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:12:28.429+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:12:28.428+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:12:28.794+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:12:28.793+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:12:29.328+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.635 seconds
[2022-12-16T15:12:39.855+0000] {processor.py:154} INFO - Started process (PID=4027) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:12:39.865+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:12:39.869+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:12:39.868+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:12:39.963+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:12:40.558+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:12:40.557+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:12:40.725+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:12:40.724+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:12:40.925+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.088 seconds
[2022-12-16T15:12:51.236+0000] {processor.py:154} INFO - Started process (PID=4044) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:12:51.311+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:12:51.320+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:12:51.314+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:12:51.553+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:12:52.613+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:12:52.612+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:12:52.964+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:12:52.963+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:12:53.212+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.011 seconds
[2022-12-16T15:13:03.766+0000] {processor.py:154} INFO - Started process (PID=4054) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:13:03.804+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:13:03.814+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:13:03.813+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:13:04.068+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:13:04.254+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:13:04.253+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:13:04.403+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:13:04.402+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:13:04.626+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.905 seconds
[2022-12-16T15:13:14.966+0000] {processor.py:154} INFO - Started process (PID=4064) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:13:14.991+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:13:14.996+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:13:14.995+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:13:15.096+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:13:15.285+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:13:15.284+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:13:15.722+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:13:15.721+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:13:15.999+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.051 seconds
[2022-12-16T15:13:26.572+0000] {processor.py:154} INFO - Started process (PID=4081) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:13:26.596+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:13:26.608+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:13:26.607+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:13:26.862+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:13:27.810+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:13:27.809+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:13:28.063+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:13:28.054+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:13:28.226+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.724 seconds
[2022-12-16T15:13:38.524+0000] {processor.py:154} INFO - Started process (PID=4093) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:13:38.536+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:13:38.541+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:13:38.540+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:13:38.658+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:13:39.700+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T15:13:50.001+0000] {processor.py:154} INFO - Started process (PID=4103) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:13:50.027+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:13:50.031+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:13:50.030+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:13:50.182+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:13:50.873+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:13:50.866+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:13:51.190+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:13:51.189+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:13:51.346+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.362 seconds
[2022-12-16T15:14:01.692+0000] {processor.py:154} INFO - Started process (PID=4113) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:14:01.717+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:14:01.721+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:14:01.720+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:14:01.877+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:14:02.359+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:14:02.358+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:14:02.544+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:14:02.543+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:14:02.682+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.008 seconds
[2022-12-16T15:14:12.973+0000] {processor.py:154} INFO - Started process (PID=4131) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:14:13.158+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:14:13.167+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:14:13.162+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:14:13.331+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:14:14.136+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:14:14.135+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:14:14.797+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:14:14.796+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:14:15.100+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.155 seconds
[2022-12-16T15:14:25.624+0000] {processor.py:154} INFO - Started process (PID=4141) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:14:25.658+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:14:25.684+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:14:25.683+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:14:25.954+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:14:26.119+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:14:26.118+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:14:26.242+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:14:26.241+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:14:26.364+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.752 seconds
[2022-12-16T15:14:36.857+0000] {processor.py:154} INFO - Started process (PID=4151) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:14:36.897+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:14:36.901+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:14:36.900+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:14:37.397+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:14:39.086+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:14:39.085+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:14:39.235+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:14:39.234+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:14:39.572+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.771 seconds
[2022-12-16T15:14:49.922+0000] {processor.py:154} INFO - Started process (PID=4167) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:14:49.948+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:14:49.957+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:14:49.956+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:14:50.086+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:14:50.340+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:14:50.339+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:14:50.558+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:14:50.558+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:14:50.772+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.888 seconds
[2022-12-16T15:15:01.146+0000] {processor.py:154} INFO - Started process (PID=4180) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:15:01.191+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:15:01.195+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:15:01.194+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:15:01.309+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:15:01.976+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:15:01.975+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:15:02.102+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:15:02.101+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:15:02.211+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.098 seconds
[2022-12-16T15:15:12.576+0000] {processor.py:154} INFO - Started process (PID=4188) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:15:12.608+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:15:12.620+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:15:12.619+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:15:12.769+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:15:12.961+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:15:12.960+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:15:13.154+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:15:13.153+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:15:13.267+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.720 seconds
[2022-12-16T15:15:23.597+0000] {processor.py:154} INFO - Started process (PID=4200) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:15:23.622+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:15:23.630+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:15:23.629+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:15:24.049+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:15:24.357+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:15:24.356+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:15:24.561+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:15:24.560+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:15:24.671+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.104 seconds
[2022-12-16T15:15:35.021+0000] {processor.py:154} INFO - Started process (PID=4219) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:15:35.050+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:15:35.062+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:15:35.061+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:15:35.229+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:15:35.818+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:15:35.817+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:15:36.136+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:15:36.135+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:15:36.417+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.412 seconds
[2022-12-16T15:15:46.730+0000] {processor.py:154} INFO - Started process (PID=4229) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:15:46.780+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:15:46.784+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:15:46.783+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:15:46.909+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:15:48.341+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T15:15:58.766+0000] {processor.py:154} INFO - Started process (PID=4239) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:15:58.802+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:15:58.806+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:15:58.805+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:15:58.895+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:15:59.374+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T15:16:09.978+0000] {processor.py:154} INFO - Started process (PID=4249) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:16:10.037+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:16:10.046+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:16:10.045+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:16:10.465+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:16:11.767+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:16:11.766+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:16:11.963+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:16:11.962+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:16:12.208+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.347 seconds
[2022-12-16T15:16:22.796+0000] {processor.py:154} INFO - Started process (PID=4267) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:16:22.823+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:16:22.828+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:16:22.826+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:16:22.916+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:16:23.084+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:16:23.083+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:16:23.207+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:16:23.206+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:16:23.455+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.673 seconds
[2022-12-16T15:16:33.738+0000] {processor.py:154} INFO - Started process (PID=4277) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:16:33.760+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:16:33.764+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:16:33.763+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:16:33.862+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:16:34.498+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T15:16:45.014+0000] {processor.py:154} INFO - Started process (PID=4287) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:16:45.052+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:16:45.059+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:16:45.056+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:16:45.216+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:16:45.384+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:16:45.383+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:16:45.571+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:16:45.570+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:16:45.781+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.786 seconds
[2022-12-16T15:16:56.288+0000] {processor.py:154} INFO - Started process (PID=4306) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:16:56.333+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:16:56.337+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:16:56.336+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:16:56.480+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:16:56.645+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:16:56.644+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:16:57.086+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:16:57.085+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:16:57.541+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.286 seconds
[2022-12-16T15:17:08.053+0000] {processor.py:154} INFO - Started process (PID=4316) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:17:08.076+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:17:08.080+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:17:08.079+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:17:08.170+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:17:08.334+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:17:08.333+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:17:08.568+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:17:08.567+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:17:08.806+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.766 seconds
[2022-12-16T15:17:19.337+0000] {processor.py:154} INFO - Started process (PID=4326) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:17:19.390+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:17:19.396+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:17:19.395+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:17:19.499+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:17:19.678+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:17:19.677+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:17:19.929+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:17:19.928+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:17:20.096+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.773 seconds
[2022-12-16T15:17:30.431+0000] {processor.py:154} INFO - Started process (PID=4336) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:17:30.477+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:17:30.481+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:17:30.480+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:17:30.607+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:17:31.681+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T15:17:42.090+0000] {processor.py:154} INFO - Started process (PID=4354) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:17:42.194+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:17:42.198+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:17:42.197+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:17:42.339+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:17:42.838+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:17:42.837+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:17:42.959+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:17:42.958+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:17:43.104+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.028 seconds
[2022-12-16T15:17:53.531+0000] {processor.py:154} INFO - Started process (PID=4364) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:17:53.534+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:17:53.538+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:17:53.537+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:17:53.629+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:17:53.777+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:17:53.776+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:17:53.973+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:17:53.972+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:17:54.082+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.565 seconds
[2022-12-16T15:18:04.223+0000] {processor.py:154} INFO - Started process (PID=4374) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:18:04.231+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:18:04.236+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:18:04.234+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:18:04.384+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:18:04.753+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:18:04.752+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:18:04.870+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:18:04.869+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:18:04.996+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.792 seconds
[2022-12-16T15:18:15.409+0000] {processor.py:154} INFO - Started process (PID=4393) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:18:15.417+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:18:15.421+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:18:15.420+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:18:15.658+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:18:15.942+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:18:15.941+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:18:16.088+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:18:16.087+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:18:16.242+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.861 seconds
[2022-12-16T15:18:26.457+0000] {processor.py:154} INFO - Started process (PID=4403) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:18:26.502+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:18:26.505+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:18:26.504+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:18:26.599+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:18:26.743+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:18:26.742+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:18:26.880+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:18:26.880+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:18:27.022+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.580 seconds
[2022-12-16T15:18:37.397+0000] {processor.py:154} INFO - Started process (PID=4413) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:18:37.401+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:18:37.405+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:18:37.404+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:18:37.492+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:18:37.654+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:18:37.653+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:18:37.769+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:18:37.768+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:18:37.909+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.531 seconds
[2022-12-16T15:18:48.353+0000] {processor.py:154} INFO - Started process (PID=4423) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:18:48.376+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:18:48.380+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:18:48.379+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:18:48.542+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:18:48.901+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:18:48.900+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:18:49.182+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:18:49.181+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:18:49.335+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.010 seconds
[2022-12-16T15:18:59.745+0000] {processor.py:154} INFO - Started process (PID=4438) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:18:59.753+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:18:59.757+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:18:59.756+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:18:59.992+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:19:00.235+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:19:00.234+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:19:00.395+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:19:00.394+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:19:00.621+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.903 seconds
[2022-12-16T15:19:11.135+0000] {processor.py:154} INFO - Started process (PID=4448) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:19:11.158+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:19:11.169+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:19:11.168+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:19:11.274+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:19:11.417+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:19:11.416+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:19:11.530+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:19:11.529+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:19:11.661+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.577 seconds
[2022-12-16T15:19:21.890+0000] {processor.py:154} INFO - Started process (PID=4455) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:19:22.072+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:19:22.077+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:19:22.076+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:19:22.195+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:19:22.517+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:19:22.516+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:19:22.651+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:19:22.650+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:19:23.211+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.334 seconds
[2022-12-16T15:19:33.996+0000] {processor.py:154} INFO - Started process (PID=4471) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:19:34.079+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:19:34.096+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:19:34.082+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:19:34.373+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:19:34.614+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:19:34.613+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:19:34.791+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:19:34.790+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:19:34.928+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.140 seconds
[2022-12-16T15:19:45.086+0000] {processor.py:154} INFO - Started process (PID=4483) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:19:45.109+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:19:45.114+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:19:45.113+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:19:45.204+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:19:46.400+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T15:19:57.201+0000] {processor.py:154} INFO - Started process (PID=4496) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:19:57.286+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:19:57.294+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:19:57.293+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:19:57.646+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:19:59.142+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:19:59.141+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:19:59.315+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:19:59.314+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:19:59.432+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.323 seconds
[2022-12-16T15:20:09.714+0000] {processor.py:154} INFO - Started process (PID=4506) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:20:09.717+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:20:09.721+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:20:09.720+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:20:09.803+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:20:09.938+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:20:09.937+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:20:10.066+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:20:10.066+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:20:10.225+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.525 seconds
[2022-12-16T15:20:20.600+0000] {processor.py:154} INFO - Started process (PID=4520) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:20:20.608+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:20:20.619+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:20:20.617+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:20:20.719+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:20:22.363+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:20:22.362+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:20:22.509+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:20:22.508+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:20:22.734+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.150 seconds
[2022-12-16T15:20:33.825+0000] {processor.py:154} INFO - Started process (PID=4535) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:20:33.828+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:20:33.832+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:20:33.831+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:20:33.921+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:20:34.089+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:20:34.089+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:20:34.206+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:20:34.205+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:20:34.345+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.532 seconds
[2022-12-16T15:20:44.628+0000] {processor.py:154} INFO - Started process (PID=4545) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:20:44.676+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:20:44.680+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:20:44.679+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:20:44.769+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:20:45.251+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:20:45.250+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:20:45.368+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:20:45.368+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:20:45.478+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.864 seconds
[2022-12-16T15:20:55.978+0000] {processor.py:154} INFO - Started process (PID=4560) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:20:56.003+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:20:56.017+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:20:56.006+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:20:56.518+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:20:56.890+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:20:56.889+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:20:57.166+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:20:57.165+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:20:57.607+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.687 seconds
[2022-12-16T15:21:08.058+0000] {processor.py:154} INFO - Started process (PID=4573) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:21:08.062+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:21:08.066+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:21:08.065+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:21:08.182+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:21:09.204+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:21:09.203+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:21:09.574+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:21:09.573+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:21:09.812+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.780 seconds
[2022-12-16T15:21:20.185+0000] {processor.py:154} INFO - Started process (PID=4583) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:21:20.207+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:21:20.212+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:21:20.210+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:21:20.514+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:21:20.716+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:21:20.715+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:21:20.832+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:21:20.831+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:21:20.990+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.884 seconds
[2022-12-16T15:21:31.292+0000] {processor.py:154} INFO - Started process (PID=4593) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:21:31.324+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:21:31.328+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:21:31.327+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:21:31.423+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:21:31.578+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:21:31.577+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:21:31.698+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:21:31.697+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:21:31.839+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.561 seconds
[2022-12-16T15:21:42.198+0000] {processor.py:154} INFO - Started process (PID=4609) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:21:42.206+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:21:42.210+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:21:42.209+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:21:42.426+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:21:43.091+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:21:43.090+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:21:43.362+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:21:43.361+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:21:43.585+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.423 seconds
[2022-12-16T15:21:53.992+0000] {processor.py:154} INFO - Started process (PID=4619) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:21:54.007+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:21:54.011+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:21:54.010+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:21:54.127+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:21:54.569+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:21:54.568+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:21:54.688+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:21:54.687+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:21:54.821+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.846 seconds
[2022-12-16T15:22:05.145+0000] {processor.py:154} INFO - Started process (PID=4629) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:22:05.171+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:22:05.176+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:22:05.175+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:22:05.267+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:22:06.609+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T15:22:16.966+0000] {processor.py:154} INFO - Started process (PID=4646) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:22:17.012+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:22:17.017+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:22:17.016+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:22:17.145+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:22:17.415+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:22:17.414+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:22:17.688+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:22:17.682+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:22:17.831+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.884 seconds
[2022-12-16T15:22:28.340+0000] {processor.py:154} INFO - Started process (PID=4657) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:22:28.377+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:22:28.381+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:22:28.380+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:22:28.470+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:22:29.612+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T15:22:39.989+0000] {processor.py:154} INFO - Started process (PID=4667) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:22:40.025+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:22:40.030+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:22:40.029+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:22:40.149+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:22:40.718+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:22:40.717+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:22:40.833+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:22:40.832+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:22:40.948+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.972 seconds
[2022-12-16T15:22:51.227+0000] {processor.py:154} INFO - Started process (PID=4677) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:22:51.234+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:22:51.241+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:22:51.240+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:22:51.342+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:22:51.989+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:22:51.988+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:22:52.145+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:22:52.144+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:22:52.283+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.091 seconds
[2022-12-16T15:23:02.703+0000] {processor.py:154} INFO - Started process (PID=4694) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:23:02.740+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:23:02.748+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:23:02.747+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:23:02.862+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:23:03.089+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:23:03.088+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:23:03.254+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:23:03.254+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:23:03.434+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.749 seconds
[2022-12-16T15:23:13.953+0000] {processor.py:154} INFO - Started process (PID=4702) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:23:13.977+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:23:13.989+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:23:13.988+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:23:14.110+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:23:15.482+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:23:15.481+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:23:15.617+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:23:15.616+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:23:15.752+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.815 seconds
[2022-12-16T15:23:26.015+0000] {processor.py:154} INFO - Started process (PID=4715) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:23:26.019+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:23:26.023+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:23:26.022+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:23:26.129+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:23:26.354+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:23:26.353+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:23:26.542+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:23:26.541+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:23:26.709+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.709 seconds
[2022-12-16T15:23:37.051+0000] {processor.py:154} INFO - Started process (PID=4725) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:23:37.134+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:23:37.138+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:23:37.137+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:23:37.341+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:23:38.253+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T15:23:48.701+0000] {processor.py:154} INFO - Started process (PID=4740) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:23:48.725+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:23:48.729+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:23:48.728+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:23:48.812+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:23:48.946+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:23:48.945+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:23:49.097+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:23:49.096+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:23:49.300+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.614 seconds
[2022-12-16T15:23:59.656+0000] {processor.py:154} INFO - Started process (PID=4750) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:23:59.960+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:23:59.965+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:23:59.964+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:24:00.060+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:24:01.838+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:24:01.837+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:24:02.117+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:24:02.116+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:24:02.385+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.747 seconds
[2022-12-16T15:24:12.720+0000] {processor.py:154} INFO - Started process (PID=4763) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:24:12.736+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:24:12.741+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:24:12.739+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:24:12.824+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:24:12.968+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:24:12.967+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:24:13.115+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:24:13.114+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:24:13.337+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.631 seconds
[2022-12-16T15:24:23.774+0000] {processor.py:154} INFO - Started process (PID=4779) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:24:23.818+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:24:23.832+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:24:23.828+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:24:24.127+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:24:24.283+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:24:24.282+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:24:24.537+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:24:24.536+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:24:24.754+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.993 seconds
[2022-12-16T15:24:35.170+0000] {processor.py:154} INFO - Started process (PID=4790) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:24:35.240+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:24:35.250+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:24:35.249+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:24:35.366+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:24:35.528+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:24:35.527+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:24:35.670+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:24:35.669+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:24:35.897+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.750 seconds
[2022-12-16T15:24:46.203+0000] {processor.py:154} INFO - Started process (PID=4800) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:24:46.233+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:24:46.238+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:24:46.236+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:24:46.358+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:24:46.516+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:24:46.515+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:24:46.639+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:24:46.637+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:24:46.774+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.585 seconds
[2022-12-16T15:24:57.229+0000] {processor.py:154} INFO - Started process (PID=4809) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:24:57.258+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:24:57.262+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:24:57.261+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:24:57.446+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:24:57.726+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:24:57.725+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:24:58.121+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:24:58.120+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:24:58.271+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.101 seconds
[2022-12-16T15:25:08.786+0000] {processor.py:154} INFO - Started process (PID=4825) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:25:08.816+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:25:08.830+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:25:08.829+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:25:09.072+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:25:09.370+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:25:09.369+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:25:09.556+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:25:09.555+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:25:09.713+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.943 seconds
[2022-12-16T15:25:19.993+0000] {processor.py:154} INFO - Started process (PID=4835) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:25:19.996+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:25:20.001+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:25:20.000+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:25:20.095+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:25:20.970+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:25:20.969+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:25:21.102+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:25:21.102+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:25:21.221+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.243 seconds
[2022-12-16T15:25:31.535+0000] {processor.py:154} INFO - Started process (PID=4845) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:25:31.553+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:25:31.558+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:25:31.557+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:25:31.640+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:25:32.105+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:25:32.104+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:25:32.238+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:25:32.237+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:25:32.431+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.910 seconds
[2022-12-16T15:25:42.614+0000] {processor.py:154} INFO - Started process (PID=4863) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:25:42.638+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:25:42.646+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:25:42.645+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:25:42.802+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:25:43.013+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:25:43.012+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:25:43.246+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:25:43.245+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:25:43.572+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.973 seconds
[2022-12-16T15:25:54.171+0000] {processor.py:154} INFO - Started process (PID=4873) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:25:54.218+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:25:54.223+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:25:54.221+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:25:54.304+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:25:54.441+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:25:54.440+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:25:54.570+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:25:54.569+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:25:54.684+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.529 seconds
[2022-12-16T15:26:05.254+0000] {processor.py:154} INFO - Started process (PID=4883) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:26:05.321+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:26:05.326+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:26:05.325+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:26:05.410+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:26:05.817+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:26:05.816+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:26:05.935+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:26:05.934+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:26:06.070+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.831 seconds
[2022-12-16T15:26:16.679+0000] {processor.py:154} INFO - Started process (PID=4893) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:26:16.734+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:26:16.738+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:26:16.737+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:26:16.823+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:26:17.304+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:26:17.303+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:26:17.484+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:26:17.478+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:26:17.761+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.101 seconds
[2022-12-16T15:26:28.622+0000] {processor.py:154} INFO - Started process (PID=4910) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:26:28.656+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:26:28.664+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:26:28.663+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:26:28.843+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:26:29.025+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:26:29.023+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:26:29.192+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:26:29.192+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:26:29.312+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.814 seconds
[2022-12-16T15:26:39.753+0000] {processor.py:154} INFO - Started process (PID=4920) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:26:39.793+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:26:39.798+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:26:39.797+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:26:39.881+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:26:40.178+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:26:40.175+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:26:40.333+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:26:40.332+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:26:40.440+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.701 seconds
[2022-12-16T15:26:50.704+0000] {processor.py:154} INFO - Started process (PID=4930) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:26:50.731+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:26:50.736+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:26:50.735+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:26:50.820+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:26:51.144+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:26:51.143+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:26:51.254+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:26:51.253+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:26:51.375+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.685 seconds
[2022-12-16T15:27:01.769+0000] {processor.py:154} INFO - Started process (PID=4946) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:27:01.822+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:27:01.829+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:27:01.829+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:27:01.957+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:27:02.428+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:27:02.427+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:27:02.573+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:27:02.573+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:27:02.714+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.958 seconds
[2022-12-16T15:27:13.333+0000] {processor.py:154} INFO - Started process (PID=4957) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:27:13.356+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:27:13.360+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:27:13.359+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:27:13.474+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:27:13.963+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:27:13.962+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:27:14.091+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:27:14.090+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:27:14.538+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.244 seconds
[2022-12-16T15:27:24.869+0000] {processor.py:154} INFO - Started process (PID=4967) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:27:24.924+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:27:24.928+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:27:24.927+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:27:25.012+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:27:25.173+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:27:25.172+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:27:25.290+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:27:25.289+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:27:25.394+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.540 seconds
[2022-12-16T15:27:36.230+0000] {processor.py:154} INFO - Started process (PID=4977) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:27:36.260+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:27:36.265+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:27:36.264+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:27:36.350+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:27:36.681+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:27:36.680+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:27:36.792+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:27:36.791+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:27:36.928+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.710 seconds
[2022-12-16T15:27:47.278+0000] {processor.py:154} INFO - Started process (PID=4995) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:27:47.284+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:27:47.296+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:27:47.286+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:27:47.436+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:27:48.795+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:27:48.794+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:27:48.938+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:27:48.937+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:27:49.085+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.823 seconds
[2022-12-16T15:27:59.896+0000] {processor.py:154} INFO - Started process (PID=5005) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:27:59.917+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:27:59.922+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:27:59.921+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:28:00.039+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:28:01.128+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:28:01.127+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:28:01.246+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:28:01.245+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:28:01.461+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.580 seconds
[2022-12-16T15:28:11.678+0000] {processor.py:154} INFO - Started process (PID=5015) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:28:11.706+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:28:11.710+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:28:11.709+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:28:11.793+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:28:12.634+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T15:28:23.262+0000] {processor.py:154} INFO - Started process (PID=5032) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:28:23.285+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:28:23.289+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:28:23.288+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:28:23.394+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:28:23.969+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:28:23.968+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:28:24.145+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:28:24.144+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:28:24.273+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.024 seconds
[2022-12-16T15:28:34.605+0000] {processor.py:154} INFO - Started process (PID=5043) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:28:34.632+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:28:34.637+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:28:34.636+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:28:34.741+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:28:34.892+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:28:34.891+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:28:35.033+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:28:35.032+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:28:35.187+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.600 seconds
[2022-12-16T15:28:45.455+0000] {processor.py:154} INFO - Started process (PID=5053) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:28:45.459+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:28:45.463+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:28:45.462+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:28:45.545+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:28:45.686+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:28:45.685+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:28:45.798+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:28:45.797+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:28:45.918+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.476 seconds
[2022-12-16T15:28:56.161+0000] {processor.py:154} INFO - Started process (PID=5063) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:28:56.180+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:28:56.184+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:28:56.183+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:28:56.269+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:28:56.400+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:28:56.399+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:28:56.511+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:28:56.510+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:28:56.643+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.496 seconds
[2022-12-16T15:29:07.205+0000] {processor.py:154} INFO - Started process (PID=5081) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:29:07.212+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:29:07.231+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:29:07.226+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:29:07.433+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:29:07.594+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:29:07.593+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:29:07.997+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:29:07.996+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:29:08.368+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.227 seconds
[2022-12-16T15:29:18.941+0000] {processor.py:154} INFO - Started process (PID=5091) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:29:18.972+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:29:18.977+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:29:18.976+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:29:19.110+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:29:19.572+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:29:19.566+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:29:20.119+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:29:20.118+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:29:20.376+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.482 seconds
[2022-12-16T15:29:31.391+0000] {processor.py:154} INFO - Started process (PID=5101) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:29:31.412+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:29:31.425+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:29:31.423+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:29:31.653+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:29:31.794+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:29:31.793+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:29:31.911+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:29:31.910+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:29:32.035+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.709 seconds
[2022-12-16T15:29:42.665+0000] {processor.py:154} INFO - Started process (PID=5111) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:29:42.689+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:29:42.693+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:29:42.692+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:29:42.799+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:29:42.961+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:29:42.960+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:29:43.086+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:29:43.085+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:29:43.243+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.635 seconds
[2022-12-16T15:29:53.781+0000] {processor.py:154} INFO - Started process (PID=5129) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:29:53.814+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:29:53.818+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:29:53.817+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:29:53.938+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:29:54.381+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:29:54.380+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:29:54.589+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:29:54.588+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:29:54.791+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.026 seconds
[2022-12-16T15:30:05.101+0000] {processor.py:154} INFO - Started process (PID=5139) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:30:05.128+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:30:05.136+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:30:05.134+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:30:05.230+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:30:06.444+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:30:06.443+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:30:06.572+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:30:06.571+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:30:06.693+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.606 seconds
[2022-12-16T15:30:17.531+0000] {processor.py:154} INFO - Started process (PID=5149) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:30:17.544+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:30:17.561+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:30:17.551+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:30:17.659+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:30:17.799+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:30:17.798+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:30:17.910+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:30:17.909+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:30:18.018+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.502 seconds
[2022-12-16T15:30:28.437+0000] {processor.py:154} INFO - Started process (PID=5167) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:30:28.445+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:30:28.460+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:30:28.459+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:30:28.640+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:30:28.844+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:30:28.843+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:30:28.976+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:30:28.976+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:30:29.189+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.768 seconds
[2022-12-16T15:30:39.756+0000] {processor.py:154} INFO - Started process (PID=5177) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:30:39.805+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:30:39.810+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:30:39.809+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:30:39.905+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:30:41.290+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T15:30:51.896+0000] {processor.py:154} INFO - Started process (PID=5187) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:30:51.922+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:30:51.929+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:30:51.928+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:30:52.013+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:30:52.151+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:30:52.150+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:30:52.273+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:30:52.272+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:30:52.394+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.513 seconds
[2022-12-16T15:31:02.763+0000] {processor.py:154} INFO - Started process (PID=5197) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:31:02.783+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:31:02.786+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:31:02.785+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:31:02.933+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:31:04.624+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:31:04.623+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:31:04.833+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:31:04.832+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:31:05.043+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.298 seconds
[2022-12-16T15:31:15.513+0000] {processor.py:154} INFO - Started process (PID=5216) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:31:15.517+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:31:15.521+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:31:15.520+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:31:15.605+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:31:16.838+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T15:31:27.385+0000] {processor.py:154} INFO - Started process (PID=5226) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:31:27.420+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:31:27.431+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:31:27.429+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:31:27.523+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:31:28.290+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:31:28.289+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:31:28.430+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:31:28.429+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:31:28.545+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.211 seconds
[2022-12-16T15:31:39.566+0000] {processor.py:154} INFO - Started process (PID=5236) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:31:39.594+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:31:39.599+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:31:39.598+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:31:39.681+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:31:40.076+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:31:40.075+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:31:40.188+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:31:40.188+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:31:40.292+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.738 seconds
[2022-12-16T15:31:50.478+0000] {processor.py:154} INFO - Started process (PID=5253) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:31:50.482+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:31:50.488+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:31:50.486+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:31:50.870+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:31:51.250+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:31:51.249+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:31:51.394+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:31:51.393+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:31:51.517+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.055 seconds
[2022-12-16T15:32:01.732+0000] {processor.py:154} INFO - Started process (PID=5263) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:32:01.745+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:32:01.749+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:32:01.748+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:32:01.837+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:32:02.041+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:32:02.040+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:32:02.341+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:32:02.340+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:32:02.552+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.834 seconds
[2022-12-16T15:32:13.024+0000] {processor.py:154} INFO - Started process (PID=5273) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:32:13.048+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:32:13.052+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:32:13.051+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:32:13.134+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:32:14.512+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T15:32:25.325+0000] {processor.py:154} INFO - Started process (PID=5283) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:32:25.357+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:32:25.373+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:32:25.372+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:32:25.840+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:32:27.530+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:32:27.529+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:32:27.886+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:32:27.885+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:32:28.300+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.988 seconds
[2022-12-16T15:32:38.904+0000] {processor.py:154} INFO - Started process (PID=5302) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:32:38.951+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:32:38.956+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:32:38.955+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:32:39.036+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:32:40.224+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:32:40.223+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:32:40.383+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:32:40.382+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:32:40.530+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.642 seconds
[2022-12-16T15:32:50.826+0000] {processor.py:154} INFO - Started process (PID=5315) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:32:50.830+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:32:50.834+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:32:50.833+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:32:50.933+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:32:51.327+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:32:51.326+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:32:51.481+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:32:51.480+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:32:51.600+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.787 seconds
[2022-12-16T15:33:01.961+0000] {processor.py:154} INFO - Started process (PID=5325) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:33:01.981+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:33:01.985+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:33:01.984+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:33:02.114+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:33:02.282+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:33:02.281+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:33:02.440+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:33:02.439+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:33:02.700+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.774 seconds
[2022-12-16T15:33:13.555+0000] {processor.py:154} INFO - Started process (PID=5342) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:33:13.578+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:33:13.582+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:33:13.581+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:33:13.793+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:33:13.986+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:33:13.985+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:33:14.174+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:33:14.173+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:33:14.311+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.830 seconds
[2022-12-16T15:33:24.800+0000] {processor.py:154} INFO - Started process (PID=5349) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:33:24.851+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:33:24.855+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:33:24.854+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:33:24.940+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:33:25.487+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T15:33:35.853+0000] {processor.py:154} INFO - Started process (PID=5359) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:33:35.894+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:33:35.902+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:33:35.901+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:33:36.001+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:33:36.151+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:33:36.150+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:33:36.275+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:33:36.274+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:33:36.396+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.556 seconds
[2022-12-16T15:33:46.674+0000] {processor.py:154} INFO - Started process (PID=5369) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:33:46.678+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:33:46.683+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:33:46.682+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:33:46.786+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:33:46.937+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:33:46.936+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:33:47.051+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:33:47.050+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:33:47.158+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.500 seconds
[2022-12-16T15:33:57.670+0000] {processor.py:154} INFO - Started process (PID=5386) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:33:57.677+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:33:57.682+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:33:57.682+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:33:57.959+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:33:59.423+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T15:34:09.826+0000] {processor.py:154} INFO - Started process (PID=5396) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:34:09.857+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:34:09.862+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:34:09.861+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:34:09.974+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:34:10.607+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:34:10.606+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:34:10.778+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:34:10.778+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:34:10.913+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.103 seconds
[2022-12-16T15:34:21.255+0000] {processor.py:154} INFO - Started process (PID=5409) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:34:21.281+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:34:21.285+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:34:21.284+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:34:21.397+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:34:22.442+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T15:34:33.066+0000] {processor.py:154} INFO - Started process (PID=5427) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:34:33.109+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:34:33.137+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:34:33.136+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:34:33.536+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:34:35.094+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T15:34:45.696+0000] {processor.py:154} INFO - Started process (PID=5439) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:34:45.700+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:34:45.707+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:34:45.706+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:34:45.812+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:34:46.341+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:34:46.340+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:34:46.479+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:34:46.478+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:34:46.584+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.903 seconds
[2022-12-16T15:34:56.877+0000] {processor.py:154} INFO - Started process (PID=5449) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:34:56.881+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:34:56.885+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:34:56.884+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:34:56.989+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:34:57.326+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:34:57.325+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:34:57.449+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:34:57.448+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:34:57.599+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.738 seconds
[2022-12-16T15:35:07.949+0000] {processor.py:154} INFO - Started process (PID=5457) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:35:08.022+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:35:08.026+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:35:08.025+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:35:08.114+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:35:08.553+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:35:08.551+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:35:09.027+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:35:09.026+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:35:09.328+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.393 seconds
[2022-12-16T15:35:20.096+0000] {processor.py:154} INFO - Started process (PID=5475) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:35:20.129+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:35:20.138+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:35:20.137+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:35:20.281+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:35:20.690+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:35:20.689+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:35:20.821+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:35:20.820+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:35:20.964+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.907 seconds
[2022-12-16T15:35:31.357+0000] {processor.py:154} INFO - Started process (PID=5485) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:35:31.389+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:35:31.394+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:35:31.393+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:35:31.512+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:35:32.640+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T15:35:43.055+0000] {processor.py:154} INFO - Started process (PID=5495) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:35:43.069+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:35:43.074+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:35:43.073+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:35:43.156+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:35:44.401+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:35:44.400+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:35:44.518+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:35:44.517+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:35:44.658+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.621 seconds
[2022-12-16T15:35:55.000+0000] {processor.py:154} INFO - Started process (PID=5514) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:35:55.004+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:35:55.009+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:35:55.008+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:35:55.104+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:35:55.280+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:35:55.279+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:35:55.483+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:35:55.482+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:35:55.693+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.710 seconds
[2022-12-16T15:36:06.120+0000] {processor.py:154} INFO - Started process (PID=5524) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:36:06.147+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:36:06.151+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:36:06.150+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:36:06.277+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:36:06.414+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:36:06.414+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:36:06.532+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:36:06.531+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:36:06.689+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.584 seconds
[2022-12-16T15:36:16.957+0000] {processor.py:154} INFO - Started process (PID=5531) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:36:16.960+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:36:16.964+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:36:16.963+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:36:17.047+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:36:18.312+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T15:36:28.571+0000] {processor.py:154} INFO - Started process (PID=5541) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:36:28.621+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:36:28.625+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:36:28.624+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:36:28.721+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:36:29.088+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:36:29.087+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:36:29.275+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:36:29.274+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:36:29.390+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.835 seconds
[2022-12-16T15:36:40.042+0000] {processor.py:154} INFO - Started process (PID=5558) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:36:40.061+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:36:40.069+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:36:40.068+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:36:40.216+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:36:40.631+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:36:40.626+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:36:41.241+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:36:41.240+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:36:41.742+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.738 seconds
[2022-12-16T15:36:52.032+0000] {processor.py:154} INFO - Started process (PID=5568) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:36:52.062+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:36:52.067+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:36:52.066+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:36:52.151+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:36:52.290+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:36:52.289+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:36:52.401+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:36:52.400+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:36:52.547+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.528 seconds
[2022-12-16T15:37:02.819+0000] {processor.py:154} INFO - Started process (PID=5578) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:37:02.823+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:37:02.828+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:37:02.826+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:37:02.912+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:37:03.047+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:37:03.046+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:37:03.162+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:37:03.161+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:37:03.320+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.514 seconds
[2022-12-16T15:37:13.712+0000] {processor.py:154} INFO - Started process (PID=5596) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:37:13.729+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:37:13.734+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:37:13.733+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:37:13.828+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:37:13.982+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:37:13.981+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:37:14.119+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:37:14.118+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:37:14.307+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.609 seconds
[2022-12-16T15:37:24.899+0000] {processor.py:154} INFO - Started process (PID=5607) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:37:24.925+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:37:24.930+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:37:24.929+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:37:25.039+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:37:25.205+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:37:25.205+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:37:25.330+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:37:25.329+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:37:25.475+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.599 seconds
[2022-12-16T15:37:36.381+0000] {processor.py:154} INFO - Started process (PID=5617) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:37:36.384+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:37:36.388+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:37:36.387+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:37:36.477+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:37:36.605+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:37:36.604+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:37:36.713+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:37:36.713+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:37:36.856+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.488 seconds
[2022-12-16T15:37:47.144+0000] {processor.py:154} INFO - Started process (PID=5627) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:37:47.148+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:37:47.152+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:37:47.151+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:37:47.233+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:37:47.368+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:37:47.367+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:37:47.478+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:37:47.477+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:37:47.614+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.487 seconds
[2022-12-16T15:37:57.950+0000] {processor.py:154} INFO - Started process (PID=5645) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:37:57.986+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:37:57.994+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:37:57.993+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:37:58.096+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:37:58.311+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:37:58.310+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:37:58.700+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:37:58.698+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:37:58.997+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.067 seconds
[2022-12-16T15:38:09.486+0000] {processor.py:154} INFO - Started process (PID=5655) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:38:09.493+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:38:09.497+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:38:09.496+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:38:09.698+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:38:10.247+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:38:10.246+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:38:10.373+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:38:10.372+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:38:10.504+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.034 seconds
[2022-12-16T15:38:20.775+0000] {processor.py:154} INFO - Started process (PID=5665) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:38:20.801+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:38:20.805+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:38:20.804+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:38:21.028+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:38:21.822+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T15:38:32.128+0000] {processor.py:154} INFO - Started process (PID=5675) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:38:32.159+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:38:32.173+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:38:32.166+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:38:32.418+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:38:32.697+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:38:32.696+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:38:32.859+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:38:32.858+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:38:32.980+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.867 seconds
[2022-12-16T15:38:43.247+0000] {processor.py:154} INFO - Started process (PID=5693) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:38:43.284+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:38:43.293+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:38:43.292+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:38:43.533+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:38:45.442+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:38:45.440+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:38:45.584+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:38:45.583+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:38:45.848+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.643 seconds
[2022-12-16T15:38:56.635+0000] {processor.py:154} INFO - Started process (PID=5703) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:38:56.639+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:38:56.644+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:38:56.643+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:38:56.725+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:38:57.360+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:38:57.359+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:38:57.479+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:38:57.478+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:38:57.646+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.025 seconds
[2022-12-16T15:39:07.917+0000] {processor.py:154} INFO - Started process (PID=5713) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:39:07.948+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:39:07.952+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:39:07.951+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:39:08.045+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:39:08.192+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:39:08.192+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:39:08.313+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:39:08.312+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:39:08.416+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.514 seconds
[2022-12-16T15:39:19.290+0000] {processor.py:154} INFO - Started process (PID=5730) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:39:19.372+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:39:19.377+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:39:19.376+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:39:19.547+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:39:19.699+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:39:19.698+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:39:19.846+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:39:19.846+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:39:19.975+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.750 seconds
[2022-12-16T15:39:30.348+0000] {processor.py:154} INFO - Started process (PID=5740) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:39:30.387+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:39:30.391+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:39:30.390+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:39:30.473+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:39:30.673+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:39:30.670+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:39:30.893+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:39:30.892+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:39:31.088+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.755 seconds
[2022-12-16T15:39:41.400+0000] {processor.py:154} INFO - Started process (PID=5750) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:39:41.404+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:39:41.408+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:39:41.407+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:39:41.495+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:39:41.649+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:39:41.649+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:39:41.772+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:39:41.771+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:39:41.909+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.523 seconds
[2022-12-16T15:39:52.102+0000] {processor.py:154} INFO - Started process (PID=5760) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:39:52.128+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:39:52.132+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:39:52.130+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:39:52.235+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:39:53.116+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T15:40:03.392+0000] {processor.py:154} INFO - Started process (PID=5778) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:40:03.419+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:40:03.424+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:40:03.422+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:40:03.532+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:40:03.700+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:40:03.699+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:40:03.832+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:40:03.831+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:40:03.987+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.631 seconds
[2022-12-16T15:40:14.376+0000] {processor.py:154} INFO - Started process (PID=5788) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:40:14.423+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:40:14.428+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:40:14.427+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:40:14.511+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:40:15.309+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:40:15.308+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:40:15.427+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:40:15.426+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:40:15.563+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.201 seconds
[2022-12-16T15:40:25.885+0000] {processor.py:154} INFO - Started process (PID=5798) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:40:25.902+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:40:25.906+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:40:25.905+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:40:25.990+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:40:26.126+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:40:26.125+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:40:26.239+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:40:26.238+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:40:26.368+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.498 seconds
[2022-12-16T15:40:37.127+0000] {processor.py:154} INFO - Started process (PID=5815) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:40:37.160+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:40:37.165+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:40:37.164+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:40:37.268+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:40:37.441+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:40:37.440+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:40:37.573+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:40:37.572+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:40:37.718+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.608 seconds
[2022-12-16T15:40:48.101+0000] {processor.py:154} INFO - Started process (PID=5826) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:40:48.131+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:40:48.139+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:40:48.138+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:40:48.219+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:40:48.774+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:40:48.774+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:40:48.886+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:40:48.885+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:40:49.142+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.055 seconds
[2022-12-16T15:40:59.530+0000] {processor.py:154} INFO - Started process (PID=5836) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:40:59.535+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:40:59.539+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:40:59.538+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:40:59.624+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:40:59.757+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:40:59.756+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:40:59.874+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:40:59.873+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:41:00.032+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.518 seconds
[2022-12-16T15:41:10.978+0000] {processor.py:154} INFO - Started process (PID=5846) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:41:11.007+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:41:11.017+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:41:11.010+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:41:11.213+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:41:11.814+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T15:41:22.266+0000] {processor.py:154} INFO - Started process (PID=5863) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:41:22.292+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:41:22.297+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:41:22.295+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:41:22.394+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:41:23.647+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:41:23.646+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:41:23.793+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:41:23.792+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:41:24.197+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.969 seconds
[2022-12-16T15:41:34.500+0000] {processor.py:154} INFO - Started process (PID=5873) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:41:34.525+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:41:34.529+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:41:34.528+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:41:34.614+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:41:34.743+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:41:34.742+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:41:34.854+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:41:34.853+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:41:34.992+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.506 seconds
[2022-12-16T15:41:45.358+0000] {processor.py:154} INFO - Started process (PID=5883) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:41:45.384+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:41:45.388+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:41:45.387+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:41:45.472+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:41:45.605+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:41:45.604+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:41:45.724+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:41:45.723+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:41:45.894+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.550 seconds
[2022-12-16T15:41:56.106+0000] {processor.py:154} INFO - Started process (PID=5893) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:41:56.299+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:41:56.303+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:41:56.302+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:41:56.393+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:41:56.607+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:41:56.605+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:41:56.802+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:41:56.801+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:41:57.227+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.164 seconds
[2022-12-16T15:42:07.685+0000] {processor.py:154} INFO - Started process (PID=5911) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:42:07.697+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:42:07.705+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:42:07.704+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:42:07.881+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:42:08.098+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:42:08.097+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:42:08.243+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:42:08.242+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:42:08.412+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.796 seconds
[2022-12-16T15:42:19.125+0000] {processor.py:154} INFO - Started process (PID=5921) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:42:19.165+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:42:19.170+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:42:19.169+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:42:19.267+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:42:20.393+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:42:20.393+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:42:20.528+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:42:20.527+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:42:20.666+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.576 seconds
[2022-12-16T15:42:31.462+0000] {processor.py:154} INFO - Started process (PID=5931) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:42:31.491+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:42:31.496+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:42:31.495+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:42:31.580+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:42:31.713+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:42:31.712+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:42:31.834+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:42:31.833+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:42:31.968+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.519 seconds
[2022-12-16T15:42:42.357+0000] {processor.py:154} INFO - Started process (PID=5950) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:42:42.449+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:42:42.470+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:42:42.469+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:42:42.665+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:42:42.839+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:42:42.838+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:42:42.974+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:42:42.974+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:42:43.086+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.805 seconds
[2022-12-16T15:42:53.476+0000] {processor.py:154} INFO - Started process (PID=5960) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:42:53.519+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:42:53.524+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:42:53.523+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:42:53.624+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:42:54.325+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:42:54.324+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:42:54.440+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:42:54.440+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:42:54.560+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.102 seconds
[2022-12-16T15:43:04.926+0000] {processor.py:154} INFO - Started process (PID=5970) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:43:04.977+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:43:04.982+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:43:04.980+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:43:05.066+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:43:05.234+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:43:05.233+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:43:05.357+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:43:05.356+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:43:05.465+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.553 seconds
[2022-12-16T15:43:15.735+0000] {processor.py:154} INFO - Started process (PID=5980) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:43:15.739+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:43:15.743+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:43:15.742+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:43:15.829+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:43:15.991+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:43:15.990+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:43:16.113+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:43:16.112+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:43:16.256+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.538 seconds
[2022-12-16T15:43:27.174+0000] {processor.py:154} INFO - Started process (PID=5999) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:43:27.205+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:43:27.208+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:43:27.207+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:43:27.374+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:43:27.593+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:43:27.592+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:43:27.725+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:43:27.724+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:43:27.945+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.810 seconds
[2022-12-16T15:43:38.425+0000] {processor.py:154} INFO - Started process (PID=6009) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:43:38.441+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:43:38.445+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:43:38.444+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:43:38.535+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:43:39.174+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:43:39.173+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:43:39.460+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:43:39.455+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:43:39.807+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.395 seconds
[2022-12-16T15:43:50.521+0000] {processor.py:154} INFO - Started process (PID=6019) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:43:50.551+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:43:50.560+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:43:50.554+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:43:50.755+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:43:50.951+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:43:50.950+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:43:51.147+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:43:51.146+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:43:51.330+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.869 seconds
[2022-12-16T15:44:01.706+0000] {processor.py:154} INFO - Started process (PID=6029) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:44:01.809+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:44:01.814+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:44:01.813+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:44:01.914+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:44:02.109+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:44:02.108+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:44:02.519+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:44:02.506+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:44:02.893+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.206 seconds
[2022-12-16T15:44:13.182+0000] {processor.py:154} INFO - Started process (PID=6048) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:44:13.212+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:44:13.224+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:44:13.223+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:44:13.500+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:44:13.912+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:44:13.911+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:44:14.217+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:44:14.216+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:44:14.579+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.417 seconds
[2022-12-16T15:44:25.119+0000] {processor.py:154} INFO - Started process (PID=6058) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:44:25.145+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:44:25.149+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:44:25.148+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:44:25.245+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:44:25.557+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:44:25.554+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:44:25.743+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:44:25.742+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:44:26.024+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.924 seconds
[2022-12-16T15:44:36.465+0000] {processor.py:154} INFO - Started process (PID=6068) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:44:36.491+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:44:36.495+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:44:36.494+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:44:36.595+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:44:36.765+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:44:36.764+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:44:36.891+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:44:36.890+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:44:37.049+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.598 seconds
[2022-12-16T15:44:47.566+0000] {processor.py:154} INFO - Started process (PID=6085) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:44:47.617+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:44:47.625+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:44:47.624+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:44:47.792+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:44:48.049+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:44:48.048+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:44:48.297+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:44:48.296+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:44:48.577+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.061 seconds
[2022-12-16T15:44:58.826+0000] {processor.py:154} INFO - Started process (PID=6096) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:44:58.931+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:44:58.936+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:44:58.934+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:44:59.022+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:44:59.170+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:44:59.169+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:44:59.375+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:44:59.374+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:44:59.499+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.685 seconds
[2022-12-16T15:45:09.691+0000] {processor.py:154} INFO - Started process (PID=6106) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:45:09.694+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:45:09.699+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:45:09.698+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:45:09.784+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:45:09.925+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:45:09.924+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:45:10.044+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:45:10.043+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:45:10.265+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.588 seconds
[2022-12-16T15:45:20.600+0000] {processor.py:154} INFO - Started process (PID=6116) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:45:20.649+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:45:20.653+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:45:20.652+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:45:20.743+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:45:20.995+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:45:20.994+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:45:21.129+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:45:21.128+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:45:21.315+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.729 seconds
[2022-12-16T15:45:31.747+0000] {processor.py:154} INFO - Started process (PID=6134) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:45:31.777+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:45:31.790+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:45:31.789+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:45:31.980+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:45:32.433+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T15:45:43.313+0000] {processor.py:154} INFO - Started process (PID=6144) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:45:43.343+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:45:43.346+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:45:43.345+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:45:43.500+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:45:44.073+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T15:45:55.033+0000] {processor.py:154} INFO - Started process (PID=6154) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:45:55.054+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:45:55.068+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:45:55.058+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:45:55.268+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:45:55.415+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:45:55.414+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:45:55.536+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:45:55.536+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:45:55.642+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.668 seconds
[2022-12-16T15:46:06.344+0000] {processor.py:154} INFO - Started process (PID=6164) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:46:06.512+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:46:06.517+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:46:06.516+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:46:06.608+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:46:06.830+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:46:06.829+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:46:06.978+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:46:06.977+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:46:07.601+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.271 seconds
[2022-12-16T15:46:18.158+0000] {processor.py:154} INFO - Started process (PID=6182) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:46:18.169+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:46:18.173+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:46:18.172+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:46:18.304+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:46:19.018+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:46:19.017+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:46:19.269+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:46:19.268+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:46:19.453+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.309 seconds
[2022-12-16T15:46:29.759+0000] {processor.py:154} INFO - Started process (PID=6192) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:46:29.766+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:46:29.770+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:46:29.769+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:46:29.906+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:46:31.367+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:46:31.366+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:46:31.481+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:46:31.480+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:46:31.597+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.857 seconds
[2022-12-16T15:46:41.910+0000] {processor.py:154} INFO - Started process (PID=6202) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:46:41.914+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:46:41.918+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:46:41.917+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:46:42.005+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:46:42.173+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:46:42.172+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:46:42.452+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:46:42.451+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:46:42.653+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.757 seconds
[2022-12-16T15:46:53.137+0000] {processor.py:154} INFO - Started process (PID=6220) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:46:53.185+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:46:53.190+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:46:53.189+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:46:53.318+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:46:53.544+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:46:53.542+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:46:53.712+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:46:53.711+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:46:53.953+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.832 seconds
[2022-12-16T15:47:04.476+0000] {processor.py:154} INFO - Started process (PID=6230) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:47:04.487+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:47:04.492+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:47:04.490+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:47:04.585+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:47:05.046+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:47:05.045+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:47:05.159+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:47:05.158+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:47:05.364+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.912 seconds
[2022-12-16T15:47:15.612+0000] {processor.py:154} INFO - Started process (PID=6240) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:47:15.668+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:47:15.673+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:47:15.671+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:47:15.905+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:47:16.509+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:47:16.508+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:47:16.673+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:47:16.672+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:47:16.817+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.220 seconds
[2022-12-16T15:47:27.013+0000] {processor.py:154} INFO - Started process (PID=6250) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:47:27.026+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:47:27.030+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:47:27.029+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:47:27.121+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:47:28.209+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T15:47:38.730+0000] {processor.py:154} INFO - Started process (PID=6269) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:47:38.760+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:47:38.765+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:47:38.764+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:47:38.861+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:47:39.897+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:47:39.896+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:47:40.034+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:47:40.033+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:47:40.204+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.489 seconds
[2022-12-16T15:47:50.341+0000] {processor.py:154} INFO - Started process (PID=6279) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:47:50.350+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:47:50.354+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:47:50.353+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:47:50.442+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:47:50.677+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:47:50.676+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:47:50.823+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:47:50.822+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:47:51.010+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.683 seconds
[2022-12-16T15:48:01.353+0000] {processor.py:154} INFO - Started process (PID=6289) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:48:01.368+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:48:01.373+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:48:01.372+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:48:01.515+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:48:01.688+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:48:01.687+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:48:01.800+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:48:01.800+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:48:01.916+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.595 seconds
[2022-12-16T15:48:12.303+0000] {processor.py:154} INFO - Started process (PID=6305) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:48:12.380+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:48:12.392+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:48:12.391+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:48:12.500+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:48:12.666+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:48:12.664+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:48:12.806+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:48:12.805+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:48:13.131+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.844 seconds
[2022-12-16T15:48:23.433+0000] {processor.py:154} INFO - Started process (PID=6315) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:48:23.442+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:48:23.452+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:48:23.451+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:48:23.577+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:48:24.318+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:48:24.317+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:48:24.452+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:48:24.451+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:48:24.600+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.190 seconds
[2022-12-16T15:48:34.894+0000] {processor.py:154} INFO - Started process (PID=6325) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:48:34.903+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:48:34.912+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:48:34.910+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:48:35.031+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:48:35.724+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:48:35.723+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:48:35.850+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:48:35.849+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:48:35.999+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.120 seconds
[2022-12-16T15:48:46.340+0000] {processor.py:154} INFO - Started process (PID=6335) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:48:46.358+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:48:46.367+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:48:46.361+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:48:46.509+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:48:46.958+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:48:46.956+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:48:47.196+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:48:47.195+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:48:47.316+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.015 seconds
[2022-12-16T15:48:57.664+0000] {processor.py:154} INFO - Started process (PID=6354) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:48:57.678+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:48:57.682+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:48:57.681+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:48:57.871+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:48:59.609+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:48:59.608+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:48:59.806+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:48:59.805+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:49:00.003+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.366 seconds
[2022-12-16T15:49:10.296+0000] {processor.py:154} INFO - Started process (PID=6366) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:49:10.463+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:49:10.467+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:49:10.466+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:49:10.552+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:49:10.711+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:49:10.710+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:49:10.884+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:49:10.883+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:49:11.064+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.787 seconds
[2022-12-16T15:49:21.584+0000] {processor.py:154} INFO - Started process (PID=6376) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:49:21.610+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:49:21.614+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:49:21.613+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:49:21.699+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:49:21.863+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:49:21.862+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:49:22.007+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:49:22.006+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:49:22.130+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.561 seconds
[2022-12-16T15:49:32.272+0000] {processor.py:154} INFO - Started process (PID=6393) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:49:32.276+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:49:32.281+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:49:32.280+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:49:32.544+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:49:32.980+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:49:32.979+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:49:33.138+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:49:33.138+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:49:33.371+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.115 seconds
[2022-12-16T15:49:43.634+0000] {processor.py:154} INFO - Started process (PID=6404) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:49:43.638+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:49:43.641+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:49:43.640+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:49:43.752+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:49:44.412+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:49:44.411+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:49:44.535+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:49:44.534+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:49:44.652+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.036 seconds
[2022-12-16T15:49:55.104+0000] {processor.py:154} INFO - Started process (PID=6412) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:49:55.163+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:49:55.167+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:49:55.166+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:49:55.314+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:49:57.000+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:49:56.999+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:49:57.234+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:49:57.233+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:49:57.513+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.440 seconds
[2022-12-16T15:50:07.855+0000] {processor.py:154} INFO - Started process (PID=6424) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:50:07.883+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:50:07.886+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:50:07.885+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:50:08.069+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:50:08.565+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:50:08.564+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:50:08.903+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:50:08.902+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:50:09.232+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.398 seconds
[2022-12-16T15:50:20.120+0000] {processor.py:154} INFO - Started process (PID=6442) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:50:20.134+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:50:20.139+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:50:20.137+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:50:20.458+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:50:21.932+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:50:21.931+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:50:22.238+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:50:22.237+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:50:22.920+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.836 seconds
[2022-12-16T15:50:33.488+0000] {processor.py:154} INFO - Started process (PID=6452) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:50:33.511+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:50:33.517+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:50:33.516+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:50:33.617+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:50:34.006+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:50:34.005+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:50:34.132+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:50:34.131+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:50:34.375+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.906 seconds
[2022-12-16T15:50:44.842+0000] {processor.py:154} INFO - Started process (PID=6462) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:50:44.920+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:50:44.924+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:50:44.923+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:50:45.038+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:50:45.368+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:50:45.366+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:50:45.544+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:50:45.542+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:50:45.705+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.916 seconds
[2022-12-16T15:50:56.164+0000] {processor.py:154} INFO - Started process (PID=6472) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:50:56.200+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:50:56.205+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:50:56.204+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:50:56.322+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:50:56.702+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:50:56.701+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:50:56.956+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:50:56.956+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:50:57.069+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.945 seconds
[2022-12-16T15:51:07.724+0000] {processor.py:154} INFO - Started process (PID=6490) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:51:07.736+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:51:07.749+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:51:07.748+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:51:08.239+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:51:08.972+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:51:08.965+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:51:09.280+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:51:09.279+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:51:09.597+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.953 seconds
[2022-12-16T15:51:20.067+0000] {processor.py:154} INFO - Started process (PID=6500) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:51:20.108+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:51:20.113+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:51:20.112+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:51:20.393+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:51:20.762+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:51:20.758+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:51:21.018+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:51:21.001+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:51:21.219+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.184 seconds
[2022-12-16T15:51:31.616+0000] {processor.py:154} INFO - Started process (PID=6510) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:51:31.636+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:51:31.641+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:51:31.640+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:51:31.921+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:51:33.168+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T15:51:44.110+0000] {processor.py:154} INFO - Started process (PID=6528) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:51:44.202+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:51:44.206+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:51:44.205+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:51:44.502+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:51:45.686+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T15:51:57.075+0000] {processor.py:154} INFO - Started process (PID=6538) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:51:57.112+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:51:57.121+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:51:57.120+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:51:57.536+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:51:58.025+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:51:58.024+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:51:58.940+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:51:58.939+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:51:59.607+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.629 seconds
[2022-12-16T15:52:10.773+0000] {processor.py:154} INFO - Started process (PID=6548) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:52:10.784+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:52:10.799+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:52:10.798+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:52:10.951+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:52:11.495+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:52:11.494+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:52:11.909+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:52:11.908+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:52:12.231+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.505 seconds
[2022-12-16T15:52:22.509+0000] {processor.py:154} INFO - Started process (PID=6558) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:52:22.519+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:52:22.530+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:52:22.529+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:52:22.775+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:52:23.090+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:52:23.085+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:52:23.345+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:52:23.344+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:52:23.818+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.335 seconds
[2022-12-16T15:52:34.175+0000] {processor.py:154} INFO - Started process (PID=6575) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:52:34.178+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:52:34.191+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:52:34.190+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:52:34.538+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:52:36.000+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:52:35.999+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:52:36.633+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:52:36.632+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:52:37.527+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.403 seconds
[2022-12-16T15:52:48.154+0000] {processor.py:154} INFO - Started process (PID=6586) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:52:48.163+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:52:48.180+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:52:48.179+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:52:48.553+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:52:49.400+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:52:49.399+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:52:49.838+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:52:49.829+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:52:50.400+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.305 seconds
[2022-12-16T15:53:00.917+0000] {processor.py:154} INFO - Started process (PID=6596) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:53:00.962+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:53:00.970+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:53:00.964+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:53:01.238+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:53:02.552+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T15:53:13.525+0000] {processor.py:154} INFO - Started process (PID=6606) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:53:13.537+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:53:13.551+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:53:13.550+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:53:13.877+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:53:14.905+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:53:14.904+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:53:15.123+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:53:15.122+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:53:15.359+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.907 seconds
[2022-12-16T15:53:25.746+0000] {processor.py:154} INFO - Started process (PID=6616) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:53:25.755+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:53:25.767+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:53:25.766+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:53:26.062+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:53:26.396+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:53:26.395+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:53:26.681+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:53:26.680+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:53:27.121+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.418 seconds
[2022-12-16T15:53:37.723+0000] {processor.py:154} INFO - Started process (PID=6633) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:53:37.796+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:53:37.817+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:53:37.816+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:53:38.292+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:53:39.804+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:53:39.803+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:53:40.236+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:53:40.235+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:53:40.597+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.017 seconds
[2022-12-16T15:53:51.374+0000] {processor.py:154} INFO - Started process (PID=6644) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:53:51.392+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:53:51.404+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:53:51.403+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:53:51.789+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:53:52.319+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:53:52.318+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:53:52.739+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:53:52.738+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:53:53.000+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.665 seconds
[2022-12-16T15:54:04.083+0000] {processor.py:154} INFO - Started process (PID=6654) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:54:04.116+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:54:04.147+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:54:04.130+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:54:04.432+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:54:05.093+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:54:05.092+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:54:05.393+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:54:05.392+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:54:06.067+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.104 seconds
[2022-12-16T15:54:16.769+0000] {processor.py:154} INFO - Started process (PID=6664) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:54:16.827+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:54:16.839+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:54:16.838+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:54:17.200+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:54:17.443+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:54:17.442+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:54:17.655+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:54:17.654+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:54:17.908+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.230 seconds
[2022-12-16T15:54:28.475+0000] {processor.py:154} INFO - Started process (PID=6674) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:54:28.484+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:54:28.488+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:54:28.487+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:54:28.656+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:54:29.492+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:54:29.491+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:54:29.708+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:54:29.707+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:54:29.915+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.461 seconds
[2022-12-16T15:54:40.384+0000] {processor.py:154} INFO - Started process (PID=6691) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:54:40.393+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:54:40.405+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:54:40.404+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:54:40.870+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:54:41.762+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:54:41.757+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:54:42.145+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:54:42.133+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:54:42.531+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.188 seconds
[2022-12-16T15:54:53.043+0000] {processor.py:154} INFO - Started process (PID=6701) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:54:53.053+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:54:53.061+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:54:53.060+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:54:53.536+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:54:54.458+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:54:54.441+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:54:55.106+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:54:55.105+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:54:55.452+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.450 seconds
[2022-12-16T15:55:05.999+0000] {processor.py:154} INFO - Started process (PID=6711) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:55:06.002+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:55:06.011+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:55:06.010+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:55:06.279+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:55:06.689+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:55:06.682+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:55:07.173+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:55:07.172+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:55:07.476+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.512 seconds
[2022-12-16T15:55:17.812+0000] {processor.py:154} INFO - Started process (PID=6721) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:55:17.857+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:55:17.862+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:55:17.860+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:55:18.018+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:55:18.239+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:55:18.238+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:55:18.466+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:55:18.465+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:55:18.800+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.023 seconds
[2022-12-16T15:55:29.281+0000] {processor.py:154} INFO - Started process (PID=6739) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:55:29.297+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:55:29.320+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:55:29.319+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:55:29.636+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:55:30.963+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:55:30.962+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:55:31.532+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:55:31.531+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:55:31.958+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.755 seconds
[2022-12-16T15:55:42.549+0000] {processor.py:154} INFO - Started process (PID=6750) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:55:42.568+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:55:42.572+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:55:42.571+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:55:42.837+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:55:43.572+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:55:43.571+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:55:43.900+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:55:43.899+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:55:44.168+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.686 seconds
[2022-12-16T15:55:54.812+0000] {processor.py:154} INFO - Started process (PID=6760) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:55:54.901+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:55:54.908+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:55:54.904+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:55:55.119+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:55:55.339+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:55:55.338+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:55:55.485+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:55:55.484+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:55:55.693+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.921 seconds
[2022-12-16T15:56:06.083+0000] {processor.py:154} INFO - Started process (PID=6770) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:56:06.114+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:56:06.122+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:56:06.117+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:56:06.252+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:56:06.847+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:56:06.839+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:56:07.319+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:56:07.313+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:56:07.483+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.422 seconds
[2022-12-16T15:56:18.058+0000] {processor.py:154} INFO - Started process (PID=6788) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:56:18.071+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:56:18.108+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:56:18.107+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:56:18.466+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:56:19.050+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:56:19.048+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:56:19.467+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:56:19.466+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:56:20.399+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.471 seconds
[2022-12-16T15:56:31.338+0000] {processor.py:154} INFO - Started process (PID=6799) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:56:31.349+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:56:31.366+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:56:31.365+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:56:31.656+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:56:32.534+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:56:32.532+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:56:32.971+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:56:32.971+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:56:33.634+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.352 seconds
[2022-12-16T15:56:44.643+0000] {processor.py:154} INFO - Started process (PID=6809) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:56:44.649+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:56:44.679+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:56:44.657+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:56:45.115+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:56:45.359+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:56:45.357+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:56:45.556+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:56:45.555+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:56:45.850+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.263 seconds
[2022-12-16T15:56:56.487+0000] {processor.py:154} INFO - Started process (PID=6819) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:56:56.499+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:56:56.519+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:56:56.507+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:56:56.794+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:56:57.076+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:56:57.074+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:56:57.263+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:56:57.262+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:56:57.465+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.041 seconds
[2022-12-16T15:57:07.823+0000] {processor.py:154} INFO - Started process (PID=6829) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:57:07.833+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:57:07.843+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:57:07.842+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:57:08.049+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:57:09.552+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T15:57:20.843+0000] {processor.py:154} INFO - Started process (PID=6847) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:57:20.872+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:57:20.876+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:57:20.875+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:57:21.209+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:57:21.591+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:57:21.590+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:57:22.061+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:57:22.060+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:57:22.505+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.710 seconds
[2022-12-16T15:57:33.016+0000] {processor.py:154} INFO - Started process (PID=6857) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:57:33.027+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:57:33.038+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:57:33.037+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:57:33.208+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:57:33.486+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:57:33.485+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:57:33.800+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:57:33.799+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:57:34.441+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.450 seconds
[2022-12-16T15:57:45.027+0000] {processor.py:154} INFO - Started process (PID=6867) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:57:45.032+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:57:45.036+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:57:45.035+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:57:45.361+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:57:46.275+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:57:46.274+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:57:46.633+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:57:46.631+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:57:47.161+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.184 seconds
[2022-12-16T15:57:57.844+0000] {processor.py:154} INFO - Started process (PID=6877) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:57:57.852+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:57:57.861+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:57:57.860+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:57:58.046+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:57:58.495+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:57:58.494+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:57:58.874+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:57:58.867+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:57:59.224+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.410 seconds
[2022-12-16T15:58:09.735+0000] {processor.py:154} INFO - Started process (PID=6895) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:58:09.745+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:58:09.755+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:58:09.754+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:58:09.996+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:58:10.491+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:58:10.491+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:58:10.832+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:58:10.829+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:58:11.147+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.465 seconds
[2022-12-16T15:58:21.469+0000] {processor.py:154} INFO - Started process (PID=6905) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:58:21.516+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:58:21.526+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:58:21.520+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:58:21.795+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:58:22.021+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:58:22.020+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:58:22.196+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:58:22.195+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:58:22.387+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.963 seconds
[2022-12-16T15:58:33.112+0000] {processor.py:154} INFO - Started process (PID=6913) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:58:33.158+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:58:33.172+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:58:33.171+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:58:33.826+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:58:34.173+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:58:34.172+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:58:34.395+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:58:34.393+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:58:34.712+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.685 seconds
[2022-12-16T15:58:45.489+0000] {processor.py:154} INFO - Started process (PID=6923) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:58:45.548+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:58:45.564+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:58:45.563+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:58:46.200+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:58:46.836+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:58:46.831+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:58:47.033+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:58:47.032+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:58:47.221+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.827 seconds
[2022-12-16T15:58:57.694+0000] {processor.py:154} INFO - Started process (PID=6940) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:58:57.748+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:58:57.759+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:58:57.757+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:58:58.394+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:58:59.541+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:58:59.532+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:59:00.160+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:59:00.155+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:59:00.506+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.878 seconds
[2022-12-16T15:59:11.399+0000] {processor.py:154} INFO - Started process (PID=6950) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:59:11.403+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:59:11.436+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:59:11.434+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:59:12.162+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:59:13.948+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:59:13.947+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:59:14.151+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:59:14.148+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:59:14.363+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.006 seconds
[2022-12-16T15:59:25.062+0000] {processor.py:154} INFO - Started process (PID=6962) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:59:25.075+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:59:25.084+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:59:25.083+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:59:25.380+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:59:26.166+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T15:59:36.849+0000] {processor.py:154} INFO - Started process (PID=6970) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:59:36.892+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:59:36.896+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:59:36.895+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:59:37.285+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:59:38.542+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:59:38.541+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:59:38.973+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:59:38.972+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:59:39.218+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.418 seconds
[2022-12-16T15:59:49.552+0000] {processor.py:154} INFO - Started process (PID=6990) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:59:49.555+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T15:59:49.564+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:59:49.563+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:59:49.840+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T15:59:50.771+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:59:50.770+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T15:59:51.170+0000] {logging_mixin.py:137} INFO - [2022-12-16T15:59:51.169+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T15:59:51.411+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.894 seconds
[2022-12-16T16:00:02.391+0000] {processor.py:154} INFO - Started process (PID=6998) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:00:02.396+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:00:02.401+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:00:02.400+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:00:02.572+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:00:03.254+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:00:03.253+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:00:03.576+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:00:03.575+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:00:03.844+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.493 seconds
[2022-12-16T16:00:14.420+0000] {processor.py:154} INFO - Started process (PID=7010) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:00:14.425+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:00:14.433+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:00:14.432+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:00:14.591+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:00:15.977+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:00:15.976+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:00:16.232+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:00:16.231+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:00:16.779+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.385 seconds
[2022-12-16T16:00:27.554+0000] {processor.py:154} INFO - Started process (PID=7020) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:00:27.565+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:00:27.570+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:00:27.568+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:00:27.712+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:00:29.067+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T16:00:39.847+0000] {processor.py:154} INFO - Started process (PID=7038) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:00:39.910+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:00:39.920+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:00:39.918+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:00:40.661+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:00:42.470+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:00:42.464+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:00:42.780+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:00:42.779+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:00:43.117+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.331 seconds
[2022-12-16T16:00:53.955+0000] {processor.py:154} INFO - Started process (PID=7046) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:00:54.005+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:00:54.045+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:00:54.008+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:00:54.475+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:00:55.061+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T16:01:05.392+0000] {processor.py:154} INFO - Started process (PID=7056) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:01:05.401+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:01:05.408+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:01:05.407+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:01:05.908+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:01:06.823+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:01:06.822+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:01:07.413+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:01:07.412+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:01:07.674+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.313 seconds
[2022-12-16T16:01:18.162+0000] {processor.py:154} INFO - Started process (PID=7066) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:01:18.172+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:01:18.179+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:01:18.175+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:01:18.395+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:01:19.917+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T16:01:30.738+0000] {processor.py:154} INFO - Started process (PID=7083) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:01:30.760+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:01:30.788+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:01:30.787+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:01:31.687+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:01:33.179+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T16:01:44.294+0000] {processor.py:154} INFO - Started process (PID=7093) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:01:44.349+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:01:44.363+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:01:44.362+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:01:44.865+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:01:45.348+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:01:45.347+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:01:45.836+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:01:45.835+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:01:46.595+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.370 seconds
[2022-12-16T16:01:57.607+0000] {processor.py:154} INFO - Started process (PID=7102) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:01:57.617+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:01:57.622+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:01:57.621+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:01:57.833+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:01:58.473+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:01:58.472+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:01:58.878+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:01:58.877+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:01:59.201+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.623 seconds
[2022-12-16T16:02:10.244+0000] {processor.py:154} INFO - Started process (PID=7113) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:02:10.252+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:02:10.256+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:02:10.255+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:02:10.570+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:02:11.102+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:02:11.101+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:02:11.510+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:02:11.500+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:02:12.028+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.809 seconds
[2022-12-16T16:02:23.142+0000] {processor.py:154} INFO - Started process (PID=7129) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:02:23.150+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:02:23.164+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:02:23.163+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:02:23.452+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:02:24.440+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:02:24.439+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:02:24.987+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:02:24.968+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:02:25.595+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.530 seconds
[2022-12-16T16:02:37.041+0000] {processor.py:154} INFO - Started process (PID=7142) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:02:37.053+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:02:37.062+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:02:37.057+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:02:37.289+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:02:37.858+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:02:37.856+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:02:38.309+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:02:38.308+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:02:38.651+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.633 seconds
[2022-12-16T16:02:49.017+0000] {processor.py:154} INFO - Started process (PID=7152) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:02:49.032+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:02:49.036+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:02:49.035+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:02:49.278+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:02:50.320+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:02:50.319+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:02:50.680+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:02:50.672+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:02:50.896+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.911 seconds
[2022-12-16T16:03:01.537+0000] {processor.py:154} INFO - Started process (PID=7162) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:03:01.548+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:03:01.560+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:03:01.559+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:03:01.890+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:03:02.695+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:03:02.694+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:03:03.092+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:03:03.091+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:03:03.387+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.899 seconds
[2022-12-16T16:03:14.300+0000] {processor.py:154} INFO - Started process (PID=7180) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:03:14.305+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:03:14.316+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:03:14.315+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:03:14.673+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:03:16.288+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:03:16.287+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:03:17.000+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:03:16.999+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:03:17.563+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.351 seconds
[2022-12-16T16:03:28.960+0000] {processor.py:154} INFO - Started process (PID=7192) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:03:28.972+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:03:28.988+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:03:28.987+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:03:29.237+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:03:30.756+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T16:03:41.414+0000] {processor.py:154} INFO - Started process (PID=7202) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:03:41.444+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:03:41.453+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:03:41.448+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:03:41.648+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:03:43.173+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:03:43.172+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:03:43.369+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:03:43.367+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:03:43.567+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.177 seconds
[2022-12-16T16:03:54.516+0000] {processor.py:154} INFO - Started process (PID=7219) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:03:54.553+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:03:54.574+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:03:54.556+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:03:54.831+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:03:55.130+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:03:55.121+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:03:55.534+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:03:55.533+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:03:56.258+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.818 seconds
[2022-12-16T16:04:06.670+0000] {processor.py:154} INFO - Started process (PID=7230) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:04:06.679+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:04:06.687+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:04:06.686+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:04:07.454+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:04:08.152+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:04:08.151+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:04:08.529+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:04:08.528+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:04:08.794+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.166 seconds
[2022-12-16T16:04:19.659+0000] {processor.py:154} INFO - Started process (PID=7240) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:04:19.687+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:04:19.692+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:04:19.691+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:04:19.885+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:04:20.318+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:04:20.316+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:04:20.582+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:04:20.581+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:04:21.086+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.457 seconds
[2022-12-16T16:04:31.701+0000] {processor.py:154} INFO - Started process (PID=7250) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:04:31.708+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:04:31.711+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:04:31.710+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:04:31.884+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:04:32.244+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:04:32.243+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:04:32.735+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:04:32.734+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:04:33.135+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.478 seconds
[2022-12-16T16:04:43.457+0000] {processor.py:154} INFO - Started process (PID=7260) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:04:43.465+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:04:43.473+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:04:43.469+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:04:43.596+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:04:44.112+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:04:44.111+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:04:44.463+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:04:44.462+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:04:44.868+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.453 seconds
[2022-12-16T16:04:56.124+0000] {processor.py:154} INFO - Started process (PID=7279) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:04:56.152+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:04:56.156+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:04:56.155+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:04:56.395+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:04:57.044+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:04:57.042+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:04:57.463+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:04:57.453+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:04:57.931+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.855 seconds
[2022-12-16T16:05:08.466+0000] {processor.py:154} INFO - Started process (PID=7289) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:05:08.471+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:05:08.476+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:05:08.475+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:05:08.612+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:05:09.274+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:05:09.273+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:05:09.484+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:05:09.483+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:05:09.664+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.221 seconds
[2022-12-16T16:05:20.237+0000] {processor.py:154} INFO - Started process (PID=7299) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:05:20.244+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:05:20.252+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:05:20.251+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:05:20.447+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:05:21.326+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:05:21.325+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:05:21.545+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:05:21.544+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:05:21.769+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.579 seconds
[2022-12-16T16:05:31.971+0000] {processor.py:154} INFO - Started process (PID=7309) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:05:31.976+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:05:31.987+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:05:31.986+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:05:32.155+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:05:32.602+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:05:32.601+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:05:32.896+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:05:32.895+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:05:33.174+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.228 seconds
[2022-12-16T16:05:43.515+0000] {processor.py:154} INFO - Started process (PID=7327) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:05:43.528+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:05:43.537+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:05:43.536+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:05:43.812+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:05:44.615+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:05:44.588+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:05:45.412+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:05:45.411+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:05:45.765+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.299 seconds
[2022-12-16T16:05:56.642+0000] {processor.py:154} INFO - Started process (PID=7337) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:05:56.662+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:05:56.670+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:05:56.665+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:05:56.932+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:05:57.218+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:05:57.213+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:05:57.436+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:05:57.429+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:05:57.881+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.295 seconds
[2022-12-16T16:06:08.408+0000] {processor.py:154} INFO - Started process (PID=7347) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:06:08.463+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:06:08.476+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:06:08.474+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:06:08.704+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:06:09.419+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:06:09.418+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:06:09.580+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:06:09.579+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:06:09.788+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.427 seconds
[2022-12-16T16:06:20.364+0000] {processor.py:154} INFO - Started process (PID=7357) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:06:20.376+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:06:20.407+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:06:20.405+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:06:20.814+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:06:21.909+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T16:06:32.972+0000] {processor.py:154} INFO - Started process (PID=7375) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:06:32.994+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:06:33.014+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:06:32.997+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:06:33.433+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:06:33.902+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:06:33.900+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:06:34.431+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:06:34.422+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:06:34.785+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.929 seconds
[2022-12-16T16:06:45.549+0000] {processor.py:154} INFO - Started process (PID=7385) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:06:45.571+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:06:45.591+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:06:45.589+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:06:45.766+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:06:47.837+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:06:47.836+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:06:48.250+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:06:48.249+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:06:48.695+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.208 seconds
[2022-12-16T16:06:59.341+0000] {processor.py:154} INFO - Started process (PID=7395) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:06:59.391+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:06:59.403+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:06:59.402+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:06:59.583+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:06:59.848+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:06:59.847+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:07:00.079+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:07:00.078+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:07:00.421+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.121 seconds
[2022-12-16T16:07:11.056+0000] {processor.py:154} INFO - Started process (PID=7405) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:07:11.068+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:07:11.072+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:07:11.071+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:07:11.392+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:07:12.948+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:07:12.947+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:07:13.270+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:07:13.270+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:07:13.603+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.576 seconds
[2022-12-16T16:07:24.345+0000] {processor.py:154} INFO - Started process (PID=7422) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:07:24.353+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:07:24.369+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:07:24.368+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:07:25.069+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:07:26.387+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:07:26.386+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:07:26.788+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:07:26.787+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:07:27.098+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.794 seconds
[2022-12-16T16:07:37.964+0000] {processor.py:154} INFO - Started process (PID=7433) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:07:37.976+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:07:37.981+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:07:37.980+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:07:38.249+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:07:39.291+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:07:39.289+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:07:39.574+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:07:39.572+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:07:39.966+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.061 seconds
[2022-12-16T16:07:50.749+0000] {processor.py:154} INFO - Started process (PID=7443) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:07:50.763+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:07:50.774+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:07:50.773+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:07:51.060+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:07:51.364+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:07:51.363+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:07:51.545+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:07:51.543+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:07:51.721+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.998 seconds
[2022-12-16T16:08:02.241+0000] {processor.py:154} INFO - Started process (PID=7453) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:08:02.291+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:08:02.299+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:08:02.298+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:08:02.440+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:08:02.653+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:08:02.651+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:08:02.841+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:08:02.840+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:08:03.132+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.912 seconds
[2022-12-16T16:08:13.712+0000] {processor.py:154} INFO - Started process (PID=7470) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:08:13.726+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:08:13.738+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:08:13.731+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:08:13.988+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:08:14.401+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:08:14.399+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:08:14.863+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:08:14.859+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:08:15.134+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.448 seconds
[2022-12-16T16:08:25.733+0000] {processor.py:154} INFO - Started process (PID=7481) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:08:25.741+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:08:25.757+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:08:25.756+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:08:26.240+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:08:26.904+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:08:26.903+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:08:27.349+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:08:27.347+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:08:27.765+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.075 seconds
[2022-12-16T16:08:38.563+0000] {processor.py:154} INFO - Started process (PID=7491) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:08:38.569+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:08:38.574+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:08:38.573+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:08:38.734+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:08:39.357+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:08:39.356+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:08:39.796+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:08:39.795+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:08:40.028+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.500 seconds
[2022-12-16T16:08:50.557+0000] {processor.py:154} INFO - Started process (PID=7501) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:08:50.575+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:08:50.580+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:08:50.579+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:08:50.939+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:08:51.354+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:08:51.353+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:08:51.555+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:08:51.554+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:08:51.763+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.230 seconds
[2022-12-16T16:09:02.442+0000] {processor.py:154} INFO - Started process (PID=7511) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:09:02.454+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:09:02.462+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:09:02.457+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:09:02.733+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:09:03.912+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T16:09:14.870+0000] {processor.py:154} INFO - Started process (PID=7529) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:09:14.894+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:09:14.917+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:09:14.916+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:09:15.173+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:09:16.812+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:09:16.806+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:09:17.172+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:09:17.171+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:09:17.563+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.754 seconds
[2022-12-16T16:09:28.542+0000] {processor.py:154} INFO - Started process (PID=7539) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:09:28.549+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:09:28.555+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:09:28.554+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:09:28.735+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:09:29.029+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:09:29.028+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:09:29.541+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:09:29.540+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:09:29.813+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.289 seconds
[2022-12-16T16:09:40.542+0000] {processor.py:154} INFO - Started process (PID=7549) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:09:40.547+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:09:40.551+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:09:40.550+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:09:40.731+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:09:41.089+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:09:41.088+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:09:41.290+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:09:41.287+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:09:41.579+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.076 seconds
[2022-12-16T16:09:52.180+0000] {processor.py:154} INFO - Started process (PID=7559) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:09:52.188+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:09:52.197+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:09:52.195+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:09:52.385+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:09:52.599+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:09:52.598+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:09:52.776+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:09:52.775+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:09:52.986+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.832 seconds
[2022-12-16T16:10:03.801+0000] {processor.py:154} INFO - Started process (PID=7576) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:10:03.805+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:10:03.842+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:10:03.828+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:10:04.381+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:10:04.863+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:10:04.862+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:10:05.201+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:10:05.192+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:10:05.979+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.277 seconds
[2022-12-16T16:10:16.281+0000] {processor.py:154} INFO - Started process (PID=7586) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:10:16.292+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:10:16.296+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:10:16.295+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:10:16.462+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:10:16.716+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:10:16.715+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:10:16.916+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:10:16.914+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:10:17.131+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.872 seconds
[2022-12-16T16:10:27.766+0000] {processor.py:154} INFO - Started process (PID=7596) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:10:27.814+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:10:27.818+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:10:27.817+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:10:28.012+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:10:28.259+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:10:28.258+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:10:28.581+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:10:28.580+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:10:28.888+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.178 seconds
[2022-12-16T16:10:39.449+0000] {processor.py:154} INFO - Started process (PID=7606) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:10:39.492+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:10:39.497+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:10:39.496+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:10:39.758+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:10:40.381+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:10:40.380+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:10:40.563+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:10:40.562+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:10:40.717+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.297 seconds
[2022-12-16T16:10:51.222+0000] {processor.py:154} INFO - Started process (PID=7624) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:10:51.255+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:10:51.259+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:10:51.258+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:10:51.444+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:10:51.839+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:10:51.838+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:10:51.994+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:10:51.993+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:10:52.133+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.953 seconds
[2022-12-16T16:11:02.469+0000] {processor.py:154} INFO - Started process (PID=7634) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:11:02.492+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:11:02.500+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:11:02.495+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:11:02.612+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:11:03.562+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T16:11:13.899+0000] {processor.py:154} INFO - Started process (PID=7644) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:11:14.007+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:11:14.011+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:11:14.010+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:11:14.138+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:11:14.298+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:11:14.297+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:11:14.416+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:11:14.415+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:11:14.600+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.717 seconds
[2022-12-16T16:11:25.060+0000] {processor.py:154} INFO - Started process (PID=7654) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:11:25.150+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:11:25.164+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:11:25.163+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:11:25.532+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:11:26.007+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:11:26.006+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:11:26.136+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:11:26.135+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:11:26.390+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.408 seconds
[2022-12-16T16:11:37.228+0000] {processor.py:154} INFO - Started process (PID=7672) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:11:37.647+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:11:37.656+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:11:37.655+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:11:37.888+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:11:38.324+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:11:38.321+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:11:38.795+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:11:38.794+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:11:39.519+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.341 seconds
[2022-12-16T16:11:50.593+0000] {processor.py:154} INFO - Started process (PID=7682) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:11:50.600+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:11:50.609+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:11:50.608+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:11:50.704+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:11:51.229+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T16:12:01.762+0000] {processor.py:154} INFO - Started process (PID=7692) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:12:01.766+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:12:01.776+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:12:01.775+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:12:01.888+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:12:02.189+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:12:02.187+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:12:02.372+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:12:02.371+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:12:02.594+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.850 seconds
[2022-12-16T16:12:12.968+0000] {processor.py:154} INFO - Started process (PID=7702) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:12:12.972+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:12:12.976+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:12:12.975+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:12:13.069+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:12:13.301+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:12:13.300+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:12:13.440+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:12:13.439+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:12:13.553+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.599 seconds
[2022-12-16T16:13:09.472+0000] {processor.py:154} INFO - Started process (PID=176) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:13:09.495+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:13:09.512+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:13:09.511+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:13:09.891+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:13:11.003+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:13:11.002+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:13:11.440+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:13:11.439+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:13:12.151+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.722 seconds
[2022-12-16T16:13:22.936+0000] {processor.py:154} INFO - Started process (PID=186) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:13:22.968+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:13:22.977+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:13:22.973+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:13:23.525+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:13:24.534+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:13:24.533+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:13:24.647+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:13:24.646+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:13:24.837+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.960 seconds
[2022-12-16T16:13:35.118+0000] {processor.py:154} INFO - Started process (PID=196) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:13:35.139+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:13:35.143+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:13:35.142+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:13:35.268+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:13:35.483+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:13:35.482+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:13:35.772+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:13:35.771+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:13:36.027+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.928 seconds
[2022-12-16T16:13:46.566+0000] {processor.py:154} INFO - Started process (PID=212) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:13:46.630+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:13:46.662+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:13:46.636+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:13:46.956+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:13:47.605+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:13:47.604+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:13:47.879+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:13:47.864+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:13:48.221+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.809 seconds
[2022-12-16T16:13:58.928+0000] {processor.py:154} INFO - Started process (PID=223) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:13:58.972+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:13:58.976+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:13:58.975+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:13:59.264+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:13:59.441+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:13:59.440+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:13:59.576+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:13:59.575+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:13:59.715+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.808 seconds
[2022-12-16T16:14:10.107+0000] {processor.py:154} INFO - Started process (PID=233) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:14:10.122+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:14:10.126+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:14:10.125+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:14:10.237+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:14:11.180+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:14:11.179+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:14:11.327+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:14:11.327+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:14:11.478+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.402 seconds
[2022-12-16T16:14:21.758+0000] {processor.py:154} INFO - Started process (PID=243) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:14:21.779+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:14:21.791+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:14:21.790+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:14:21.894+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:14:22.678+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:14:22.677+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:14:22.807+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:14:22.806+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:14:22.963+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.220 seconds
[2022-12-16T16:14:33.445+0000] {processor.py:154} INFO - Started process (PID=261) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:14:33.451+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:14:33.457+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:14:33.454+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:14:33.732+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:14:34.975+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:14:34.974+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:14:35.201+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:14:35.200+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:14:35.460+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.031 seconds
[2022-12-16T16:14:45.915+0000] {processor.py:154} INFO - Started process (PID=271) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:14:45.922+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:14:45.938+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:14:45.931+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:14:46.183+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:14:46.688+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:14:46.681+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:14:47.155+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:14:47.154+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:14:47.727+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.863 seconds
[2022-12-16T16:14:57.895+0000] {processor.py:154} INFO - Started process (PID=283) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:14:57.899+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:14:57.903+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:14:57.902+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:14:58.007+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:14:58.166+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:14:58.164+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:14:58.323+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:14:58.322+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:14:58.798+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.917 seconds
[2022-12-16T16:15:09.716+0000] {processor.py:154} INFO - Started process (PID=298) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:15:09.727+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:15:09.745+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:15:09.736+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:15:10.145+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:15:10.698+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:15:10.695+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:15:10.998+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:15:10.997+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:15:11.255+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.704 seconds
[2022-12-16T16:15:21.899+0000] {processor.py:154} INFO - Started process (PID=311) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:15:21.928+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:15:21.932+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:15:21.931+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:15:22.086+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:15:22.536+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:15:22.535+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:15:22.758+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:15:22.757+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:15:22.903+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.020 seconds
[2022-12-16T16:15:33.206+0000] {processor.py:154} INFO - Started process (PID=321) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:15:33.210+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:15:33.214+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:15:33.213+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:15:33.304+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:15:33.497+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:15:33.495+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:15:33.624+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:15:33.623+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:15:33.772+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.580 seconds
[2022-12-16T16:15:44.015+0000] {processor.py:154} INFO - Started process (PID=331) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:15:44.030+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:15:44.036+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:15:44.033+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:15:44.149+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:15:44.802+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:15:44.801+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:15:44.925+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:15:44.924+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:15:45.067+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.076 seconds
[2022-12-16T16:15:55.581+0000] {processor.py:154} INFO - Started process (PID=349) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:15:55.588+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:15:55.614+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:15:55.601+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:15:55.754+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:15:57.510+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T16:16:08.151+0000] {processor.py:154} INFO - Started process (PID=359) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:16:08.175+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:16:08.196+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:16:08.195+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:16:08.484+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:16:09.359+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:16:09.358+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:16:09.534+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:16:09.533+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:16:09.730+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.635 seconds
[2022-12-16T16:16:20.230+0000] {processor.py:154} INFO - Started process (PID=369) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:16:20.257+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:16:20.266+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:16:20.260+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:16:20.523+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:16:21.301+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:16:21.300+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:16:21.434+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:16:21.433+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:16:21.828+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.634 seconds
[2022-12-16T16:16:33.080+0000] {processor.py:154} INFO - Started process (PID=386) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:16:33.113+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:16:33.118+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:16:33.117+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:16:33.316+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:16:33.588+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:16:33.586+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:16:33.763+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:16:33.760+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:16:33.971+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.932 seconds
[2022-12-16T16:16:44.540+0000] {processor.py:154} INFO - Started process (PID=397) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:16:44.572+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:16:44.580+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:16:44.579+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:16:44.820+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:16:45.555+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:16:45.553+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:16:45.847+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:16:45.846+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:16:46.063+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.559 seconds
[2022-12-16T16:16:56.790+0000] {processor.py:154} INFO - Started process (PID=407) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:16:56.812+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:16:56.816+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:16:56.815+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:16:57.126+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:16:57.373+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:16:57.372+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:16:57.550+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:16:57.549+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:16:57.727+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.989 seconds
[2022-12-16T16:17:08.093+0000] {processor.py:154} INFO - Started process (PID=417) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:17:08.098+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:17:08.105+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:17:08.101+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:17:08.331+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:17:08.582+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:17:08.569+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:17:09.108+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:17:09.107+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:17:09.322+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.253 seconds
[2022-12-16T16:17:19.580+0000] {processor.py:154} INFO - Started process (PID=434) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:17:19.608+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:17:19.617+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:17:19.616+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:17:19.769+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:17:20.110+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:17:20.108+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:17:20.468+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:17:20.463+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:17:20.955+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.407 seconds
[2022-12-16T16:17:31.229+0000] {processor.py:154} INFO - Started process (PID=444) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:17:31.261+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:17:31.265+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:17:31.264+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:17:31.391+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:17:31.554+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:17:31.553+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:17:31.670+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:17:31.669+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:17:31.819+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.612 seconds
[2022-12-16T16:17:42.443+0000] {processor.py:154} INFO - Started process (PID=454) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:17:42.483+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:17:42.487+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:17:42.486+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:17:42.580+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:17:42.860+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:17:42.858+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:17:43.040+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:17:43.038+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:17:43.196+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.767 seconds
[2022-12-16T16:17:53.590+0000] {processor.py:154} INFO - Started process (PID=464) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:17:53.628+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:17:53.652+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:17:53.640+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:17:53.803+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:17:54.483+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T16:18:04.928+0000] {processor.py:154} INFO - Started process (PID=482) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:18:04.934+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:18:04.938+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:18:04.937+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:18:05.073+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:18:05.313+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:18:05.312+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:18:05.490+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:18:05.489+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:18:05.695+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.787 seconds
[2022-12-16T16:18:16.524+0000] {processor.py:154} INFO - Started process (PID=492) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:18:16.549+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:18:16.554+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:18:16.553+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:18:16.641+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:18:17.288+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:18:17.287+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:18:17.416+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:18:17.415+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:18:17.622+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.110 seconds
[2022-12-16T16:18:27.797+0000] {processor.py:154} INFO - Started process (PID=502) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:18:27.849+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:18:27.853+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:18:27.852+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:18:27.948+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:18:28.088+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:18:28.087+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:18:28.250+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:18:28.248+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:18:28.463+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.681 seconds
[2022-12-16T16:18:39.001+0000] {processor.py:154} INFO - Started process (PID=518) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:18:39.015+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:18:39.024+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:18:39.023+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:18:39.229+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:18:39.423+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:18:39.422+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:18:39.573+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:18:39.572+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:18:39.756+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.790 seconds
[2022-12-16T16:18:50.052+0000] {processor.py:154} INFO - Started process (PID=529) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:18:50.082+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:18:50.097+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:18:50.085+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:18:50.305+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:18:50.503+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:18:50.502+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:18:50.634+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:18:50.633+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:18:50.804+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.811 seconds
[2022-12-16T16:19:01.382+0000] {processor.py:154} INFO - Started process (PID=539) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:19:01.460+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:19:01.472+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:19:01.471+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:19:01.698+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:19:01.954+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:19:01.953+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:19:02.183+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:19:02.173+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:19:02.527+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.187 seconds
[2022-12-16T16:19:12.874+0000] {processor.py:154} INFO - Started process (PID=549) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:19:12.932+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:19:12.936+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:19:12.935+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:19:13.079+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:19:13.748+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:19:13.746+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:19:13.992+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:19:13.991+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:19:14.355+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.495 seconds
[2022-12-16T16:19:24.850+0000] {processor.py:154} INFO - Started process (PID=568) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:19:24.872+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:19:24.880+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:19:24.879+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:19:25.177+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:19:26.797+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T16:19:37.245+0000] {processor.py:154} INFO - Started process (PID=578) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:19:37.270+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:19:37.274+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:19:37.273+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:19:37.392+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:19:37.651+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:19:37.650+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:19:37.800+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:19:37.799+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:19:37.917+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.693 seconds
[2022-12-16T16:19:48.135+0000] {processor.py:154} INFO - Started process (PID=588) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:19:48.185+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:19:48.189+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:19:48.188+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:19:48.365+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:19:49.169+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:19:49.157+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:19:49.585+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:19:49.584+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:19:50.046+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.932 seconds
[2022-12-16T16:20:00.975+0000] {processor.py:154} INFO - Started process (PID=596) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:20:01.045+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:20:01.063+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:20:01.062+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:20:01.959+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:20:02.920+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:20:02.919+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:20:03.669+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:20:03.649+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:20:04.450+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.651 seconds
[2022-12-16T16:20:15.794+0000] {processor.py:154} INFO - Started process (PID=614) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:20:15.847+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:20:15.856+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:20:15.855+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:20:16.529+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:20:18.949+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:20:18.948+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:20:20.608+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:20:20.607+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:20:21.112+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 5.421 seconds
[2022-12-16T16:20:32.030+0000] {processor.py:154} INFO - Started process (PID=626) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:20:32.044+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:20:32.047+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:20:32.047+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:20:32.208+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:20:33.075+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:20:33.074+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:20:34.088+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:20:34.087+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:20:34.615+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.765 seconds
[2022-12-16T16:20:45.675+0000] {processor.py:154} INFO - Started process (PID=636) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:20:45.709+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:20:45.719+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:20:45.718+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:20:45.935+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:20:46.874+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:20:46.873+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:20:47.231+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:20:47.230+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:20:47.592+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.981 seconds
[2022-12-16T16:20:58.153+0000] {processor.py:154} INFO - Started process (PID=646) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:20:58.208+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:20:58.219+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:20:58.218+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:20:58.560+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:20:59.231+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:20:59.230+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:20:59.483+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:20:59.482+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:20:59.710+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.597 seconds
[2022-12-16T16:21:10.302+0000] {processor.py:154} INFO - Started process (PID=664) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:21:10.330+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:21:10.339+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:21:10.333+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:21:10.566+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:21:10.764+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:21:10.763+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:21:10.917+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:21:10.916+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:21:11.155+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.950 seconds
[2022-12-16T16:21:21.461+0000] {processor.py:154} INFO - Started process (PID=674) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:21:21.513+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:21:21.517+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:21:21.516+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:21:21.632+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:21:22.044+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:21:22.043+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:21:22.545+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:21:22.544+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:21:22.744+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.303 seconds
[2022-12-16T16:21:33.279+0000] {processor.py:154} INFO - Started process (PID=684) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:21:33.302+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:21:33.310+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:21:33.305+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:21:33.450+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:21:33.763+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:21:33.762+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:21:34.064+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:21:34.063+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:21:34.331+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.090 seconds
[2022-12-16T16:21:44.957+0000] {processor.py:154} INFO - Started process (PID=694) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:21:44.970+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:21:44.988+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:21:44.987+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:21:45.345+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:21:46.600+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:21:46.599+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:21:47.012+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:21:47.011+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:21:47.335+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.451 seconds
[2022-12-16T16:21:57.619+0000] {processor.py:154} INFO - Started process (PID=712) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:21:57.627+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:21:57.640+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:21:57.639+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:21:57.849+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:21:59.780+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:21:59.779+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:22:00.198+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:22:00.197+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:22:00.512+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.933 seconds
[2022-12-16T16:22:10.971+0000] {processor.py:154} INFO - Started process (PID=722) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:22:11.017+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:22:11.021+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:22:11.020+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:22:11.131+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:22:11.382+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:22:11.381+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:22:11.708+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:22:11.707+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:22:11.923+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.969 seconds
[2022-12-16T16:22:22.399+0000] {processor.py:154} INFO - Started process (PID=732) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:22:22.444+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:22:22.447+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:22:22.446+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:22:22.584+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:22:22.782+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:22:22.781+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:22:22.939+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:22:22.938+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:22:23.053+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.666 seconds
[2022-12-16T16:22:33.496+0000] {processor.py:154} INFO - Started process (PID=749) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:22:33.519+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:22:33.527+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:22:33.526+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:22:33.627+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:22:35.296+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:22:35.295+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:22:35.772+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:22:35.771+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:22:36.078+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.598 seconds
[2022-12-16T16:22:46.507+0000] {processor.py:154} INFO - Started process (PID=760) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:22:46.530+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:22:46.534+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:22:46.533+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:22:46.626+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:22:46.778+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:22:46.777+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:22:46.908+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:22:46.907+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:22:47.098+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.607 seconds
[2022-12-16T16:22:57.491+0000] {processor.py:154} INFO - Started process (PID=770) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:22:57.539+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:22:57.543+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:22:57.542+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:22:57.803+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:22:57.980+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:22:57.979+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:22:58.228+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:22:58.227+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:22:58.427+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.953 seconds
[2022-12-16T16:23:08.736+0000] {processor.py:154} INFO - Started process (PID=780) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:23:08.757+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:23:08.761+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:23:08.760+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:23:08.847+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:23:09.000+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:23:08.999+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:23:09.127+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:23:09.126+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:23:09.236+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.534 seconds
[2022-12-16T16:23:20.335+0000] {processor.py:154} INFO - Started process (PID=798) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:23:20.381+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:23:20.386+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:23:20.385+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:23:20.635+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:23:21.027+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:23:21.026+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:23:21.343+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:23:21.342+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:23:21.574+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.278 seconds
[2022-12-16T16:23:32.086+0000] {processor.py:154} INFO - Started process (PID=808) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:23:32.090+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:23:32.102+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:23:32.097+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:23:32.237+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:23:32.412+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:23:32.411+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:23:32.533+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:23:32.532+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:23:32.664+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.602 seconds
[2022-12-16T16:23:43.051+0000] {processor.py:154} INFO - Started process (PID=818) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:23:43.085+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:23:43.090+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:23:43.089+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:23:43.177+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:23:43.681+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:23:43.680+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:23:43.792+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:23:43.791+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:23:43.916+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.881 seconds
[2022-12-16T16:23:54.409+0000] {processor.py:154} INFO - Started process (PID=828) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:23:54.449+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:23:54.453+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:23:54.452+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:23:54.560+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:23:54.914+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T16:24:05.483+0000] {processor.py:154} INFO - Started process (PID=846) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:24:05.537+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:24:05.542+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:24:05.541+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:24:05.838+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:24:06.150+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:24:06.144+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:24:06.432+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:24:06.431+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:24:06.649+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.215 seconds
[2022-12-16T16:24:17.172+0000] {processor.py:154} INFO - Started process (PID=856) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:24:17.176+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:24:17.180+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:24:17.179+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:24:17.273+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:24:17.469+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:24:17.468+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:24:17.590+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:24:17.589+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:24:17.905+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.746 seconds
[2022-12-16T16:24:28.401+0000] {processor.py:154} INFO - Started process (PID=866) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:24:28.450+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:24:28.460+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:24:28.459+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:24:28.807+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:24:29.053+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:24:29.052+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:24:29.169+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:24:29.168+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:24:29.307+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.935 seconds
[2022-12-16T16:24:39.669+0000] {processor.py:154} INFO - Started process (PID=876) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:24:39.679+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:24:39.683+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:24:39.682+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:24:39.824+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:24:40.351+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:24:40.350+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:24:40.609+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:24:40.605+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:24:40.784+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.133 seconds
[2022-12-16T16:24:51.332+0000] {processor.py:154} INFO - Started process (PID=893) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:24:51.359+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:24:51.369+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:24:51.364+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:24:51.645+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:24:51.855+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:24:51.854+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:24:52.129+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:24:52.128+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:24:52.403+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.150 seconds
[2022-12-16T16:25:02.606+0000] {processor.py:154} INFO - Started process (PID=903) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:25:02.615+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:25:02.620+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:25:02.618+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:25:02.720+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:25:02.905+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:25:02.904+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:25:03.121+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:25:03.120+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:25:03.361+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.770 seconds
[2022-12-16T16:25:13.701+0000] {processor.py:154} INFO - Started process (PID=913) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:25:13.750+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:25:13.759+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:25:13.752+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:25:13.948+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:25:15.016+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:25:15.015+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:25:15.168+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:25:15.167+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:25:15.306+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.646 seconds
[2022-12-16T16:25:25.804+0000] {processor.py:154} INFO - Started process (PID=923) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:25:25.826+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:25:25.851+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:25:25.845+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:25:25.998+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:25:26.324+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:25:26.323+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:25:26.440+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:25:26.439+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:25:26.579+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.842 seconds
[2022-12-16T16:25:37.294+0000] {processor.py:154} INFO - Started process (PID=941) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:25:37.398+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:25:37.414+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:25:37.401+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:25:37.924+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:25:38.470+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:25:38.469+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:25:38.764+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:25:38.751+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:25:39.040+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.820 seconds
[2022-12-16T16:25:49.259+0000] {processor.py:154} INFO - Started process (PID=951) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:25:49.313+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:25:49.317+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:25:49.316+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:25:49.406+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:25:50.162+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:25:50.161+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:25:50.295+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:25:50.294+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:25:50.677+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.435 seconds
[2022-12-16T16:26:00.967+0000] {processor.py:154} INFO - Started process (PID=961) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:26:00.991+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:26:00.996+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:26:00.995+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:26:01.085+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:26:01.222+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:26:01.221+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:26:01.335+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:26:01.334+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:26:01.473+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.523 seconds
[2022-12-16T16:26:12.329+0000] {processor.py:154} INFO - Started process (PID=978) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:26:12.348+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:26:12.364+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:26:12.363+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:26:12.779+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:26:13.318+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:26:13.317+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:26:13.681+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:26:13.680+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:26:14.347+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.071 seconds
[2022-12-16T16:26:24.766+0000] {processor.py:154} INFO - Started process (PID=988) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:26:24.771+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:26:24.787+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:26:24.786+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:26:24.944+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:26:25.127+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:26:25.126+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:26:25.280+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:26:25.279+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:26:25.473+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.728 seconds
[2022-12-16T16:26:35.857+0000] {processor.py:154} INFO - Started process (PID=998) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:26:35.860+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:26:35.864+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:26:35.863+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:26:35.961+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:26:36.170+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:26:36.169+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:26:36.368+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:26:36.367+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:26:36.504+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.661 seconds
[2022-12-16T16:26:46.814+0000] {processor.py:154} INFO - Started process (PID=1008) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:26:46.840+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:26:46.845+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:26:46.844+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:26:46.932+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:26:47.978+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T16:26:58.632+0000] {processor.py:154} INFO - Started process (PID=1026) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:26:58.641+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:26:58.650+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:26:58.644+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:26:58.862+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:26:59.675+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:26:59.674+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:27:00.049+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:27:00.044+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:27:00.217+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.617 seconds
[2022-12-16T16:27:10.755+0000] {processor.py:154} INFO - Started process (PID=1036) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:27:10.787+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:27:10.791+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:27:10.790+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:27:10.981+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:27:11.686+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:27:11.683+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:27:11.931+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:27:11.930+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:27:12.149+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.417 seconds
[2022-12-16T16:27:22.551+0000] {processor.py:154} INFO - Started process (PID=1046) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:27:22.578+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:27:22.582+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:27:22.580+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:27:22.671+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:27:22.815+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:27:22.814+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:27:22.940+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:27:22.939+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:27:23.058+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.531 seconds
[2022-12-16T16:27:33.788+0000] {processor.py:154} INFO - Started process (PID=1056) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:27:33.806+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:27:33.814+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:27:33.813+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:27:33.906+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:27:34.041+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:27:34.040+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:27:34.156+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:27:34.155+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:27:34.303+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.538 seconds
[2022-12-16T16:27:44.864+0000] {processor.py:154} INFO - Started process (PID=1074) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:27:44.913+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:27:44.917+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:27:44.916+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:27:45.119+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:27:45.806+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:27:45.801+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:27:46.070+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:27:46.069+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:27:46.358+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.526 seconds
[2022-12-16T16:27:56.686+0000] {processor.py:154} INFO - Started process (PID=1084) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:27:56.689+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:27:56.693+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:27:56.692+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:27:56.853+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:27:57.168+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:27:57.165+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:27:57.352+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:27:57.351+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:27:57.554+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.885 seconds
[2022-12-16T16:28:07.880+0000] {processor.py:154} INFO - Started process (PID=1094) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:28:07.910+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:28:07.914+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:28:07.913+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:28:08.017+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:28:08.168+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:28:08.167+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:28:08.313+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:28:08.312+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:28:08.444+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.582 seconds
[2022-12-16T16:28:18.770+0000] {processor.py:154} INFO - Started process (PID=1104) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:28:18.773+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:28:18.778+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:28:18.777+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:28:18.866+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:28:18.997+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:28:18.996+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:28:19.119+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:28:19.118+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:28:19.265+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.510 seconds
[2022-12-16T16:28:29.706+0000] {processor.py:154} INFO - Started process (PID=1122) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:28:29.731+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:28:29.740+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:28:29.739+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:28:29.966+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:28:30.266+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:28:30.265+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:28:30.581+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:28:30.580+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:28:30.776+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.133 seconds
[2022-12-16T16:28:41.348+0000] {processor.py:154} INFO - Started process (PID=1132) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:28:41.378+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:28:41.382+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:28:41.381+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:28:41.607+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:28:42.449+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:28:42.448+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:28:42.664+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:28:42.663+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:28:42.846+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.526 seconds
[2022-12-16T16:28:53.251+0000] {processor.py:154} INFO - Started process (PID=1142) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:28:53.277+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:28:53.288+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:28:53.287+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:28:53.573+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:28:54.080+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:28:54.079+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:28:54.551+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:28:54.550+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:28:54.786+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.572 seconds
[2022-12-16T16:29:05.166+0000] {processor.py:154} INFO - Started process (PID=1160) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:29:05.218+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:29:05.227+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:29:05.226+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:29:05.351+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:29:06.335+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:29:06.334+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:29:06.646+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:29:06.645+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:29:06.839+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.691 seconds
[2022-12-16T16:29:17.392+0000] {processor.py:154} INFO - Started process (PID=1170) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:29:17.396+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:29:17.400+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:29:17.399+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:29:17.566+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:29:18.004+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:29:18.003+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:29:18.282+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:29:18.276+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:29:18.587+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.219 seconds
[2022-12-16T16:29:28.861+0000] {processor.py:154} INFO - Started process (PID=1180) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:29:28.906+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:29:28.917+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:29:28.916+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:29:29.158+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:29:29.546+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:29:29.545+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:29:30.232+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:29:30.230+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:29:30.599+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.753 seconds
[2022-12-16T16:29:41.386+0000] {processor.py:154} INFO - Started process (PID=1190) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:29:41.390+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:29:41.399+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:29:41.398+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:29:41.541+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:29:42.475+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:29:42.474+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:29:42.677+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:29:42.676+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:29:42.868+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.500 seconds
[2022-12-16T16:29:53.148+0000] {processor.py:154} INFO - Started process (PID=1209) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:29:53.151+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:29:53.160+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:29:53.159+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:29:53.387+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:29:53.743+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:29:53.742+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:29:54.280+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:29:54.279+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:29:54.699+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.583 seconds
[2022-12-16T16:30:04.875+0000] {processor.py:154} INFO - Started process (PID=1219) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:30:04.885+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:30:04.902+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:30:04.894+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:30:05.011+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:30:06.496+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:30:06.495+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:30:06.607+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:30:06.606+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:30:06.736+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.875 seconds
[2022-12-16T16:30:17.109+0000] {processor.py:154} INFO - Started process (PID=1229) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:30:17.117+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:30:17.123+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:30:17.122+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:30:17.242+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:30:17.410+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:30:17.409+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:30:17.544+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:30:17.543+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:30:17.663+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.595 seconds
[2022-12-16T16:30:28.012+0000] {processor.py:154} INFO - Started process (PID=1246) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:30:28.016+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:30:28.021+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:30:28.020+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:30:28.137+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:30:28.498+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:30:28.497+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:30:28.666+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:30:28.665+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:30:28.988+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.992 seconds
[2022-12-16T16:30:39.476+0000] {processor.py:154} INFO - Started process (PID=1257) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:30:39.479+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:30:39.483+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:30:39.482+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:30:39.566+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:30:39.718+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:30:39.717+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:30:39.848+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:30:39.847+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:30:39.957+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.496 seconds
[2022-12-16T16:30:50.245+0000] {processor.py:154} INFO - Started process (PID=1267) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:30:50.249+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:30:50.253+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:30:50.252+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:30:50.339+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:30:50.777+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:30:50.776+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:30:50.894+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:30:50.893+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:30:51.010+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.779 seconds
[2022-12-16T16:31:01.281+0000] {processor.py:154} INFO - Started process (PID=1277) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:31:01.284+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:31:01.289+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:31:01.288+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:31:01.374+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:31:01.854+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T16:31:12.575+0000] {processor.py:154} INFO - Started process (PID=1295) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:31:12.598+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:31:12.603+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:31:12.602+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:31:12.811+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:31:13.623+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T16:31:23.903+0000] {processor.py:154} INFO - Started process (PID=1305) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:31:23.948+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:31:23.956+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:31:23.954+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:31:24.070+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:31:24.252+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:31:24.251+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:31:24.461+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:31:24.460+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:31:24.785+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.897 seconds
[2022-12-16T16:31:35.291+0000] {processor.py:154} INFO - Started process (PID=1315) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:31:35.342+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:31:35.347+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:31:35.345+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:31:35.536+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:31:36.126+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:31:36.125+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:31:36.285+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:31:36.284+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:31:36.399+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.128 seconds
[2022-12-16T16:31:46.757+0000] {processor.py:154} INFO - Started process (PID=1325) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:31:46.780+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:31:46.789+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:31:46.787+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:31:46.927+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:31:47.346+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:31:47.345+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:31:47.484+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:31:47.482+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:31:47.644+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.905 seconds
[2022-12-16T16:31:57.878+0000] {processor.py:154} INFO - Started process (PID=1342) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:31:57.908+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:31:57.919+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:31:57.918+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:31:58.160+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:31:58.907+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T16:32:09.171+0000] {processor.py:154} INFO - Started process (PID=1352) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:32:09.184+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:32:09.188+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:32:09.187+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:32:09.324+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:32:09.716+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:32:09.715+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:32:09.840+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:32:09.839+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:32:10.006+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.855 seconds
[2022-12-16T16:32:20.645+0000] {processor.py:154} INFO - Started process (PID=1362) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:32:20.662+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:32:20.667+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:32:20.665+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:32:20.809+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:32:22.108+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T16:32:32.764+0000] {processor.py:154} INFO - Started process (PID=1378) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:32:32.797+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:32:32.805+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:32:32.804+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:32:32.978+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:32:33.712+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:32:33.711+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:32:33.872+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:32:33.871+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:32:33.992+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.289 seconds
[2022-12-16T16:32:44.192+0000] {processor.py:154} INFO - Started process (PID=1390) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:32:44.246+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:32:44.250+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:32:44.249+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:32:44.374+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:32:45.208+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T16:32:55.423+0000] {processor.py:154} INFO - Started process (PID=1400) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:32:55.473+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:32:55.486+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:32:55.485+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:32:55.600+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:32:55.774+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:32:55.773+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:32:55.974+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:32:55.973+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:32:56.336+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.929 seconds
[2022-12-16T16:33:06.684+0000] {processor.py:154} INFO - Started process (PID=1410) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:33:06.733+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:33:06.738+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:33:06.737+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:33:06.823+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:33:06.955+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:33:06.954+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:33:07.067+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:33:07.066+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:33:07.204+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.534 seconds
[2022-12-16T16:33:17.721+0000] {processor.py:154} INFO - Started process (PID=1428) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:33:17.793+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:33:17.797+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:33:17.796+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:33:17.940+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:33:18.102+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:33:18.100+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:33:18.333+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:33:18.332+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:33:18.692+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.990 seconds
[2022-12-16T16:33:28.866+0000] {processor.py:154} INFO - Started process (PID=1438) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:33:28.898+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:33:28.902+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:33:28.901+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:33:28.991+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:33:29.664+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:33:29.663+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:33:29.796+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:33:29.795+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:33:29.942+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.092 seconds
[2022-12-16T16:33:40.238+0000] {processor.py:154} INFO - Started process (PID=1448) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:33:40.248+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:33:40.253+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:33:40.252+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:33:40.337+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:33:40.569+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:33:40.568+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:33:40.707+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:33:40.706+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:33:40.818+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.594 seconds
[2022-12-16T16:33:51.184+0000] {processor.py:154} INFO - Started process (PID=1458) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:33:51.209+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:33:51.215+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:33:51.214+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:33:51.365+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:33:51.749+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:33:51.748+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:33:51.864+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:33:51.863+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:33:52.007+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.862 seconds
[2022-12-16T16:34:02.573+0000] {processor.py:154} INFO - Started process (PID=1476) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:34:02.601+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:34:02.620+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:34:02.613+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:34:03.005+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:34:04.116+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T16:34:14.568+0000] {processor.py:154} INFO - Started process (PID=1486) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:34:14.601+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:34:14.610+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:34:14.609+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:34:14.799+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:34:15.418+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T16:34:25.766+0000] {processor.py:154} INFO - Started process (PID=1496) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:34:25.797+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:34:25.804+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:34:25.803+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:34:25.976+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:34:27.153+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:34:27.152+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:34:27.304+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:34:27.303+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:34:27.456+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.705 seconds
[2022-12-16T16:34:37.796+0000] {processor.py:154} INFO - Started process (PID=1514) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:34:37.826+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:34:37.850+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:34:37.845+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:34:38.040+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:34:38.984+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T16:34:49.287+0000] {processor.py:154} INFO - Started process (PID=1524) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:34:49.299+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:34:49.304+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:34:49.303+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:34:49.389+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:34:49.960+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T16:35:00.377+0000] {processor.py:154} INFO - Started process (PID=1534) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:35:00.406+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:35:00.411+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:35:00.410+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:35:00.496+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:35:00.633+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:35:00.632+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:35:00.748+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:35:00.747+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:35:00.900+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.537 seconds
[2022-12-16T16:35:11.207+0000] {processor.py:154} INFO - Started process (PID=1544) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:35:11.255+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:35:11.261+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:35:11.260+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:35:11.360+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:35:11.636+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:35:11.633+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:35:12.181+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:35:12.180+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:35:12.369+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.179 seconds
[2022-12-16T16:35:23.039+0000] {processor.py:154} INFO - Started process (PID=1561) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:35:23.087+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:35:23.096+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:35:23.092+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:35:23.240+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:35:23.479+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:35:23.478+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:35:23.701+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:35:23.699+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:35:23.916+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.915 seconds
[2022-12-16T16:35:34.446+0000] {processor.py:154} INFO - Started process (PID=1571) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:35:34.450+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:35:34.459+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:35:34.458+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:35:34.569+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:35:35.263+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:35:35.262+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:35:35.397+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:35:35.396+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:35:35.537+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.106 seconds
[2022-12-16T16:35:45.799+0000] {processor.py:154} INFO - Started process (PID=1581) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:35:45.849+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:35:45.854+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:35:45.853+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:35:45.943+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:35:46.960+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:35:46.959+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:35:47.088+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:35:47.087+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:35:47.219+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.435 seconds
[2022-12-16T16:35:58.010+0000] {processor.py:154} INFO - Started process (PID=1591) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:35:58.014+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:35:58.018+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:35:58.017+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:35:58.114+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:35:59.756+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:35:59.755+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:36:00.354+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:36:00.345+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:36:00.695+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.701 seconds
[2022-12-16T16:36:11.246+0000] {processor.py:154} INFO - Started process (PID=1608) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:36:11.291+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:36:11.297+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:36:11.295+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:36:11.406+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:36:12.999+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T16:36:23.551+0000] {processor.py:154} INFO - Started process (PID=1618) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:36:23.567+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:36:23.571+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:36:23.570+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:36:23.844+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:36:24.375+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T16:36:34.752+0000] {processor.py:154} INFO - Started process (PID=1628) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:36:34.755+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:36:34.759+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:36:34.758+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:36:34.869+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:36:35.156+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:36:35.155+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:36:35.433+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:36:35.432+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:36:35.559+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.824 seconds
[2022-12-16T16:36:45.910+0000] {processor.py:154} INFO - Started process (PID=1645) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:36:45.914+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:36:45.919+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:36:45.918+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:36:46.131+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:36:46.744+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:36:46.743+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:36:47.041+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:36:47.040+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:36:47.500+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.609 seconds
[2022-12-16T16:36:57.976+0000] {processor.py:154} INFO - Started process (PID=1655) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:36:58.022+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:36:58.026+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:36:58.025+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:36:58.221+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:36:58.731+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:36:58.730+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:36:58.867+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:36:58.866+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:36:59.135+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.179 seconds
[2022-12-16T16:37:09.476+0000] {processor.py:154} INFO - Started process (PID=1665) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:37:09.480+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:37:09.484+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:37:09.483+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:37:09.574+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:37:10.246+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:37:10.245+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:37:10.516+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:37:10.515+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:37:10.730+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.267 seconds
[2022-12-16T16:37:21.167+0000] {processor.py:154} INFO - Started process (PID=1675) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:37:21.215+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:37:21.219+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:37:21.218+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:37:21.504+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:37:21.754+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:37:21.753+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:37:22.065+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:37:22.049+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:37:22.563+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.467 seconds
[2022-12-16T16:37:33.428+0000] {processor.py:154} INFO - Started process (PID=1693) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:37:33.459+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:37:33.463+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:37:33.462+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:37:33.674+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:37:34.714+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:37:34.713+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:37:34.918+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:37:34.916+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:37:35.125+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.753 seconds
[2022-12-16T16:37:45.454+0000] {processor.py:154} INFO - Started process (PID=1703) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:37:45.461+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:37:45.468+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:37:45.467+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:37:45.558+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:37:45.937+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:37:45.930+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:37:46.201+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:37:46.200+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:37:46.403+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.962 seconds
[2022-12-16T16:37:56.673+0000] {processor.py:154} INFO - Started process (PID=1713) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:37:56.730+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:37:56.738+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:37:56.733+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:37:56.856+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:37:57.248+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T16:38:07.483+0000] {processor.py:154} INFO - Started process (PID=1731) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:38:07.491+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:38:07.495+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:38:07.494+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:38:07.643+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:38:07.818+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:38:07.808+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:38:08.039+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:38:08.038+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:38:08.198+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.740 seconds
[2022-12-16T16:38:18.967+0000] {processor.py:154} INFO - Started process (PID=1741) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:38:19.015+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:38:19.024+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:38:19.020+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:38:19.260+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:38:20.656+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:38:20.655+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:38:20.883+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:38:20.882+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:38:21.134+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.189 seconds
[2022-12-16T16:38:31.568+0000] {processor.py:154} INFO - Started process (PID=1751) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:38:31.591+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:38:31.595+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:38:31.594+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:38:31.715+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:38:33.055+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T16:38:43.511+0000] {processor.py:154} INFO - Started process (PID=1761) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:38:43.518+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:38:43.526+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:38:43.521+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:38:43.621+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:38:44.463+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:38:44.462+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:38:44.619+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:38:44.618+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:38:44.790+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.299 seconds
[2022-12-16T16:38:55.614+0000] {processor.py:154} INFO - Started process (PID=1780) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:38:55.622+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:38:55.630+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:38:55.624+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:38:55.979+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:38:58.092+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:38:58.080+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:38:58.556+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:38:58.555+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:38:58.839+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.249 seconds
[2022-12-16T16:39:09.384+0000] {processor.py:154} INFO - Started process (PID=1790) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:39:09.464+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:39:09.469+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:39:09.468+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:39:09.744+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:39:10.160+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:39:10.158+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:39:10.327+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:39:10.326+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:39:10.483+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.142 seconds
[2022-12-16T16:39:20.693+0000] {processor.py:154} INFO - Started process (PID=1800) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:39:20.744+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:39:20.755+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:39:20.754+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:39:20.934+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:39:21.988+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:39:21.987+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:39:22.105+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:39:22.104+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:39:22.225+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.546 seconds
[2022-12-16T16:39:32.708+0000] {processor.py:154} INFO - Started process (PID=1810) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:39:32.870+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:39:32.874+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:39:32.873+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:39:33.294+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:39:33.680+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:39:33.679+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:39:33.856+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:39:33.855+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:39:34.173+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.479 seconds
[2022-12-16T16:39:45.197+0000] {processor.py:154} INFO - Started process (PID=1828) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:39:45.221+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:39:45.225+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:39:45.224+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:39:45.312+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:39:46.936+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:39:46.935+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:39:47.153+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:39:47.151+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:39:47.300+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.118 seconds
[2022-12-16T16:39:57.519+0000] {processor.py:154} INFO - Started process (PID=1841) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:39:57.534+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:39:57.538+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:39:57.537+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:39:57.667+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:39:58.867+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:39:58.866+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:39:58.989+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:39:58.988+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:39:59.174+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.671 seconds
[2022-12-16T16:40:09.589+0000] {processor.py:154} INFO - Started process (PID=1851) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:40:09.614+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:40:09.618+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:40:09.617+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:40:09.739+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:40:10.334+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:40:10.329+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:40:10.498+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:40:10.497+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:40:10.650+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.077 seconds
[2022-12-16T16:40:21.188+0000] {processor.py:154} INFO - Started process (PID=1869) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:40:21.219+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:40:21.223+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:40:21.222+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:40:21.392+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:40:21.648+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:40:21.647+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:40:21.833+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:40:21.832+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:40:22.127+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.970 seconds
[2022-12-16T16:40:32.585+0000] {processor.py:154} INFO - Started process (PID=1879) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:40:32.648+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:40:32.657+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:40:32.656+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:40:32.944+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:40:33.232+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:40:33.231+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:40:33.391+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:40:33.390+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:40:33.541+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.970 seconds
[2022-12-16T16:40:43.791+0000] {processor.py:154} INFO - Started process (PID=1889) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:40:43.795+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:40:43.799+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:40:43.798+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:40:43.943+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:40:44.141+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:40:44.140+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:40:44.283+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:40:44.282+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:40:44.476+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.701 seconds
[2022-12-16T16:40:55.259+0000] {processor.py:154} INFO - Started process (PID=1899) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:40:55.320+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:40:55.332+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:40:55.331+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:40:55.547+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:40:56.356+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:40:56.355+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:40:56.642+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:40:56.641+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:40:56.891+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.700 seconds
[2022-12-16T16:41:07.605+0000] {processor.py:154} INFO - Started process (PID=1917) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:41:07.613+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:41:07.641+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:41:07.639+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:41:07.736+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:41:07.903+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:41:07.902+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:41:08.118+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:41:08.108+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:41:08.251+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.675 seconds
[2022-12-16T16:41:18.514+0000] {processor.py:154} INFO - Started process (PID=1924) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:41:18.524+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:41:18.529+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:41:18.528+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:41:18.615+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:41:18.747+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:41:18.746+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:41:18.931+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:41:18.930+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:41:19.066+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.566 seconds
[2022-12-16T16:41:29.727+0000] {processor.py:154} INFO - Started process (PID=1934) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:41:29.734+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:41:29.739+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:41:29.738+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:41:29.834+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:41:29.982+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:41:29.981+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:41:30.112+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:41:30.111+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:41:30.316+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.603 seconds
[2022-12-16T16:41:40.720+0000] {processor.py:154} INFO - Started process (PID=1951) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:41:40.745+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:41:40.750+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:41:40.748+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:41:40.906+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:41:42.672+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:41:42.671+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:41:42.916+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:41:42.915+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:41:43.130+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.431 seconds
[2022-12-16T16:41:53.408+0000] {processor.py:154} INFO - Started process (PID=1964) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:41:53.445+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:41:53.455+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:41:53.454+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:41:53.712+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:41:54.534+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:41:54.533+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:41:54.657+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:41:54.656+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:41:54.810+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.417 seconds
[2022-12-16T16:42:05.136+0000] {processor.py:154} INFO - Started process (PID=1976) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:42:05.180+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:42:05.184+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:42:05.183+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:42:05.280+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:42:05.914+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:42:05.909+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:42:06.036+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:42:06.035+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:42:06.170+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.049 seconds
[2022-12-16T16:42:16.432+0000] {processor.py:154} INFO - Started process (PID=1984) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:42:16.480+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:42:16.485+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:42:16.484+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:42:16.567+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:42:16.704+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:42:16.703+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:42:16.935+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:42:16.934+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:42:17.347+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.926 seconds
[2022-12-16T16:42:28.067+0000] {processor.py:154} INFO - Started process (PID=2000) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:42:28.073+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:42:28.078+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:42:28.076+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:42:28.183+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:42:28.464+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:42:28.463+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:42:29.152+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:42:29.151+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:42:29.672+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.637 seconds
[2022-12-16T16:42:40.110+0000] {processor.py:154} INFO - Started process (PID=2010) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:42:40.134+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:42:40.139+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:42:40.138+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:42:40.226+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:42:40.752+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:42:40.752+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:42:40.920+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:42:40.919+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:42:41.039+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.942 seconds
[2022-12-16T16:42:51.404+0000] {processor.py:154} INFO - Started process (PID=2020) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:42:51.434+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:42:51.438+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:42:51.437+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:42:51.547+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:42:52.091+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T16:43:02.556+0000] {processor.py:154} INFO - Started process (PID=2036) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:43:02.645+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:43:02.650+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:43:02.649+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:43:02.805+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:43:03.490+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:43:03.489+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:43:03.814+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:43:03.814+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:43:04.074+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.562 seconds
[2022-12-16T16:43:10.731+0000] {processor.py:154} INFO - Started process (PID=2054) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:43:10.743+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:43:10.753+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:43:10.752+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:43:10.921+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:43:11.076+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:43:11.075+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:43:11.199+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:43:11.198+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:43:11.736+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.041 seconds
[2022-12-16T16:43:22.431+0000] {processor.py:154} INFO - Started process (PID=2062) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:43:22.459+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:43:22.471+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:43:22.470+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:43:22.608+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:43:22.779+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:43:22.778+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:43:22.954+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:43:22.954+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:43:23.141+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.754 seconds
[2022-12-16T16:43:33.940+0000] {processor.py:154} INFO - Started process (PID=2072) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:43:33.985+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:43:33.994+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:43:33.993+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:43:34.103+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:43:34.239+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:43:34.238+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:43:34.384+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:43:34.384+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:43:34.540+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.615 seconds
[2022-12-16T16:43:44.911+0000] {processor.py:154} INFO - Started process (PID=2089) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:43:44.956+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:43:44.969+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:43:44.968+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:43:45.173+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:43:45.472+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:43:45.471+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:43:45.712+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:43:45.711+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:43:45.971+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.091 seconds
[2022-12-16T16:43:56.486+0000] {processor.py:154} INFO - Started process (PID=2100) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:43:56.556+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:43:56.561+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:43:56.560+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:43:56.647+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:43:57.755+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T16:44:07.987+0000] {processor.py:154} INFO - Started process (PID=2110) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:44:08.012+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:44:08.016+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:44:08.015+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:44:08.104+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:44:08.405+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:44:08.404+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:44:08.554+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:44:08.553+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:44:08.786+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.812 seconds
[2022-12-16T16:44:19.148+0000] {processor.py:154} INFO - Started process (PID=2120) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:44:19.194+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:44:19.198+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:44:19.197+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:44:19.283+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:44:19.776+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T16:44:30.163+0000] {processor.py:154} INFO - Started process (PID=2138) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:44:30.211+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:44:30.215+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:44:30.214+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:44:30.376+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:44:30.544+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:44:30.543+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:44:30.668+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:44:30.667+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:44:30.837+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.689 seconds
[2022-12-16T16:44:40.982+0000] {processor.py:154} INFO - Started process (PID=2148) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:44:40.986+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:44:40.991+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:44:40.989+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:44:41.076+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:44:41.438+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:44:41.437+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:44:41.548+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:44:41.548+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:44:41.690+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.722 seconds
[2022-12-16T16:44:52.021+0000] {processor.py:154} INFO - Started process (PID=2158) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:44:52.066+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:44:52.071+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:44:52.069+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:44:52.161+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:44:52.574+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:44:52.573+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:44:52.685+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:44:52.684+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:44:52.817+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.810 seconds
[2022-12-16T16:45:03.097+0000] {processor.py:154} INFO - Started process (PID=2168) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:45:03.123+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:45:03.128+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:45:03.127+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:45:03.220+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:45:03.353+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:45:03.352+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:45:03.463+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:45:03.463+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:45:03.599+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.516 seconds
[2022-12-16T16:45:13.735+0000] {processor.py:154} INFO - Started process (PID=2187) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:45:13.760+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:45:13.767+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:45:13.764+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:45:13.880+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:45:14.454+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:45:14.453+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:45:14.877+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:45:14.876+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:45:15.211+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.492 seconds
[2022-12-16T16:45:25.561+0000] {processor.py:154} INFO - Started process (PID=2197) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:45:25.582+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:45:25.587+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:45:25.586+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:45:25.676+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:45:26.843+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T16:45:37.240+0000] {processor.py:154} INFO - Started process (PID=2207) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:45:37.260+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:45:37.265+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:45:37.264+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:45:37.355+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:45:38.047+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:45:38.046+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:45:38.333+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:45:38.332+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:45:38.461+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.235 seconds
[2022-12-16T16:45:48.943+0000] {processor.py:154} INFO - Started process (PID=2225) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:45:48.993+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:45:48.996+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:45:48.996+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:45:49.213+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:45:50.247+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:45:50.246+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:45:50.413+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:45:50.412+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:45:50.588+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.685 seconds
[2022-12-16T16:46:01.003+0000] {processor.py:154} INFO - Started process (PID=2235) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:46:01.047+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:46:01.051+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:46:01.050+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:46:01.147+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:46:02.327+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:46:02.325+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:46:02.624+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:46:02.623+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:46:02.806+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.821 seconds
[2022-12-16T16:46:13.128+0000] {processor.py:154} INFO - Started process (PID=2245) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:46:13.156+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:46:13.167+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:46:13.159+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:46:13.336+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:46:14.107+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:46:14.106+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:46:14.280+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:46:14.279+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:46:14.542+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.442 seconds
[2022-12-16T16:46:25.856+0000] {processor.py:154} INFO - Started process (PID=2255) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:46:25.915+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:46:25.924+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:46:25.923+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:46:26.087+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:46:27.047+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T16:46:37.666+0000] {processor.py:154} INFO - Started process (PID=2274) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:46:37.715+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:46:37.727+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:46:37.726+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:46:37.966+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:46:39.144+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T16:46:50.056+0000] {processor.py:154} INFO - Started process (PID=2284) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:46:50.105+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:46:50.110+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:46:50.109+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:46:50.193+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:46:51.012+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:46:51.011+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:46:51.165+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:46:51.164+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:46:51.310+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.271 seconds
[2022-12-16T16:47:01.587+0000] {processor.py:154} INFO - Started process (PID=2294) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:47:01.606+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:47:01.611+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:47:01.610+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:47:01.706+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:47:02.352+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:47:02.350+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:47:02.507+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:47:02.506+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:47:02.648+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.079 seconds
[2022-12-16T16:47:13.114+0000] {processor.py:154} INFO - Started process (PID=2312) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:47:13.129+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:47:13.134+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:47:13.133+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:47:13.450+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:47:14.136+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:47:14.133+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:47:14.428+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:47:14.427+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:47:14.747+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.664 seconds
[2022-12-16T16:47:25.202+0000] {processor.py:154} INFO - Started process (PID=2322) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:47:25.206+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:47:25.210+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:47:25.208+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:47:25.333+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:47:25.546+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:47:25.545+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:47:25.672+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:47:25.672+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:47:25.806+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.623 seconds
[2022-12-16T16:47:36.112+0000] {processor.py:154} INFO - Started process (PID=2332) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:47:36.217+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:47:36.222+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:47:36.221+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:47:36.314+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:47:37.065+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:47:37.064+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:47:37.256+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:47:37.255+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:47:37.377+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.279 seconds
[2022-12-16T16:47:47.828+0000] {processor.py:154} INFO - Started process (PID=2342) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:47:47.872+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:47:47.876+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:47:47.875+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:47:48.000+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:47:48.268+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:47:48.267+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:47:48.453+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:47:48.452+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:47:48.670+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.885 seconds
[2022-12-16T16:47:59.655+0000] {processor.py:154} INFO - Started process (PID=2360) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:47:59.675+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:47:59.679+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:47:59.679+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:47:59.994+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:48:00.153+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:48:00.152+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:48:00.282+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:48:00.281+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:48:00.409+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.892 seconds
[2022-12-16T16:48:11.067+0000] {processor.py:154} INFO - Started process (PID=2370) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:48:11.101+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:48:11.136+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:48:11.135+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:48:11.458+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:48:11.692+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:48:11.691+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:48:11.843+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:48:11.842+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:48:12.018+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.019 seconds
[2022-12-16T16:48:22.477+0000] {processor.py:154} INFO - Started process (PID=2380) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:48:22.527+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:48:22.531+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:48:22.530+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:48:22.653+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:48:24.059+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T16:48:34.631+0000] {processor.py:154} INFO - Started process (PID=2390) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:48:34.664+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:48:34.671+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:48:34.670+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:48:35.237+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:48:35.836+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:48:35.835+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:48:36.077+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:48:36.076+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:48:36.437+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.871 seconds
[2022-12-16T16:48:47.370+0000] {processor.py:154} INFO - Started process (PID=2409) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:48:47.379+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:48:47.383+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:48:47.382+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:48:47.526+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:48:47.711+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:48:47.711+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:48:47.860+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:48:47.859+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:48:48.019+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.738 seconds
[2022-12-16T16:48:58.320+0000] {processor.py:154} INFO - Started process (PID=2419) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:48:58.368+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:48:58.372+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:48:58.371+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:48:58.537+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:48:59.075+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:48:59.072+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:48:59.228+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:48:59.227+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:48:59.387+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.086 seconds
[2022-12-16T16:49:09.724+0000] {processor.py:154} INFO - Started process (PID=2429) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:49:09.751+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:49:09.755+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:49:09.754+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:49:09.885+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:49:10.089+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:49:10.088+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:49:10.440+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:49:10.439+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:49:11.047+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.352 seconds
[2022-12-16T16:49:21.354+0000] {processor.py:154} INFO - Started process (PID=2446) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:49:21.367+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:49:21.371+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:49:21.370+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:49:21.583+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:49:23.094+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T16:49:33.714+0000] {processor.py:154} INFO - Started process (PID=2456) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:49:33.752+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:49:33.757+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:49:33.755+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:49:33.874+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:49:34.623+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T16:49:44.931+0000] {processor.py:154} INFO - Started process (PID=2466) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:49:44.980+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:49:44.985+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:49:44.984+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:49:45.085+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:49:45.238+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:49:45.237+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:49:45.351+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:49:45.350+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:49:45.474+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.557 seconds
[2022-12-16T16:49:55.789+0000] {processor.py:154} INFO - Started process (PID=2476) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:49:55.842+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:49:55.848+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:49:55.846+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:49:55.939+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:49:56.078+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:49:56.077+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:49:56.199+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:49:56.198+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:49:56.331+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.556 seconds
[2022-12-16T16:50:06.746+0000] {processor.py:154} INFO - Started process (PID=2493) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:50:06.775+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:50:06.780+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:50:06.779+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:50:06.995+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:50:07.232+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:50:07.231+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:50:07.368+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:50:07.367+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:50:07.636+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.927 seconds
[2022-12-16T16:50:18.158+0000] {processor.py:154} INFO - Started process (PID=2503) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:50:18.218+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:50:18.233+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:50:18.232+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:50:18.345+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:50:18.520+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:50:18.519+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:50:18.897+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:50:18.896+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:50:19.084+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.940 seconds
[2022-12-16T16:50:29.382+0000] {processor.py:154} INFO - Started process (PID=2513) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:50:29.504+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:50:29.508+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:50:29.507+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:50:29.655+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:50:29.785+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:50:29.784+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:50:29.919+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:50:29.919+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:50:30.038+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.673 seconds
[2022-12-16T16:50:40.322+0000] {processor.py:154} INFO - Started process (PID=2523) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:50:40.338+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:50:40.346+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:50:40.345+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:50:40.444+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:50:40.937+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:50:40.936+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:50:41.047+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:50:41.046+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:50:41.156+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.855 seconds
[2022-12-16T16:50:51.376+0000] {processor.py:154} INFO - Started process (PID=2542) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:50:51.406+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:50:51.417+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:50:51.416+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:50:51.596+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:50:51.964+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:50:51.963+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:50:52.269+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:50:52.268+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:50:52.519+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.170 seconds
[2022-12-16T16:51:02.879+0000] {processor.py:154} INFO - Started process (PID=2552) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:51:02.917+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:51:02.924+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:51:02.923+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:51:03.009+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:51:03.532+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:51:03.531+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:51:03.645+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:51:03.644+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:51:03.763+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.902 seconds
[2022-12-16T16:51:14.504+0000] {processor.py:154} INFO - Started process (PID=2562) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:51:14.557+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:51:14.562+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:51:14.561+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:51:14.647+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:51:14.812+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:51:14.811+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:51:14.943+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:51:14.942+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:51:15.051+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.561 seconds
[2022-12-16T16:51:25.679+0000] {processor.py:154} INFO - Started process (PID=2579) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:51:25.710+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:51:25.727+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:51:25.726+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:51:25.935+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:51:26.337+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:51:26.336+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:51:26.522+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:51:26.520+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:51:26.879+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.289 seconds
[2022-12-16T16:51:37.427+0000] {processor.py:154} INFO - Started process (PID=2590) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:51:37.454+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:51:37.467+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:51:37.466+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:51:37.683+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:51:38.195+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:51:38.194+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:51:38.354+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:51:38.351+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:51:38.539+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.177 seconds
[2022-12-16T16:51:49.190+0000] {processor.py:154} INFO - Started process (PID=2600) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:51:49.212+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:51:49.216+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:51:49.215+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:51:49.715+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:51:51.362+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T16:52:01.813+0000] {processor.py:154} INFO - Started process (PID=2610) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:52:01.833+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:52:01.838+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:52:01.837+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:52:01.928+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:52:02.072+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:52:02.071+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:52:02.203+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:52:02.202+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:52:02.572+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.773 seconds
[2022-12-16T16:52:13.142+0000] {processor.py:154} INFO - Started process (PID=2627) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:52:13.189+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:52:13.216+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:52:13.215+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:52:13.469+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:52:13.790+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:52:13.788+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:52:14.045+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:52:14.044+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:52:14.368+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.254 seconds
[2022-12-16T16:52:24.899+0000] {processor.py:154} INFO - Started process (PID=2637) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:52:24.924+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:52:24.933+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:52:24.930+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:52:25.076+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:52:25.612+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:52:25.611+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:52:25.886+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:52:25.885+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:52:26.024+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.169 seconds
[2022-12-16T16:52:36.373+0000] {processor.py:154} INFO - Started process (PID=2647) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:52:36.405+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:52:36.410+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:52:36.409+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:52:36.508+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:52:36.985+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:52:36.984+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:52:37.138+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:52:37.137+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:52:37.309+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.954 seconds
[2022-12-16T16:52:47.677+0000] {processor.py:154} INFO - Started process (PID=2657) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:52:47.719+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:52:47.724+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:52:47.723+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:52:47.829+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:52:47.962+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:52:47.961+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:52:48.121+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:52:48.120+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:52:48.587+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.962 seconds
[2022-12-16T16:52:59.334+0000] {processor.py:154} INFO - Started process (PID=2675) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:52:59.376+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:52:59.398+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:52:59.390+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:52:59.796+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:52:59.985+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:52:59.984+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:53:00.100+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:53:00.099+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:53:00.225+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.908 seconds
[2022-12-16T16:53:10.614+0000] {processor.py:154} INFO - Started process (PID=2685) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:53:10.635+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:53:10.647+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:53:10.640+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:53:10.784+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:53:11.076+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:53:11.075+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:53:11.357+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:53:11.352+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:53:11.539+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.944 seconds
[2022-12-16T16:53:22.029+0000] {processor.py:154} INFO - Started process (PID=2695) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:53:22.625+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:53:22.629+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:53:22.628+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:53:22.723+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:53:23.355+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T16:53:34.087+0000] {processor.py:154} INFO - Started process (PID=2713) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:53:34.138+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:53:34.147+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:53:34.141+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:53:34.258+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:53:34.557+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:53:34.556+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:53:34.884+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:53:34.876+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:53:35.063+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.005 seconds
[2022-12-16T16:53:45.659+0000] {processor.py:154} INFO - Started process (PID=2723) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:53:45.685+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:53:45.699+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:53:45.697+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:53:45.865+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:53:47.017+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:53:47.016+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:53:47.270+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:53:47.260+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:53:47.523+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.901 seconds
[2022-12-16T16:53:58.031+0000] {processor.py:154} INFO - Started process (PID=2733) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:53:58.062+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:53:58.068+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:53:58.066+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:53:58.198+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:53:58.559+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:53:58.558+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:53:58.701+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:53:58.700+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:53:58.964+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.967 seconds
[2022-12-16T16:54:09.308+0000] {processor.py:154} INFO - Started process (PID=2743) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:54:09.316+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:54:09.324+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:54:09.323+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:54:09.536+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:54:10.234+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:54:10.233+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:54:10.434+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:54:10.433+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:54:10.665+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.369 seconds
[2022-12-16T16:54:21.500+0000] {processor.py:154} INFO - Started process (PID=2761) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:54:21.504+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:54:21.513+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:54:21.512+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:54:21.731+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:54:23.455+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:54:23.454+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:54:24.022+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:54:24.016+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:54:24.247+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.788 seconds
[2022-12-16T16:54:34.814+0000] {processor.py:154} INFO - Started process (PID=2771) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:54:34.859+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:54:34.884+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:54:34.880+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:54:35.373+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:54:35.757+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:54:35.756+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:54:35.919+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:54:35.918+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:54:36.030+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.264 seconds
[2022-12-16T16:54:46.444+0000] {processor.py:154} INFO - Started process (PID=2781) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:54:46.469+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:54:46.479+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:54:46.477+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:54:46.690+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:54:47.190+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:54:47.180+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:54:47.667+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:54:47.666+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:54:48.115+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.699 seconds
[2022-12-16T16:54:58.709+0000] {processor.py:154} INFO - Started process (PID=2799) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:54:58.726+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:54:58.746+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:54:58.729+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:54:59.029+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:55:00.071+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T16:55:11.078+0000] {processor.py:154} INFO - Started process (PID=2810) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:55:11.101+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:55:11.110+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:55:11.109+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:55:11.400+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:55:12.086+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:55:12.077+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:55:12.733+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:55:12.732+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:55:13.162+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.160 seconds
[2022-12-16T16:55:23.638+0000] {processor.py:154} INFO - Started process (PID=2820) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:55:23.642+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:55:23.658+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:55:23.657+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:55:24.008+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:55:24.570+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:55:24.569+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:55:24.696+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:55:24.695+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:55:24.902+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.277 seconds
[2022-12-16T16:55:36.034+0000] {processor.py:154} INFO - Started process (PID=2830) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:55:36.053+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:55:36.066+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:55:36.056+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:55:36.251+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:55:36.466+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:55:36.465+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:55:36.581+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:55:36.580+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:55:36.776+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.804 seconds
[2022-12-16T16:55:47.503+0000] {processor.py:154} INFO - Started process (PID=2846) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:55:47.547+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:55:47.554+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:55:47.553+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:55:47.717+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:55:48.181+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:55:48.180+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:55:48.647+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:55:48.646+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:55:48.858+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.422 seconds
[2022-12-16T16:56:00.122+0000] {processor.py:154} INFO - Started process (PID=2860) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:56:00.142+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:56:00.151+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:56:00.149+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:56:00.710+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:56:00.963+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:56:00.962+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:56:01.129+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:56:01.127+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:56:01.352+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.331 seconds
[2022-12-16T16:56:11.831+0000] {processor.py:154} INFO - Started process (PID=2867) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:56:11.835+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:56:11.843+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:56:11.842+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:56:12.040+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:56:13.114+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:56:13.112+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:56:13.285+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:56:13.284+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:56:13.406+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.589 seconds
[2022-12-16T16:56:24.149+0000] {processor.py:154} INFO - Started process (PID=2880) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:56:24.230+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:56:24.239+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:56:24.237+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:56:24.570+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:56:24.861+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:56:24.860+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:56:25.103+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:56:25.102+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:56:25.267+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.163 seconds
[2022-12-16T16:56:35.865+0000] {processor.py:154} INFO - Started process (PID=2899) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:56:35.875+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:56:35.879+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:56:35.878+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:56:36.425+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:56:37.968+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:56:37.967+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:56:38.254+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:56:38.248+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:56:38.490+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.644 seconds
[2022-12-16T16:56:48.980+0000] {processor.py:154} INFO - Started process (PID=2911) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:56:48.983+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:56:48.991+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:56:48.990+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:56:49.155+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:56:49.634+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:56:49.629+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:56:49.823+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:56:49.821+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:56:49.961+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.013 seconds
[2022-12-16T16:57:00.437+0000] {processor.py:154} INFO - Started process (PID=2919) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:57:00.494+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:57:00.501+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:57:00.497+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:57:00.746+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:57:01.004+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:57:00.999+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:57:01.294+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:57:01.293+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:57:01.552+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.154 seconds
[2022-12-16T16:57:12.252+0000] {processor.py:154} INFO - Started process (PID=2936) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:57:12.261+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:57:12.303+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:57:12.289+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:57:12.584+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:57:13.204+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:57:13.202+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:57:13.550+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:57:13.549+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:57:13.672+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.487 seconds
[2022-12-16T16:57:24.752+0000] {processor.py:154} INFO - Started process (PID=2947) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:57:24.763+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:57:24.780+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:57:24.779+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:57:24.920+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:57:25.473+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:57:25.472+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:57:25.719+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:57:25.718+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:57:25.877+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.305 seconds
[2022-12-16T16:57:36.307+0000] {processor.py:154} INFO - Started process (PID=2954) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:57:36.332+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:57:36.336+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:57:36.335+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:57:36.489+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:57:36.741+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:57:36.740+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:57:36.928+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:57:36.927+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:57:37.098+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.815 seconds
[2022-12-16T16:57:47.655+0000] {processor.py:154} INFO - Started process (PID=2964) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:57:47.658+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:57:47.668+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:57:47.667+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:57:47.919+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:57:48.867+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:57:48.866+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:57:49.049+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:57:49.048+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:57:49.231+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.613 seconds
[2022-12-16T16:57:59.680+0000] {processor.py:154} INFO - Started process (PID=2982) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:57:59.719+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:57:59.729+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:57:59.728+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:58:00.190+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:58:00.805+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:58:00.804+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:58:01.209+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:58:01.208+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:58:01.535+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.903 seconds
[2022-12-16T16:58:12.240+0000] {processor.py:154} INFO - Started process (PID=2992) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:58:12.248+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:58:12.252+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:58:12.251+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:58:12.348+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:58:12.712+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:58:12.711+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:58:12.894+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:58:12.893+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:58:13.051+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.831 seconds
[2022-12-16T16:58:23.509+0000] {processor.py:154} INFO - Started process (PID=3002) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:58:23.517+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:58:23.523+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:58:23.522+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:58:23.634+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:58:24.146+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T16:58:34.718+0000] {processor.py:154} INFO - Started process (PID=3012) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:58:34.747+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:58:34.752+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:58:34.751+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:58:34.939+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:58:35.349+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:58:35.348+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:58:35.530+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:58:35.529+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:58:35.644+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.949 seconds
[2022-12-16T16:58:46.227+0000] {processor.py:154} INFO - Started process (PID=3030) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:58:46.239+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:58:46.252+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:58:46.251+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:58:46.436+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:58:47.012+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:58:47.011+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:58:47.498+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:58:47.492+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:58:47.785+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.591 seconds
[2022-12-16T16:58:58.274+0000] {processor.py:154} INFO - Started process (PID=3040) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:58:58.329+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:58:58.334+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:58:58.332+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:58:58.548+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:58:58.997+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:58:58.996+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:58:59.186+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:58:59.185+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:58:59.299+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.041 seconds
[2022-12-16T16:59:09.632+0000] {processor.py:154} INFO - Started process (PID=3050) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:59:09.635+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:59:09.639+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:59:09.638+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:59:09.727+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:59:10.410+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:59:10.408+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:59:10.882+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:59:10.881+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:59:11.156+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.540 seconds
[2022-12-16T16:59:21.908+0000] {processor.py:154} INFO - Started process (PID=3066) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:59:21.923+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:59:21.934+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:59:21.933+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:59:22.041+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:59:22.224+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:59:22.223+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:59:22.479+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:59:22.477+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:59:23.050+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.158 seconds
[2022-12-16T16:59:33.491+0000] {processor.py:154} INFO - Started process (PID=3077) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:59:33.548+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:59:33.557+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:59:33.555+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:59:33.663+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:59:33.809+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:59:33.808+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:59:33.948+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:59:33.947+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:59:34.082+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.609 seconds
[2022-12-16T16:59:44.351+0000] {processor.py:154} INFO - Started process (PID=3087) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:59:44.425+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:59:44.439+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:59:44.438+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:59:44.544+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:59:44.683+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:59:44.682+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T16:59:44.807+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:59:44.806+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T16:59:44.968+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.631 seconds
[2022-12-16T16:59:55.280+0000] {processor.py:154} INFO - Started process (PID=3097) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:59:55.340+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T16:59:55.345+0000] {logging_mixin.py:137} INFO - [2022-12-16T16:59:55.344+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:59:55.434+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T16:59:56.714+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T17:00:07.222+0000] {processor.py:154} INFO - Started process (PID=3116) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:00:07.247+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:00:07.252+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:00:07.252+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:00:07.371+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:00:07.694+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:00:07.693+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:00:07.832+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:00:07.831+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:00:07.969+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.774 seconds
[2022-12-16T17:00:18.684+0000] {processor.py:154} INFO - Started process (PID=3126) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:00:18.688+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:00:18.691+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:00:18.690+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:00:18.808+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:00:20.245+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:00:20.244+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:00:20.430+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:00:20.429+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:00:20.543+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.873 seconds
[2022-12-16T17:00:30.843+0000] {processor.py:154} INFO - Started process (PID=3136) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:00:30.866+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:00:30.870+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:00:30.869+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:00:30.966+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:00:31.638+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:00:31.637+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:00:31.767+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:00:31.766+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:00:31.882+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.054 seconds
[2022-12-16T17:00:42.206+0000] {processor.py:154} INFO - Started process (PID=3146) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:00:42.235+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:00:42.241+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:00:42.240+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:00:42.329+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:00:43.171+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:00:43.170+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:00:43.286+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:00:43.285+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:00:43.395+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.204 seconds
[2022-12-16T17:00:53.864+0000] {processor.py:154} INFO - Started process (PID=3164) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:00:53.881+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:00:53.886+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:00:53.885+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:00:53.985+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:00:55.132+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T17:01:05.319+0000] {processor.py:154} INFO - Started process (PID=3174) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:01:05.372+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:01:05.377+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:01:05.376+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:01:05.459+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:01:05.902+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:01:05.901+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:01:06.013+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:01:06.012+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:01:06.122+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.817 seconds
[2022-12-16T17:01:16.420+0000] {processor.py:154} INFO - Started process (PID=3184) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:01:16.464+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:01:16.469+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:01:16.468+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:01:16.554+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:01:16.821+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T17:01:27.099+0000] {processor.py:154} INFO - Started process (PID=3201) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:01:27.147+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:01:27.155+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:01:27.155+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:01:27.367+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:01:28.646+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T17:01:39.449+0000] {processor.py:154} INFO - Started process (PID=3212) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:01:39.479+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:01:39.484+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:01:39.483+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:01:39.568+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:01:39.700+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:01:39.699+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:01:39.811+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:01:39.810+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:01:39.942+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.505 seconds
[2022-12-16T17:01:50.243+0000] {processor.py:154} INFO - Started process (PID=3222) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:01:50.272+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:01:50.285+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:01:50.284+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:01:50.402+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:01:50.537+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:01:50.536+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:01:50.647+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:01:50.647+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:01:50.763+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.535 seconds
[2022-12-16T17:02:01.107+0000] {processor.py:154} INFO - Started process (PID=3232) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:02:01.143+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:02:01.148+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:02:01.147+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:02:01.268+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:02:02.568+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T17:02:13.033+0000] {processor.py:154} INFO - Started process (PID=3250) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:02:13.066+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:02:13.073+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:02:13.069+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:02:13.322+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:02:14.948+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T17:02:25.423+0000] {processor.py:154} INFO - Started process (PID=3260) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:02:25.487+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:02:25.491+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:02:25.490+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:02:25.587+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:02:26.203+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:02:26.203+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:02:26.320+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:02:26.319+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:02:26.689+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.284 seconds
[2022-12-16T17:02:37.742+0000] {processor.py:154} INFO - Started process (PID=3270) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:02:37.796+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:02:37.802+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:02:37.801+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:02:37.902+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:02:38.456+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:02:38.455+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:02:38.573+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:02:38.572+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:02:38.700+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.005 seconds
[2022-12-16T17:02:48.993+0000] {processor.py:154} INFO - Started process (PID=3287) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:02:49.021+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:02:49.026+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:02:49.025+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:02:49.142+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:02:50.537+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:02:50.536+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:02:50.751+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:02:50.750+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:02:51.007+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.038 seconds
[2022-12-16T17:03:01.470+0000] {processor.py:154} INFO - Started process (PID=3301) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:03:01.474+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:03:01.478+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:03:01.477+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:03:01.607+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:03:01.800+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:03:01.799+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:03:01.933+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:03:01.932+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:03:02.109+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.657 seconds
[2022-12-16T17:03:12.320+0000] {processor.py:154} INFO - Started process (PID=3308) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:03:12.369+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:03:12.385+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:03:12.384+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:03:12.482+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:03:12.619+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:03:12.618+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:03:12.739+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:03:12.738+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:03:12.898+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.594 seconds
[2022-12-16T17:03:23.210+0000] {processor.py:154} INFO - Started process (PID=3318) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:03:23.239+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:03:23.244+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:03:23.243+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:03:23.330+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:03:23.568+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:03:23.567+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:03:23.680+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:03:23.679+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:03:23.819+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.622 seconds
[2022-12-16T17:03:34.254+0000] {processor.py:154} INFO - Started process (PID=3337) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:03:34.274+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:03:34.283+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:03:34.282+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:03:34.430+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:03:34.936+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:03:34.935+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:03:35.070+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:03:35.069+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:03:35.259+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.022 seconds
[2022-12-16T17:03:45.647+0000] {processor.py:154} INFO - Started process (PID=3347) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:03:45.679+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:03:45.683+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:03:45.682+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:03:45.766+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:03:46.983+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:03:46.982+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:03:47.121+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:03:47.120+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:03:47.251+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.618 seconds
[2022-12-16T17:03:57.586+0000] {processor.py:154} INFO - Started process (PID=3357) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:03:57.657+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:03:57.662+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:03:57.660+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:03:57.745+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:03:57.917+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:03:57.915+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:03:58.046+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:03:58.045+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:03:58.157+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.585 seconds
[2022-12-16T17:04:08.508+0000] {processor.py:154} INFO - Started process (PID=3373) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:04:08.529+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:04:08.533+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:04:08.532+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:04:08.687+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:04:09.213+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:04:09.211+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:04:09.356+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:04:09.355+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:04:09.552+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.071 seconds
[2022-12-16T17:04:20.019+0000] {processor.py:154} INFO - Started process (PID=3384) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:04:20.023+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:04:20.029+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:04:20.027+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:04:20.142+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:04:20.337+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:04:20.336+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:04:20.511+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:04:20.511+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:04:20.634+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.632 seconds
[2022-12-16T17:04:30.941+0000] {processor.py:154} INFO - Started process (PID=3394) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:04:30.988+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:04:30.999+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:04:30.998+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:04:31.141+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:04:31.852+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:04:31.851+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:04:31.963+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:04:31.962+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:04:32.092+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.166 seconds
[2022-12-16T17:04:42.401+0000] {processor.py:154} INFO - Started process (PID=3404) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:04:42.446+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:04:42.451+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:04:42.450+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:04:42.575+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:04:43.004+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:04:43.003+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:04:43.163+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:04:43.162+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:04:43.306+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.919 seconds
[2022-12-16T17:04:53.660+0000] {processor.py:154} INFO - Started process (PID=3421) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:04:53.704+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:04:53.708+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:04:53.707+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:04:53.853+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:04:54.005+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:04:54.004+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:04:54.130+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:04:54.129+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:04:54.495+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.849 seconds
[2022-12-16T17:05:05.016+0000] {processor.py:154} INFO - Started process (PID=3431) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:05:05.063+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:05:05.068+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:05:05.067+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:05:05.156+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:05:05.304+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:05:05.303+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:05:05.413+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:05:05.412+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:05:05.549+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.546 seconds
[2022-12-16T17:05:16.454+0000] {processor.py:154} INFO - Started process (PID=3441) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:05:16.498+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:05:16.502+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:05:16.501+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:05:16.588+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:05:16.718+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:05:16.717+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:05:16.830+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:05:16.829+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:05:16.963+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.521 seconds
[2022-12-16T17:05:27.340+0000] {processor.py:154} INFO - Started process (PID=3451) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:05:27.369+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:05:27.375+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:05:27.373+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:05:27.463+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:05:27.849+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:05:27.848+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:05:28.105+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:05:28.104+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:05:28.392+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.066 seconds
[2022-12-16T17:05:38.888+0000] {processor.py:154} INFO - Started process (PID=3469) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:05:38.915+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:05:38.920+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:05:38.919+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:05:39.005+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:05:39.657+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:05:39.656+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:05:39.773+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:05:39.772+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:05:39.883+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.010 seconds
[2022-12-16T17:05:50.206+0000] {processor.py:154} INFO - Started process (PID=3479) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:05:50.228+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:05:50.234+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:05:50.232+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:05:50.324+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:05:51.572+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T17:06:02.477+0000] {processor.py:154} INFO - Started process (PID=3489) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:06:02.528+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:06:02.532+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:06:02.531+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:06:02.625+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:06:02.773+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:06:02.771+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:06:02.886+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:06:02.885+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:06:03.000+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.540 seconds
[2022-12-16T17:06:13.291+0000] {processor.py:154} INFO - Started process (PID=3507) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:06:13.332+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:06:13.337+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:06:13.336+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:06:13.506+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:06:13.874+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:06:13.873+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:06:14.017+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:06:14.016+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:06:14.165+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.903 seconds
[2022-12-16T17:06:24.739+0000] {processor.py:154} INFO - Started process (PID=3517) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:06:24.790+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:06:24.794+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:06:24.793+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:06:24.880+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:06:25.032+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:06:25.031+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:06:25.163+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:06:25.162+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:06:25.314+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.589 seconds
[2022-12-16T17:06:35.527+0000] {processor.py:154} INFO - Started process (PID=3527) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:06:35.587+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:06:35.595+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:06:35.595+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:06:35.698+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:06:36.341+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:06:36.340+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:06:36.460+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:06:36.459+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:06:36.600+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.101 seconds
[2022-12-16T17:06:46.737+0000] {processor.py:154} INFO - Started process (PID=3537) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:06:46.831+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:06:46.835+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:06:46.834+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:06:46.961+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:06:47.124+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:06:47.123+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:06:47.266+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:06:47.265+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:06:47.416+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.693 seconds
[2022-12-16T17:06:57.925+0000] {processor.py:154} INFO - Started process (PID=3554) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:06:57.979+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:06:57.988+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:06:57.987+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:06:58.167+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:06:58.345+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:06:58.343+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:06:58.491+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:06:58.490+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:06:58.649+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.767 seconds
[2022-12-16T17:07:08.952+0000] {processor.py:154} INFO - Started process (PID=3564) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:07:08.982+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:07:08.987+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:07:08.986+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:07:09.068+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:07:09.207+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:07:09.207+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:07:09.323+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:07:09.322+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:07:09.456+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.518 seconds
[2022-12-16T17:07:19.581+0000] {processor.py:154} INFO - Started process (PID=3574) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:07:19.665+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:07:19.677+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:07:19.673+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:07:19.789+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:07:20.208+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:07:20.207+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:07:20.365+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:07:20.364+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:07:20.509+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.948 seconds
[2022-12-16T17:07:30.869+0000] {processor.py:154} INFO - Started process (PID=3592) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:07:30.889+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:07:30.897+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:07:30.896+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:07:31.088+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:07:31.975+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T17:07:42.631+0000] {processor.py:154} INFO - Started process (PID=3603) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:07:42.679+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:07:42.684+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:07:42.683+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:07:42.769+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:07:43.495+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:07:43.494+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:07:43.745+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:07:43.744+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:07:43.963+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.346 seconds
[2022-12-16T17:07:54.284+0000] {processor.py:154} INFO - Started process (PID=3613) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:07:54.303+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:07:54.306+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:07:54.305+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:07:54.401+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:07:55.262+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T17:08:05.606+0000] {processor.py:154} INFO - Started process (PID=3623) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:08:05.627+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:08:05.632+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:08:05.631+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:08:05.719+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:08:05.854+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:08:05.853+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:08:05.967+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:08:05.966+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:08:06.102+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.510 seconds
[2022-12-16T17:08:16.311+0000] {processor.py:154} INFO - Started process (PID=3642) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:08:16.325+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:08:16.334+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:08:16.329+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:08:16.496+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:08:16.812+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:08:16.811+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:08:17.053+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:08:17.052+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:08:17.367+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.074 seconds
[2022-12-16T17:08:27.871+0000] {processor.py:154} INFO - Started process (PID=3652) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:08:27.923+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:08:27.928+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:08:27.927+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:08:28.015+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:08:28.162+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:08:28.161+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:08:28.283+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:08:28.282+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:08:28.387+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.529 seconds
[2022-12-16T17:08:39.344+0000] {processor.py:154} INFO - Started process (PID=3662) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:08:39.391+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:08:39.395+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:08:39.395+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:08:39.489+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:08:40.822+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:08:40.821+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:08:40.934+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:08:40.934+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:08:41.076+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.747 seconds
[2022-12-16T17:08:51.255+0000] {processor.py:154} INFO - Started process (PID=3672) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:08:51.281+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:08:51.286+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:08:51.285+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:08:51.370+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:08:51.510+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:08:51.509+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:08:51.640+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:08:51.639+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:08:52.053+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.812 seconds
[2022-12-16T17:09:02.460+0000] {processor.py:154} INFO - Started process (PID=3690) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:09:02.550+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:09:02.563+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:09:02.562+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:09:02.820+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:09:03.134+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:09:03.132+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:09:03.295+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:09:03.295+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:09:03.424+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.988 seconds
[2022-12-16T17:09:13.708+0000] {processor.py:154} INFO - Started process (PID=3700) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:09:13.753+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:09:13.758+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:09:13.757+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:09:13.844+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:09:13.976+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:09:13.975+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:09:14.089+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:09:14.088+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:09:14.229+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.542 seconds
[2022-12-16T17:09:24.498+0000] {processor.py:154} INFO - Started process (PID=3710) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:09:24.547+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:09:24.551+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:09:24.550+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:09:24.647+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:09:24.795+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:09:24.794+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:09:24.911+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:09:24.910+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:09:25.042+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.558 seconds
[2022-12-16T17:09:35.508+0000] {processor.py:154} INFO - Started process (PID=3727) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:09:35.519+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:09:35.523+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:09:35.522+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:09:35.701+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:09:35.936+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:09:35.935+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:09:36.201+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:09:36.201+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:09:36.487+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.009 seconds
[2022-12-16T17:09:47.093+0000] {processor.py:154} INFO - Started process (PID=3738) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:09:47.119+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:09:47.123+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:09:47.122+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:09:47.210+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:09:48.289+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:09:48.287+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:09:48.408+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:09:48.407+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:09:48.534+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.459 seconds
[2022-12-16T17:09:58.782+0000] {processor.py:154} INFO - Started process (PID=3748) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:09:58.831+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:09:58.835+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:09:58.834+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:09:58.923+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:09:59.878+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T17:10:10.192+0000] {processor.py:154} INFO - Started process (PID=3758) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:10:10.234+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:10:10.239+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:10:10.238+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:10:10.331+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:10:10.494+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:10:10.493+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:10:10.617+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:10:10.616+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:10:11.057+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.880 seconds
[2022-12-16T17:10:21.590+0000] {processor.py:154} INFO - Started process (PID=3776) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:10:21.613+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:10:21.632+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:10:21.616+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:10:21.745+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:10:22.262+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:10:22.261+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:10:22.401+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:10:22.400+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:10:22.514+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.951 seconds
[2022-12-16T17:10:32.967+0000] {processor.py:154} INFO - Started process (PID=3786) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:10:32.993+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:10:32.997+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:10:32.996+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:10:33.154+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:10:33.307+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:10:33.306+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:10:33.480+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:10:33.480+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:10:33.645+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.707 seconds
[2022-12-16T17:10:44.122+0000] {processor.py:154} INFO - Started process (PID=3796) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:10:44.144+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:10:44.171+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:10:44.170+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:10:44.464+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:10:45.164+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:10:45.163+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:10:45.288+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:10:45.287+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:10:45.406+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.304 seconds
[2022-12-16T17:10:55.904+0000] {processor.py:154} INFO - Started process (PID=3812) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:10:55.933+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:10:55.949+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:10:55.948+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:10:56.603+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:10:56.978+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:10:56.977+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:10:57.225+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:10:57.224+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:10:57.499+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.652 seconds
[2022-12-16T17:11:08.404+0000] {processor.py:154} INFO - Started process (PID=3824) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:11:08.423+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:11:08.444+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:11:08.443+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:11:08.919+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:11:09.307+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:11:09.305+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:11:09.888+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:11:09.887+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:11:10.452+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.211 seconds
[2022-12-16T17:11:21.098+0000] {processor.py:154} INFO - Started process (PID=3834) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:11:21.170+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:11:21.181+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:11:21.179+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:11:21.362+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:11:21.557+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:11:21.556+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:11:21.684+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:11:21.683+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:11:21.821+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.803 seconds
[2022-12-16T17:11:32.063+0000] {processor.py:154} INFO - Started process (PID=3844) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:11:32.110+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:11:32.115+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:11:32.113+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:11:32.234+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:11:32.636+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:11:32.635+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:11:32.780+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:11:32.780+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:11:32.922+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.873 seconds
[2022-12-16T17:11:43.731+0000] {processor.py:154} INFO - Started process (PID=3861) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:11:43.763+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:11:43.779+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:11:43.778+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:11:44.209+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:11:44.393+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:11:44.392+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:11:44.599+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:11:44.599+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:11:44.850+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.178 seconds
[2022-12-16T17:11:55.233+0000] {processor.py:154} INFO - Started process (PID=3872) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:11:55.267+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:11:55.272+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:11:55.271+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:11:55.366+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:11:56.363+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:11:56.362+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:11:56.474+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:11:56.473+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:11:56.624+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.405 seconds
[2022-12-16T17:12:06.875+0000] {processor.py:154} INFO - Started process (PID=3882) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:12:06.924+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:12:06.933+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:12:06.928+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:12:07.072+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:12:07.300+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:12:07.299+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:12:07.476+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:12:07.475+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:12:07.688+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.827 seconds
[2022-12-16T17:12:18.684+0000] {processor.py:154} INFO - Started process (PID=3892) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:12:18.713+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:12:18.721+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:12:18.720+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:12:18.822+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:12:18.951+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:12:18.950+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:12:19.064+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:12:19.063+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:12:19.240+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.586 seconds
[2022-12-16T17:12:29.662+0000] {processor.py:154} INFO - Started process (PID=3910) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:12:29.706+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:12:29.717+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:12:29.713+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:12:29.817+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:12:29.991+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:12:29.990+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:12:30.209+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:12:30.208+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:12:30.715+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.067 seconds
[2022-12-16T17:12:40.977+0000] {processor.py:154} INFO - Started process (PID=3920) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:12:40.998+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:12:41.003+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:12:41.002+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:12:41.108+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:12:41.270+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:12:41.269+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:12:41.394+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:12:41.393+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:12:41.510+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.547 seconds
[2022-12-16T17:12:51.812+0000] {processor.py:154} INFO - Started process (PID=3930) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:12:51.835+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:12:51.839+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:12:51.838+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:12:51.920+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:12:52.057+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:12:52.056+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:12:52.173+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:12:52.172+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:12:52.288+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.492 seconds
[2022-12-16T17:13:02.574+0000] {processor.py:154} INFO - Started process (PID=3940) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:13:02.585+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:13:02.589+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:13:02.588+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:13:02.674+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:13:02.804+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:13:02.803+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:13:02.920+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:13:02.920+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:13:03.062+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.504 seconds
[2022-12-16T17:13:13.801+0000] {processor.py:154} INFO - Started process (PID=3957) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:13:13.853+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:13:13.864+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:13:13.864+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:13:13.993+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:13:14.858+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:13:14.852+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:13:15.157+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:13:15.156+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:13:15.353+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.566 seconds
[2022-12-16T17:13:25.678+0000] {processor.py:154} INFO - Started process (PID=3967) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:13:25.732+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:13:25.744+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:13:25.743+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:13:25.839+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:13:26.618+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:13:26.617+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:13:26.731+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:13:26.730+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:13:26.862+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.198 seconds
[2022-12-16T17:13:37.171+0000] {processor.py:154} INFO - Started process (PID=3977) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:13:37.220+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:13:37.224+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:13:37.223+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:13:37.311+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:13:37.556+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:13:37.555+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:13:37.668+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:13:37.667+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:13:37.782+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.625 seconds
[2022-12-16T17:13:48.115+0000] {processor.py:154} INFO - Started process (PID=3995) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:13:48.167+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:13:48.171+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:13:48.170+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:13:48.311+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:13:48.553+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:13:48.551+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:13:49.013+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:13:49.012+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:13:49.207+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.108 seconds
[2022-12-16T17:13:59.652+0000] {processor.py:154} INFO - Started process (PID=4005) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:13:59.684+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:13:59.689+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:13:59.687+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:13:59.777+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:13:59.907+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:13:59.906+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:14:00.020+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:14:00.019+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:14:00.168+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.529 seconds
[2022-12-16T17:14:10.488+0000] {processor.py:154} INFO - Started process (PID=4015) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:14:10.539+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:14:10.543+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:14:10.542+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:14:10.635+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:14:10.939+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T17:14:21.371+0000] {processor.py:154} INFO - Started process (PID=4025) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:14:21.419+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:14:21.431+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:14:21.430+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:14:21.536+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:14:21.681+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:14:21.680+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:14:21.811+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:14:21.811+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:14:21.982+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.625 seconds
[2022-12-16T17:14:32.517+0000] {processor.py:154} INFO - Started process (PID=4042) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:14:32.537+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:14:32.558+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:14:32.548+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:14:32.724+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:14:34.272+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:14:34.271+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:14:34.422+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:14:34.421+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:14:34.587+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.104 seconds
[2022-12-16T17:14:44.917+0000] {processor.py:154} INFO - Started process (PID=4052) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:14:44.987+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:14:44.995+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:14:44.994+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:14:45.097+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:14:46.243+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T17:14:56.616+0000] {processor.py:154} INFO - Started process (PID=4062) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:14:56.639+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:14:56.644+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:14:56.643+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:14:56.724+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:14:56.853+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:14:56.852+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:14:56.971+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:14:56.970+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:14:57.138+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.540 seconds
[2022-12-16T17:15:07.499+0000] {processor.py:154} INFO - Started process (PID=4080) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:15:07.523+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:15:07.536+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:15:07.535+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:15:07.754+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:15:08.125+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:15:08.124+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:15:08.254+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:15:08.253+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:15:08.419+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.936 seconds
[2022-12-16T17:15:18.623+0000] {processor.py:154} INFO - Started process (PID=4091) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:15:18.639+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:15:18.643+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:15:18.642+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:15:18.731+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:15:18.870+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:15:18.869+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:15:19.036+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:15:19.035+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:15:19.146+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.537 seconds
[2022-12-16T17:15:30.167+0000] {processor.py:154} INFO - Started process (PID=4101) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:15:30.192+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:15:30.196+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:15:30.195+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:15:30.288+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:15:31.607+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T17:15:42.522+0000] {processor.py:154} INFO - Started process (PID=4111) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:15:42.543+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:15:42.548+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:15:42.547+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:15:42.631+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:15:42.761+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:15:42.760+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:15:42.873+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:15:42.872+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:15:43.020+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.513 seconds
[2022-12-16T17:15:53.464+0000] {processor.py:154} INFO - Started process (PID=4129) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:15:53.521+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:15:53.525+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:15:53.524+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:15:53.636+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:15:53.818+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:15:53.817+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:15:53.946+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:15:53.945+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:15:54.102+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.662 seconds
[2022-12-16T17:16:04.781+0000] {processor.py:154} INFO - Started process (PID=4139) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:16:04.807+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:16:04.834+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:16:04.832+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:16:05.238+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:16:06.934+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:16:06.929+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:16:07.592+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:16:07.591+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:16:07.959+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.269 seconds
[2022-12-16T17:16:18.548+0000] {processor.py:154} INFO - Started process (PID=4149) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:16:18.552+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:16:18.557+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:16:18.555+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:16:18.693+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:16:19.034+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:16:19.033+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:16:19.213+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:16:19.211+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:16:19.388+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.919 seconds
[2022-12-16T17:16:29.874+0000] {processor.py:154} INFO - Started process (PID=4166) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:16:29.898+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:16:29.914+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:16:29.913+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:16:30.112+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:16:30.586+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:16:30.585+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:16:30.872+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:16:30.871+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:16:30.990+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.147 seconds
[2022-12-16T17:16:41.622+0000] {processor.py:154} INFO - Started process (PID=4177) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:16:41.635+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:16:41.650+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:16:41.649+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:16:41.770+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:16:41.972+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:16:41.971+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:16:42.117+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:16:42.116+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:16:42.578+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.019 seconds
[2022-12-16T17:16:53.096+0000] {processor.py:154} INFO - Started process (PID=4187) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:16:53.128+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:16:53.135+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:16:53.134+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:16:53.467+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:16:53.627+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:16:53.626+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:16:53.745+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:16:53.744+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:16:53.883+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.850 seconds
[2022-12-16T17:17:04.386+0000] {processor.py:154} INFO - Started process (PID=4197) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:17:04.429+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:17:04.433+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:17:04.432+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:17:04.670+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:17:04.877+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:17:04.876+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:17:05.098+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:17:05.097+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:17:05.235+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.872 seconds
[2022-12-16T17:17:15.484+0000] {processor.py:154} INFO - Started process (PID=4214) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:17:15.496+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:17:15.501+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:17:15.500+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:17:15.624+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:17:16.984+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:17:16.983+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:17:17.351+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:17:17.350+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:17:17.852+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.384 seconds
[2022-12-16T17:17:28.237+0000] {processor.py:154} INFO - Started process (PID=4224) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:17:28.279+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:17:28.284+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:17:28.283+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:17:28.422+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:17:29.390+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T17:17:39.683+0000] {processor.py:154} INFO - Started process (PID=4234) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:17:39.733+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:17:39.738+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:17:39.737+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:17:39.850+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:17:40.410+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:17:40.405+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:17:40.670+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:17:40.669+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:17:40.907+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.239 seconds
[2022-12-16T17:17:51.240+0000] {processor.py:154} INFO - Started process (PID=4244) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:17:51.280+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:17:51.286+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:17:51.283+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:17:51.411+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:17:51.616+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:17:51.615+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:17:52.008+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:17:52.007+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:17:52.340+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.117 seconds
[2022-12-16T17:18:02.713+0000] {processor.py:154} INFO - Started process (PID=4264) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:18:02.718+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:18:02.735+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:18:02.721+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:18:02.997+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:18:03.807+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:18:03.806+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:18:04.101+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:18:04.100+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:18:04.528+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.854 seconds
[2022-12-16T17:18:15.221+0000] {processor.py:154} INFO - Started process (PID=4274) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:18:15.263+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:18:15.268+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:18:15.267+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:18:15.358+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:18:15.549+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:18:15.548+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:18:15.794+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:18:15.793+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:18:16.067+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.858 seconds
[2022-12-16T17:18:26.422+0000] {processor.py:154} INFO - Started process (PID=4284) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:18:26.641+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:18:26.658+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:18:26.644+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:18:26.827+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:18:28.739+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:18:28.738+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:18:29.023+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:18:29.022+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:18:29.165+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.760 seconds
[2022-12-16T17:18:39.733+0000] {processor.py:154} INFO - Started process (PID=4301) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:18:39.790+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:18:39.795+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:18:39.794+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:18:40.004+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:18:40.745+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:18:40.744+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:18:41.105+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:18:41.104+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:18:41.278+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.566 seconds
[2022-12-16T17:18:51.632+0000] {processor.py:154} INFO - Started process (PID=4311) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:18:51.679+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:18:51.685+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:18:51.683+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:18:51.772+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:18:51.906+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:18:51.905+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:18:52.030+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:18:52.029+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:18:52.153+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.534 seconds
[2022-12-16T17:19:02.307+0000] {processor.py:154} INFO - Started process (PID=4321) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:19:02.337+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:19:02.341+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:19:02.340+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:19:02.431+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:19:02.576+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:19:02.575+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:19:02.711+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:19:02.710+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:19:02.845+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.552 seconds
[2022-12-16T17:19:13.153+0000] {processor.py:154} INFO - Started process (PID=4331) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:19:13.206+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:19:13.211+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:19:13.210+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:19:13.324+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:19:13.676+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:19:13.675+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:19:13.848+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:19:13.847+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:19:13.956+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.817 seconds
[2022-12-16T17:19:24.390+0000] {processor.py:154} INFO - Started process (PID=4349) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:19:24.396+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:19:24.404+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:19:24.399+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:19:24.519+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:19:24.780+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:19:24.779+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:19:25.064+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:19:25.063+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:19:25.279+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.919 seconds
[2022-12-16T17:19:35.876+0000] {processor.py:154} INFO - Started process (PID=4359) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:19:35.888+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:19:35.896+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:19:35.895+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:19:36.087+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:19:36.245+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:19:36.244+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:19:36.367+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:19:36.366+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:19:36.519+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.689 seconds
[2022-12-16T17:19:46.853+0000] {processor.py:154} INFO - Started process (PID=4369) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:19:46.864+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:19:46.868+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:19:46.867+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:19:47.005+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:19:47.819+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:19:47.818+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:19:47.997+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:19:47.996+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:19:48.151+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.331 seconds
[2022-12-16T17:19:58.482+0000] {processor.py:154} INFO - Started process (PID=4379) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:19:58.486+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:19:58.490+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:19:58.489+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:19:58.580+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:19:59.634+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:19:59.632+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:19:59.765+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:19:59.764+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:19:59.929+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.461 seconds
[2022-12-16T17:20:10.440+0000] {processor.py:154} INFO - Started process (PID=4396) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:20:10.478+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:20:10.491+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:20:10.489+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:20:10.690+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:20:12.509+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:20:12.508+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:20:12.728+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:20:12.727+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:20:13.016+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.650 seconds
[2022-12-16T17:20:23.759+0000] {processor.py:154} INFO - Started process (PID=4406) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:20:23.811+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:20:23.820+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:20:23.819+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:20:23.937+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:20:24.110+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:20:24.109+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:20:24.263+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:20:24.263+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:20:24.453+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.709 seconds
[2022-12-16T17:20:34.873+0000] {processor.py:154} INFO - Started process (PID=4416) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:20:34.902+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:20:34.906+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:20:34.905+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:20:35.098+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:20:36.463+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T17:20:46.901+0000] {processor.py:154} INFO - Started process (PID=4432) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:20:46.951+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:20:46.960+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:20:46.959+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:20:47.336+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:20:48.023+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:20:48.022+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:20:48.156+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:20:48.156+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:20:48.379+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.507 seconds
[2022-12-16T17:20:59.228+0000] {processor.py:154} INFO - Started process (PID=4443) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:20:59.313+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:20:59.318+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:20:59.316+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:20:59.417+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:21:00.838+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T17:21:11.723+0000] {processor.py:154} INFO - Started process (PID=4453) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:21:11.738+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:21:11.743+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:21:11.741+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:21:11.853+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:21:12.041+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:21:12.040+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:21:12.167+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:21:12.166+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:21:12.370+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.664 seconds
[2022-12-16T17:21:22.664+0000] {processor.py:154} INFO - Started process (PID=4463) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:21:22.690+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:21:22.695+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:21:22.693+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:21:22.781+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:21:23.395+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:21:23.389+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:21:23.759+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:21:23.758+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:21:23.968+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.318 seconds
[2022-12-16T17:21:34.365+0000] {processor.py:154} INFO - Started process (PID=4481) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:21:34.385+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:21:34.398+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:21:34.388+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:21:34.567+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:21:34.731+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:21:34.729+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:21:34.880+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:21:34.879+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:21:35.504+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.169 seconds
[2022-12-16T17:21:46.178+0000] {processor.py:154} INFO - Started process (PID=4491) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:21:46.196+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:21:46.203+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:21:46.200+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:21:46.287+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:21:46.419+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:21:46.418+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:21:46.528+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:21:46.527+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:21:46.661+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.500 seconds
[2022-12-16T17:21:56.940+0000] {processor.py:154} INFO - Started process (PID=4501) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:21:56.988+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:21:56.992+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:21:56.991+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:21:57.076+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:21:57.648+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T17:22:07.997+0000] {processor.py:154} INFO - Started process (PID=4511) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:22:08.024+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:22:08.029+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:22:08.028+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:22:08.112+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:22:08.244+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:22:08.243+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:22:08.403+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:22:08.402+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:22:08.545+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.561 seconds
[2022-12-16T17:22:19.638+0000] {processor.py:154} INFO - Started process (PID=4530) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:22:19.651+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:22:19.659+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:22:19.658+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:22:19.813+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:22:20.491+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:22:20.490+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:22:20.681+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:22:20.680+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:22:20.945+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.348 seconds
[2022-12-16T17:22:31.310+0000] {processor.py:154} INFO - Started process (PID=4540) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:22:31.335+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:22:31.339+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:22:31.338+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:22:31.434+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:22:31.590+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:22:31.589+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:22:31.722+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:22:31.721+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:22:31.834+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.538 seconds
[2022-12-16T17:22:42.141+0000] {processor.py:154} INFO - Started process (PID=4550) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:22:42.188+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:22:42.192+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:22:42.191+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:22:42.287+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:22:42.495+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:22:42.485+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:22:42.674+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:22:42.673+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:22:42.786+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.663 seconds
[2022-12-16T17:22:53.108+0000] {processor.py:154} INFO - Started process (PID=4560) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:22:53.135+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:22:53.141+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:22:53.139+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:22:53.296+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:22:53.512+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:22:53.511+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:22:53.706+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:22:53.705+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:22:53.885+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.802 seconds
[2022-12-16T17:23:04.572+0000] {processor.py:154} INFO - Started process (PID=4579) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:23:04.613+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:23:04.645+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:23:04.616+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:23:04.988+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:23:05.864+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:23:05.863+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:23:06.103+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:23:06.102+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:23:06.217+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.735 seconds
[2022-12-16T17:23:16.646+0000] {processor.py:154} INFO - Started process (PID=4589) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:23:16.668+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:23:16.671+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:23:16.670+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:23:16.898+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:23:17.730+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:23:17.729+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:23:17.855+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:23:17.854+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:23:18.028+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.426 seconds
[2022-12-16T17:23:28.243+0000] {processor.py:154} INFO - Started process (PID=4599) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:23:28.269+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:23:28.273+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:23:28.272+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:23:28.501+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:23:29.464+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T17:23:39.880+0000] {processor.py:154} INFO - Started process (PID=4616) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:23:39.930+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:23:39.938+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:23:39.937+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:23:40.142+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:23:40.479+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:23:40.477+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:23:40.615+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:23:40.614+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:23:41.031+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.184 seconds
[2022-12-16T17:23:51.347+0000] {processor.py:154} INFO - Started process (PID=4627) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:23:51.375+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:23:51.380+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:23:51.379+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:23:51.507+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:23:52.004+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:23:51.994+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:23:52.205+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:23:52.204+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:23:52.398+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.072 seconds
[2022-12-16T17:24:02.786+0000] {processor.py:154} INFO - Started process (PID=4637) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:24:02.832+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:24:02.836+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:24:02.835+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:24:02.928+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:24:03.076+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:24:03.075+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:24:03.292+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:24:03.291+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:24:03.526+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.759 seconds
[2022-12-16T17:24:13.947+0000] {processor.py:154} INFO - Started process (PID=4647) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:24:13.971+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:24:13.975+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:24:13.975+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:24:14.067+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:24:14.469+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:24:14.468+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:24:14.636+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:24:14.635+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:24:14.789+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.856 seconds
[2022-12-16T17:24:25.270+0000] {processor.py:154} INFO - Started process (PID=4664) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:24:25.298+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:24:25.310+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:24:25.301+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:24:25.513+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:24:27.253+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:24:27.252+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:24:27.440+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:24:27.439+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:24:27.751+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.523 seconds
[2022-12-16T17:24:38.066+0000] {processor.py:154} INFO - Started process (PID=4674) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:24:38.118+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:24:38.126+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:24:38.121+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:24:38.228+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:24:38.371+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:24:38.370+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:24:38.483+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:24:38.482+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:24:38.588+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.537 seconds
[2022-12-16T17:24:48.900+0000] {processor.py:154} INFO - Started process (PID=4684) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:24:48.928+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:24:48.932+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:24:48.931+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:24:49.024+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:24:49.204+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:24:49.203+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:24:49.344+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:24:49.343+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:24:49.505+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.621 seconds
[2022-12-16T17:24:59.866+0000] {processor.py:154} INFO - Started process (PID=4694) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:24:59.875+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:24:59.879+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:24:59.878+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:24:59.971+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:25:00.120+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:25:00.120+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:25:00.239+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:25:00.239+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:25:00.373+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.530 seconds
[2022-12-16T17:25:10.924+0000] {processor.py:154} INFO - Started process (PID=4711) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:25:10.928+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:25:10.933+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:25:10.932+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:25:11.045+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:25:11.669+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:25:11.668+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:25:11.903+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:25:11.902+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:25:12.107+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.199 seconds
[2022-12-16T17:25:22.551+0000] {processor.py:154} INFO - Started process (PID=4721) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:25:22.622+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:25:22.626+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:25:22.625+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:25:22.749+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:25:24.199+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:25:24.198+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:25:24.626+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:25:24.624+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:25:24.792+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.259 seconds
[2022-12-16T17:25:35.204+0000] {processor.py:154} INFO - Started process (PID=4731) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:25:35.352+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:25:35.356+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:25:35.355+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:25:35.509+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:25:37.164+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:25:37.163+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:25:37.459+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:25:37.458+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:25:37.753+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.567 seconds
[2022-12-16T17:25:48.452+0000] {processor.py:154} INFO - Started process (PID=4749) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:25:48.455+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:25:48.459+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:25:48.458+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:25:48.560+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:25:48.705+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:25:48.704+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:25:48.979+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:25:48.978+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:25:49.224+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.795 seconds
[2022-12-16T17:25:59.853+0000] {processor.py:154} INFO - Started process (PID=4759) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:25:59.904+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:25:59.908+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:25:59.907+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:26:00.020+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:26:00.236+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:26:00.235+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:26:00.386+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:26:00.385+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:26:00.549+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.721 seconds
[2022-12-16T17:26:10.758+0000] {processor.py:154} INFO - Started process (PID=4769) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:26:10.807+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:26:10.811+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:26:10.810+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:26:10.958+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:26:11.391+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:26:11.390+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:26:11.684+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:26:11.683+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:26:12.069+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.328 seconds
[2022-12-16T17:26:22.260+0000] {processor.py:154} INFO - Started process (PID=4779) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:26:22.263+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:26:22.267+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:26:22.266+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:26:22.352+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:26:22.566+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:26:22.565+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:26:22.735+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:26:22.734+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:26:22.921+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.681 seconds
[2022-12-16T17:26:33.559+0000] {processor.py:154} INFO - Started process (PID=4797) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:26:33.605+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:26:33.618+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:26:33.610+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:26:33.933+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:26:34.947+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:26:34.946+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:26:35.205+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:26:35.204+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:26:35.615+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.100 seconds
[2022-12-16T17:26:46.380+0000] {processor.py:154} INFO - Started process (PID=4807) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:26:46.398+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:26:46.402+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:26:46.401+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:26:46.638+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:26:47.072+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:26:47.071+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:26:47.257+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:26:47.255+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:26:47.444+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.112 seconds
[2022-12-16T17:26:57.855+0000] {processor.py:154} INFO - Started process (PID=4817) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:26:57.913+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:26:57.917+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:26:57.916+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:26:58.044+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:26:58.351+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:26:58.350+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:26:58.501+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:26:58.500+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:26:58.931+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.091 seconds
[2022-12-16T17:27:09.269+0000] {processor.py:154} INFO - Started process (PID=4827) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:27:09.293+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:27:09.297+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:27:09.296+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:27:09.423+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:27:10.335+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:27:10.334+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:27:10.589+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:27:10.588+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:27:10.898+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.647 seconds
[2022-12-16T17:27:21.502+0000] {processor.py:154} INFO - Started process (PID=4845) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:27:21.510+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:27:21.519+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:27:21.518+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:27:21.758+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:27:23.116+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:27:23.115+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:27:23.375+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:27:23.374+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:27:23.586+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.119 seconds
[2022-12-16T17:27:34.095+0000] {processor.py:154} INFO - Started process (PID=4855) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:27:34.149+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:27:34.152+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:27:34.152+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:27:34.391+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:27:34.621+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:27:34.614+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:27:34.843+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:27:34.842+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:27:35.072+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.017 seconds
[2022-12-16T17:27:45.425+0000] {processor.py:154} INFO - Started process (PID=4865) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:27:45.495+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:27:45.499+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:27:45.498+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:27:45.685+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:27:45.849+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:27:45.848+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:27:45.973+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:27:45.972+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:27:46.146+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.735 seconds
[2022-12-16T17:27:56.920+0000] {processor.py:154} INFO - Started process (PID=4883) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:27:56.963+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:27:56.967+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:27:56.966+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:27:57.091+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:27:57.292+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:27:57.291+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:27:57.423+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:27:57.422+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:27:57.564+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.660 seconds
[2022-12-16T17:28:08.549+0000] {processor.py:154} INFO - Started process (PID=4893) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:28:08.576+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:28:08.580+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:28:08.579+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:28:08.674+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:28:08.805+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:28:08.804+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:28:08.919+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:28:08.918+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:28:09.052+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.517 seconds
[2022-12-16T17:28:19.344+0000] {processor.py:154} INFO - Started process (PID=4903) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:28:19.368+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:28:19.375+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:28:19.374+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:28:19.497+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:28:19.680+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:28:19.677+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:28:19.817+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:28:19.816+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:28:19.951+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.622 seconds
[2022-12-16T17:28:30.271+0000] {processor.py:154} INFO - Started process (PID=4913) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:28:30.294+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:28:30.299+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:28:30.298+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:28:30.385+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:28:30.519+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:28:30.518+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:28:30.631+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:28:30.630+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:28:30.767+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.509 seconds
[2022-12-16T17:28:41.206+0000] {processor.py:154} INFO - Started process (PID=4931) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:28:41.252+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:28:41.256+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:28:41.255+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:28:41.428+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:28:41.696+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:28:41.695+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:28:41.939+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:28:41.938+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:28:42.241+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.051 seconds
[2022-12-16T17:28:52.573+0000] {processor.py:154} INFO - Started process (PID=4941) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:28:52.630+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:28:52.635+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:28:52.634+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:28:52.719+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:28:52.847+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:28:52.846+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:28:52.960+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:28:52.959+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:28:53.096+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.534 seconds
[2022-12-16T17:29:03.380+0000] {processor.py:154} INFO - Started process (PID=4951) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:29:03.401+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:29:03.406+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:29:03.405+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:29:03.491+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:29:04.351+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T17:29:14.729+0000] {processor.py:154} INFO - Started process (PID=4961) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:29:14.832+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:29:14.836+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:29:14.835+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:29:14.962+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:29:15.184+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:29:15.183+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:29:15.330+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:29:15.329+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:29:15.522+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.808 seconds
[2022-12-16T17:29:25.923+0000] {processor.py:154} INFO - Started process (PID=4978) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:29:25.974+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:29:25.982+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:29:25.977+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:29:26.416+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:29:26.771+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:29:26.770+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:29:27.070+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:29:27.068+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:29:27.265+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.437 seconds
[2022-12-16T17:29:37.777+0000] {processor.py:154} INFO - Started process (PID=4988) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:29:37.821+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:29:37.829+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:29:37.827+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:29:37.934+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:29:38.117+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:29:38.116+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:29:38.229+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:29:38.228+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:29:38.349+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.625 seconds
[2022-12-16T17:29:48.507+0000] {processor.py:154} INFO - Started process (PID=4998) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:29:48.548+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:29:48.552+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:29:48.551+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:29:48.657+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:29:50.445+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:29:50.444+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:29:50.658+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:29:50.657+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:29:50.888+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.396 seconds
[2022-12-16T17:30:01.373+0000] {processor.py:154} INFO - Started process (PID=5014) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:30:01.377+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:30:01.389+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:30:01.384+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:30:01.862+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:30:03.403+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:30:03.402+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:30:03.667+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:30:03.666+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:30:03.951+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.598 seconds
[2022-12-16T17:30:14.704+0000] {processor.py:154} INFO - Started process (PID=5025) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:30:14.732+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:30:14.741+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:30:14.740+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:30:14.989+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:30:15.835+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:30:15.834+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:30:16.029+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:30:16.028+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:30:16.150+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.486 seconds
[2022-12-16T17:30:26.623+0000] {processor.py:154} INFO - Started process (PID=5035) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:30:26.655+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:30:26.664+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:30:26.663+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:30:26.828+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:30:27.108+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:30:27.107+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:30:27.423+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:30:27.422+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:30:27.616+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.076 seconds
[2022-12-16T17:30:38.139+0000] {processor.py:154} INFO - Started process (PID=5045) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:30:38.178+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:30:38.202+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:30:38.201+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:30:38.606+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:30:39.261+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:30:39.260+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:30:39.491+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:30:39.490+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:30:39.679+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.570 seconds
[2022-12-16T17:30:50.148+0000] {processor.py:154} INFO - Started process (PID=5063) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:30:50.171+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:30:50.175+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:30:50.174+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:30:50.723+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:30:52.489+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:30:52.488+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:30:52.625+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:30:52.624+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:30:52.823+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.731 seconds
[2022-12-16T17:31:03.107+0000] {processor.py:154} INFO - Started process (PID=5073) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:31:03.129+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:31:03.136+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:31:03.133+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:31:03.220+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:31:03.363+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:31:03.363+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:31:03.483+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:31:03.482+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:31:03.616+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.524 seconds
[2022-12-16T17:31:14.082+0000] {processor.py:154} INFO - Started process (PID=5083) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:31:14.254+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:31:14.268+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:31:14.267+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:31:14.408+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:31:14.929+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:31:14.927+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:31:15.094+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:31:15.093+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:31:15.298+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.246 seconds
[2022-12-16T17:31:25.881+0000] {processor.py:154} INFO - Started process (PID=5100) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:31:25.904+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:31:25.924+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:31:25.915+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:31:26.232+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:31:27.299+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:31:27.297+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:31:27.605+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:31:27.604+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:31:28.260+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.433 seconds
[2022-12-16T17:31:39.204+0000] {processor.py:154} INFO - Started process (PID=5111) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:31:39.225+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:31:39.232+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:31:39.231+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:31:39.593+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:31:40.950+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:31:40.949+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:31:41.084+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:31:41.083+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:31:41.263+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.227 seconds
[2022-12-16T17:31:51.781+0000] {processor.py:154} INFO - Started process (PID=5121) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:31:51.813+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:31:51.817+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:31:51.816+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:31:51.967+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:31:52.567+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:31:52.566+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:31:52.709+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:31:52.708+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:31:52.911+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.174 seconds
[2022-12-16T17:32:03.094+0000] {processor.py:154} INFO - Started process (PID=5131) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:32:03.116+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:32:03.120+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:32:03.119+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:32:03.205+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:32:03.355+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:32:03.354+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:32:03.468+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:32:03.467+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:32:03.613+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.532 seconds
[2022-12-16T17:32:14.166+0000] {processor.py:154} INFO - Started process (PID=5149) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:32:14.198+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:32:14.206+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:32:14.201+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:32:14.542+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:32:15.013+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:32:15.012+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:32:15.363+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:32:15.362+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:32:15.626+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.538 seconds
[2022-12-16T17:32:26.025+0000] {processor.py:154} INFO - Started process (PID=5159) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:32:26.039+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:32:26.048+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:32:26.047+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:32:26.183+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:32:27.968+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:32:27.967+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:32:28.136+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:32:28.135+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:32:28.444+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.443 seconds
[2022-12-16T17:32:38.801+0000] {processor.py:154} INFO - Started process (PID=5169) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:32:38.840+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:32:38.845+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:32:38.844+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:32:38.963+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:32:40.205+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T17:32:50.712+0000] {processor.py:154} INFO - Started process (PID=5179) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:32:50.736+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:32:50.741+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:32:50.740+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:32:50.829+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:32:50.994+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:32:50.993+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:32:51.137+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:32:51.136+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:32:51.277+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.580 seconds
[2022-12-16T17:33:01.547+0000] {processor.py:154} INFO - Started process (PID=5197) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:33:01.595+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:33:01.603+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:33:01.602+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:33:02.018+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:33:02.716+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:33:02.715+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:33:02.941+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:33:02.940+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:33:03.187+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.684 seconds
[2022-12-16T17:33:13.555+0000] {processor.py:154} INFO - Started process (PID=5207) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:33:13.570+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:33:13.575+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:33:13.574+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:33:13.858+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:33:14.224+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:33:14.223+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:33:14.386+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:33:14.385+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:33:14.527+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.986 seconds
[2022-12-16T17:33:24.719+0000] {processor.py:154} INFO - Started process (PID=5217) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:33:24.772+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:33:24.777+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:33:24.776+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:33:24.858+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:33:25.363+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T17:33:35.874+0000] {processor.py:154} INFO - Started process (PID=5233) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:33:35.905+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:33:35.909+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:33:35.908+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:33:36.063+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:33:37.341+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T17:33:48.538+0000] {processor.py:154} INFO - Started process (PID=5244) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:33:48.568+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:33:48.572+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:33:48.571+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:33:48.697+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:33:48.855+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:33:48.854+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:33:48.987+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:33:48.986+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:33:49.254+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.739 seconds
[2022-12-16T17:33:59.794+0000] {processor.py:154} INFO - Started process (PID=5254) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:33:59.871+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:33:59.879+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:33:59.878+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:33:59.965+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:34:00.126+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:34:00.124+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:34:00.264+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:34:00.263+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:34:00.398+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.629 seconds
[2022-12-16T17:34:10.690+0000] {processor.py:154} INFO - Started process (PID=5264) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:34:10.741+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:34:10.746+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:34:10.745+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:34:10.832+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:34:11.400+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T17:34:21.939+0000] {processor.py:154} INFO - Started process (PID=5282) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:34:21.959+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:34:21.971+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:34:21.970+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:34:22.168+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:34:22.514+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:34:22.512+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:34:22.868+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:34:22.851+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:34:23.088+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.196 seconds
[2022-12-16T17:34:33.439+0000] {processor.py:154} INFO - Started process (PID=5292) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:34:33.468+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:34:33.476+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:34:33.475+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:34:33.572+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:34:33.731+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:34:33.730+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:34:33.883+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:34:33.882+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:34:34.026+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.603 seconds
[2022-12-16T17:34:44.679+0000] {processor.py:154} INFO - Started process (PID=5302) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:34:44.702+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:34:44.706+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:34:44.705+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:34:44.793+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:34:44.947+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:34:44.946+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:34:45.058+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:34:45.056+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:34:45.195+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.532 seconds
[2022-12-16T17:34:55.495+0000] {processor.py:154} INFO - Started process (PID=5312) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:34:55.517+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:34:55.521+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:34:55.520+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:34:55.638+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:34:56.187+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:34:56.186+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:34:56.300+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:34:56.299+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:34:56.442+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.961 seconds
[2022-12-16T17:35:07.061+0000] {processor.py:154} INFO - Started process (PID=5329) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:35:07.102+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:35:07.114+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:35:07.105+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:35:07.429+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:35:08.124+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:35:08.123+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:35:08.412+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:35:08.411+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:35:08.671+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.629 seconds
[2022-12-16T17:35:19.300+0000] {processor.py:154} INFO - Started process (PID=5339) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:35:19.350+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:35:19.356+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:35:19.355+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:35:19.459+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:35:20.320+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T17:35:30.795+0000] {processor.py:154} INFO - Started process (PID=5349) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:35:30.824+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:35:30.827+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:35:30.826+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:35:30.944+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:35:31.159+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:35:31.156+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:35:31.344+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:35:31.343+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:35:31.587+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.816 seconds
[2022-12-16T17:35:41.773+0000] {processor.py:154} INFO - Started process (PID=5367) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:35:41.816+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:35:41.824+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:35:41.823+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:35:41.953+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:35:42.158+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:35:42.157+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:35:42.293+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:35:42.292+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:35:42.455+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.697 seconds
[2022-12-16T17:35:52.689+0000] {processor.py:154} INFO - Started process (PID=5377) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:35:52.730+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:35:52.734+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:35:52.733+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:35:52.833+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:35:53.065+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:35:53.064+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:35:53.239+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:35:53.238+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:35:53.371+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.698 seconds
[2022-12-16T17:36:03.671+0000] {processor.py:154} INFO - Started process (PID=5387) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:36:03.724+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:36:03.729+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:36:03.728+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:36:03.814+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:36:03.950+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:36:03.949+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:36:04.086+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:36:04.085+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:36:04.211+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.554 seconds
[2022-12-16T17:36:14.598+0000] {processor.py:154} INFO - Started process (PID=5397) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:36:14.670+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:36:14.675+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:36:14.674+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:36:14.861+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:36:15.070+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:36:15.069+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:36:15.222+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:36:15.222+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:36:15.364+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.818 seconds
[2022-12-16T17:36:25.934+0000] {processor.py:154} INFO - Started process (PID=5416) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:36:25.963+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:36:25.983+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:36:25.982+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:36:26.524+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:36:26.967+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:36:26.966+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:36:27.187+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:36:27.186+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:36:27.394+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.474 seconds
[2022-12-16T17:36:38.225+0000] {processor.py:154} INFO - Started process (PID=5426) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:36:38.270+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:36:38.280+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:36:38.279+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:36:38.368+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:36:39.296+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T17:36:49.474+0000] {processor.py:154} INFO - Started process (PID=5436) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:36:49.506+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:36:49.522+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:36:49.516+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:36:49.863+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:36:50.213+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:36:50.212+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:36:50.551+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:36:50.540+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:36:50.959+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.504 seconds
[2022-12-16T17:37:01.317+0000] {processor.py:154} INFO - Started process (PID=5446) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:37:01.362+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:37:01.366+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:37:01.365+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:37:01.480+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:37:02.263+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:37:02.262+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:37:02.721+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:37:02.720+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:37:02.971+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.675 seconds
[2022-12-16T17:37:13.496+0000] {processor.py:154} INFO - Started process (PID=5463) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:37:13.532+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:37:13.536+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:37:13.535+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:37:13.714+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:37:14.661+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:37:14.660+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:37:14.894+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:37:14.893+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:37:15.139+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.667 seconds
[2022-12-16T17:37:25.488+0000] {processor.py:154} INFO - Started process (PID=5473) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:37:25.515+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:37:25.522+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:37:25.521+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:37:25.766+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:37:26.786+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T17:37:37.078+0000] {processor.py:154} INFO - Started process (PID=5483) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:37:37.108+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:37:37.113+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:37:37.112+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:37:37.195+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:37:38.282+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:37:38.281+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:37:38.432+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:37:38.431+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:37:38.546+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.483 seconds
[2022-12-16T17:37:48.885+0000] {processor.py:154} INFO - Started process (PID=5501) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:37:48.946+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:37:48.954+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:37:48.949+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:37:49.085+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:37:49.952+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:37:49.951+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:37:50.190+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:37:50.189+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:37:50.304+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.434 seconds
[2022-12-16T17:38:00.431+0000] {processor.py:154} INFO - Started process (PID=5511) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:38:00.454+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:38:00.459+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:38:00.458+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:38:00.550+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:38:00.967+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:38:00.966+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:38:01.160+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:38:01.159+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:38:01.457+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.041 seconds
[2022-12-16T17:38:11.825+0000] {processor.py:154} INFO - Started process (PID=5521) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:38:11.864+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:38:11.869+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:38:11.868+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:38:11.952+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:38:12.491+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T17:38:22.916+0000] {processor.py:154} INFO - Started process (PID=5531) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:38:22.942+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:38:22.946+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:38:22.944+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:38:23.036+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:38:23.673+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:38:23.670+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:38:23.971+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:38:23.970+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:38:24.257+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.363 seconds
[2022-12-16T17:38:34.785+0000] {processor.py:154} INFO - Started process (PID=5550) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:38:34.808+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:38:34.813+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:38:34.812+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:38:34.896+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:38:35.031+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:38:35.030+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:38:35.142+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:38:35.141+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:38:35.276+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.505 seconds
[2022-12-16T17:38:45.576+0000] {processor.py:154} INFO - Started process (PID=5560) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:38:45.612+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:38:45.617+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:38:45.616+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:38:45.701+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:38:45.853+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:38:45.852+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:38:45.979+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:38:45.978+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:38:46.083+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.522 seconds
[2022-12-16T17:38:56.486+0000] {processor.py:154} INFO - Started process (PID=5570) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:38:56.507+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:38:56.519+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:38:56.518+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:38:56.654+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:38:56.835+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:38:56.832+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:38:57.050+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:38:57.049+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:38:57.247+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.782 seconds
[2022-12-16T17:39:07.961+0000] {processor.py:154} INFO - Started process (PID=5589) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:39:07.986+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:39:07.993+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:39:07.992+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:39:08.251+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:39:08.402+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:39:08.401+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:39:08.532+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:39:08.531+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:39:08.673+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.773 seconds
[2022-12-16T17:39:19.086+0000] {processor.py:154} INFO - Started process (PID=5599) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:39:19.090+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:39:19.094+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:39:19.093+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:39:19.183+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:39:19.326+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:39:19.324+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:39:19.445+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:39:19.444+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:39:19.610+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.537 seconds
[2022-12-16T17:39:29.922+0000] {processor.py:154} INFO - Started process (PID=5609) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:39:29.970+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:39:29.975+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:39:29.974+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:39:30.069+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:39:30.331+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:39:30.330+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:39:30.475+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:39:30.474+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:39:30.644+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.737 seconds
[2022-12-16T17:39:41.015+0000] {processor.py:154} INFO - Started process (PID=5619) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:39:41.029+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:39:41.042+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:39:41.033+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:39:41.238+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:39:41.416+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:39:41.415+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:39:41.557+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:39:41.556+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:39:41.827+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.888 seconds
[2022-12-16T17:39:52.489+0000] {processor.py:154} INFO - Started process (PID=5638) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:39:52.507+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:39:52.511+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:39:52.510+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:39:52.982+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:39:54.531+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T17:40:05.586+0000] {processor.py:154} INFO - Started process (PID=5648) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:40:05.600+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:40:05.605+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:40:05.604+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:40:06.028+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:40:06.291+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:40:06.289+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:40:06.565+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:40:06.564+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:40:06.792+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.254 seconds
[2022-12-16T17:40:17.254+0000] {processor.py:154} INFO - Started process (PID=5658) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:40:17.288+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:40:17.307+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:40:17.296+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:40:17.464+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:40:17.626+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:40:17.625+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:40:17.734+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:40:17.733+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:40:17.848+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.632 seconds
[2022-12-16T17:40:28.163+0000] {processor.py:154} INFO - Started process (PID=5668) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:40:28.193+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:40:28.197+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:40:28.196+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:40:28.286+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:40:28.424+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:40:28.423+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:40:28.536+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:40:28.535+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:40:29.115+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.965 seconds
[2022-12-16T17:40:40.166+0000] {processor.py:154} INFO - Started process (PID=5686) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:40:40.173+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:40:40.180+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:40:40.179+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:40:40.295+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:40:40.565+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:40:40.564+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:40:40.906+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:40:40.905+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:40:41.347+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.252 seconds
[2022-12-16T17:40:51.890+0000] {processor.py:154} INFO - Started process (PID=5696) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:40:51.903+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:40:51.908+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:40:51.907+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:40:52.011+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:40:52.210+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:40:52.208+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:40:52.441+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:40:52.440+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:40:52.806+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.934 seconds
[2022-12-16T17:41:03.282+0000] {processor.py:154} INFO - Started process (PID=5706) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:41:03.304+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:41:03.311+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:41:03.310+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:41:03.405+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:41:03.560+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:41:03.559+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:41:03.714+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:41:03.713+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:41:03.874+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.607 seconds
[2022-12-16T17:41:14.255+0000] {processor.py:154} INFO - Started process (PID=5716) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:41:14.295+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:41:14.300+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:41:14.299+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:41:14.449+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:41:15.478+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T17:41:26.731+0000] {processor.py:154} INFO - Started process (PID=5734) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:41:26.780+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:41:26.787+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:41:26.786+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:41:26.878+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:41:27.413+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:41:27.412+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:41:27.644+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:41:27.643+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:41:27.995+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.283 seconds
[2022-12-16T17:41:38.314+0000] {processor.py:154} INFO - Started process (PID=5744) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:41:38.373+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:41:38.377+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:41:38.376+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:41:38.458+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:41:38.618+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:41:38.616+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:41:38.731+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:41:38.731+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:41:38.833+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.534 seconds
[2022-12-16T17:41:49.110+0000] {processor.py:154} INFO - Started process (PID=5754) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:41:49.133+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:41:49.138+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:41:49.136+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:41:49.220+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:41:49.530+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:41:49.529+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:41:49.643+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:41:49.642+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:41:49.779+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.683 seconds
[2022-12-16T17:42:00.172+0000] {processor.py:154} INFO - Started process (PID=5772) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:42:00.212+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:42:00.221+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:42:00.220+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:42:00.347+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:42:00.566+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:42:00.565+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:42:00.685+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:42:00.684+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:42:00.807+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.650 seconds
[2022-12-16T17:42:11.201+0000] {processor.py:154} INFO - Started process (PID=5782) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:42:11.250+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:42:11.254+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:42:11.253+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:42:11.334+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:42:11.466+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:42:11.465+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:42:11.587+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:42:11.586+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:42:11.721+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.533 seconds
[2022-12-16T17:42:22.006+0000] {processor.py:154} INFO - Started process (PID=5792) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:42:22.031+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:42:22.035+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:42:22.034+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:42:22.134+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:42:22.280+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:42:22.279+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:42:22.465+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:42:22.464+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:42:22.591+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.599 seconds
[2022-12-16T17:42:33.588+0000] {processor.py:154} INFO - Started process (PID=5802) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:42:33.643+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:42:33.647+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:42:33.646+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:42:33.736+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:42:34.303+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:42:34.302+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:42:34.414+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:42:34.414+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:42:34.539+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.969 seconds
[2022-12-16T17:42:45.068+0000] {processor.py:154} INFO - Started process (PID=5820) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:42:45.299+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:42:45.310+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:42:45.302+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:42:45.505+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:42:46.950+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T17:42:57.372+0000] {processor.py:154} INFO - Started process (PID=5830) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:42:57.387+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:42:57.399+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:42:57.398+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:42:57.530+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:42:57.797+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:42:57.794+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:42:57.956+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:42:57.955+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:42:58.075+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.718 seconds
[2022-12-16T17:43:08.452+0000] {processor.py:154} INFO - Started process (PID=5840) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:43:08.479+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:43:08.483+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:43:08.482+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:43:08.574+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:43:08.808+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:43:08.807+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:43:08.973+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:43:08.972+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:43:09.129+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.692 seconds
[2022-12-16T17:43:12.304+0000] {processor.py:154} INFO - Started process (PID=5853) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:43:12.311+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:43:12.319+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:43:12.317+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:43:12.467+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:43:13.069+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:43:13.068+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:43:14.054+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:43:14.054+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:43:14.299+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.012 seconds
[2022-12-16T17:43:24.860+0000] {processor.py:154} INFO - Started process (PID=5871) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:43:24.897+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:43:24.901+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:43:24.900+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:43:25.332+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:43:25.944+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:43:25.943+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:43:26.146+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:43:26.144+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:43:26.280+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.456 seconds
[2022-12-16T17:43:36.519+0000] {processor.py:154} INFO - Started process (PID=5881) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:43:36.543+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:43:36.549+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:43:36.548+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:43:36.718+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:43:37.289+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:43:37.288+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:43:37.802+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:43:37.801+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:43:38.079+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.576 seconds
[2022-12-16T17:43:48.762+0000] {processor.py:154} INFO - Started process (PID=5891) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:43:48.766+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:43:48.771+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:43:48.771+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:43:49.023+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:43:49.541+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:43:49.540+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:43:49.675+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:43:49.674+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:43:49.834+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.136 seconds
[2022-12-16T17:44:00.431+0000] {processor.py:154} INFO - Started process (PID=5905) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:44:00.491+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:44:00.511+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:44:00.510+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:44:01.118+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:44:01.862+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:44:01.860+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:44:02.338+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:44:02.325+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:44:02.895+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.539 seconds
[2022-12-16T17:44:13.484+0000] {processor.py:154} INFO - Started process (PID=5919) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:44:13.487+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:44:13.492+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:44:13.491+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:44:13.824+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:44:14.208+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:44:14.199+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:44:14.400+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:44:14.399+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:44:14.580+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.119 seconds
[2022-12-16T17:44:25.094+0000] {processor.py:154} INFO - Started process (PID=5926) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:44:25.124+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:44:25.128+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:44:25.127+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:44:25.226+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:44:25.413+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:44:25.412+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:44:25.570+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:44:25.570+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:44:25.784+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.722 seconds
[2022-12-16T17:44:36.742+0000] {processor.py:154} INFO - Started process (PID=5936) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:44:36.781+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:44:36.786+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:44:36.785+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:44:36.877+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:44:37.360+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T17:44:48.172+0000] {processor.py:154} INFO - Started process (PID=5956) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:44:48.210+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:44:48.226+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:44:48.213+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:44:48.549+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:44:50.318+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:44:50.317+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:44:50.561+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:44:50.560+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:44:50.876+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.822 seconds
[2022-12-16T17:45:01.459+0000] {processor.py:154} INFO - Started process (PID=5968) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:45:01.703+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:45:01.710+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:45:01.709+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:45:01.913+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:45:02.169+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:45:02.168+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:45:02.346+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:45:02.344+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:45:02.535+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.095 seconds
[2022-12-16T17:45:12.710+0000] {processor.py:154} INFO - Started process (PID=5978) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:45:12.714+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:45:12.718+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:45:12.717+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:45:12.824+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:45:13.014+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:45:13.013+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:45:13.163+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:45:13.162+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:45:13.284+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.589 seconds
[2022-12-16T17:45:23.710+0000] {processor.py:154} INFO - Started process (PID=5986) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:45:23.761+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:45:23.765+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:45:23.764+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:45:24.327+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:45:24.975+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T17:45:35.855+0000] {processor.py:154} INFO - Started process (PID=6004) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:45:35.911+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:45:35.923+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:45:35.922+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:45:36.483+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:45:37.520+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:45:37.519+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:45:38.070+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:45:38.062+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:45:38.443+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.653 seconds
[2022-12-16T17:45:49.050+0000] {processor.py:154} INFO - Started process (PID=6014) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:45:49.061+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:45:49.066+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:45:49.065+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:45:49.188+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:45:49.569+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:45:49.568+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:45:49.768+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:45:49.765+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:45:49.995+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.963 seconds
[2022-12-16T17:46:00.719+0000] {processor.py:154} INFO - Started process (PID=6026) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:46:00.734+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:46:00.738+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:46:00.737+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:46:00.963+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:46:01.254+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:46:01.252+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:46:01.592+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:46:01.591+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:46:02.038+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.374 seconds
[2022-12-16T17:46:12.305+0000] {processor.py:154} INFO - Started process (PID=6036) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:46:12.327+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:46:12.332+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:46:12.331+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:46:12.476+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:46:13.245+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T17:46:24.038+0000] {processor.py:154} INFO - Started process (PID=6051) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:46:24.060+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:46:24.072+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:46:24.069+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:46:24.322+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:46:24.812+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:46:24.811+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:46:25.345+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:46:25.344+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:46:25.776+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.774 seconds
[2022-12-16T17:46:36.382+0000] {processor.py:154} INFO - Started process (PID=6063) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:46:36.385+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:46:36.389+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:46:36.388+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:46:36.559+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:46:37.281+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:46:37.280+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:46:37.591+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:46:37.590+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:46:37.741+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.373 seconds
[2022-12-16T17:46:48.146+0000] {processor.py:154} INFO - Started process (PID=6073) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:46:48.155+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:46:48.159+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:46:48.158+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:46:48.312+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:46:48.479+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:46:48.478+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:46:48.637+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:46:48.630+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:46:49.089+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.969 seconds
[2022-12-16T17:46:59.748+0000] {processor.py:154} INFO - Started process (PID=6088) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:46:59.771+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:46:59.787+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:46:59.786+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:47:00.009+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:47:00.564+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:47:00.564+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:47:00.875+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:47:00.875+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:47:01.153+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.446 seconds
[2022-12-16T17:47:11.631+0000] {processor.py:154} INFO - Started process (PID=6100) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:47:11.639+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:47:11.648+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:47:11.647+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:47:11.861+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:47:14.028+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:47:14.027+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:47:14.445+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:47:14.444+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:47:14.736+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.139 seconds
[2022-12-16T17:47:25.278+0000] {processor.py:154} INFO - Started process (PID=6110) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:47:25.299+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:47:25.303+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:47:25.302+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:47:25.401+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:47:25.590+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:47:25.589+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:47:25.736+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:47:25.735+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:47:25.924+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.677 seconds
[2022-12-16T17:47:36.310+0000] {processor.py:154} INFO - Started process (PID=6120) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:47:36.325+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:47:36.330+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:47:36.328+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:47:36.480+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:47:37.041+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:47:37.040+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:47:37.568+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:47:37.567+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:47:37.779+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.509 seconds
[2022-12-16T17:47:48.124+0000] {processor.py:154} INFO - Started process (PID=6139) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:47:48.143+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:47:48.147+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:47:48.146+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:47:48.447+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:47:49.028+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:47:49.027+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:47:49.468+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:47:49.467+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:47:49.699+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.615 seconds
[2022-12-16T17:48:00.247+0000] {processor.py:154} INFO - Started process (PID=6149) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:48:00.291+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:48:00.296+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:48:00.295+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:48:00.536+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:48:00.773+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:48:00.772+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:48:00.960+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:48:00.959+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:48:01.153+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.919 seconds
[2022-12-16T17:48:11.635+0000] {processor.py:154} INFO - Started process (PID=6159) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:48:11.671+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:48:11.675+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:48:11.674+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:48:11.799+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:48:12.038+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:48:12.032+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:48:12.284+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:48:12.283+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:48:12.482+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.866 seconds
[2022-12-16T17:48:22.967+0000] {processor.py:154} INFO - Started process (PID=6169) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:48:23.017+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:48:23.022+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:48:23.020+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:48:23.168+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:48:23.336+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:48:23.335+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:48:23.632+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:48:23.631+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:48:24.159+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.206 seconds
[2022-12-16T17:48:34.648+0000] {processor.py:154} INFO - Started process (PID=6187) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:48:34.656+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:48:34.665+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:48:34.664+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:48:35.094+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:48:35.861+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:48:35.857+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:48:36.627+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:48:36.626+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:48:37.011+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.447 seconds
[2022-12-16T17:48:47.544+0000] {processor.py:154} INFO - Started process (PID=6197) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:48:47.547+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:48:47.551+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:48:47.550+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:48:47.685+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:48:48.533+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:48:48.515+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:48:48.782+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:48:48.781+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:48:49.032+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.521 seconds
[2022-12-16T17:48:59.925+0000] {processor.py:154} INFO - Started process (PID=6207) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:48:59.945+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:48:59.954+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:48:59.953+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:49:00.066+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:49:01.002+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:49:01.001+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:49:01.141+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:49:01.140+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:49:01.252+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.354 seconds
[2022-12-16T17:49:11.622+0000] {processor.py:154} INFO - Started process (PID=6217) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:49:11.635+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:49:11.644+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:49:11.643+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:49:11.818+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:49:12.816+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T17:49:23.626+0000] {processor.py:154} INFO - Started process (PID=6235) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:49:23.629+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:49:23.641+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:49:23.632+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:49:23.783+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:49:23.953+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:49:23.951+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:49:24.169+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:49:24.168+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:49:24.396+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.804 seconds
[2022-12-16T17:49:35.021+0000] {processor.py:154} INFO - Started process (PID=6245) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:49:35.048+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:49:35.056+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:49:35.055+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:49:35.188+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:49:35.575+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:49:35.570+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:49:35.799+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:49:35.798+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:49:35.901+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.906 seconds
[2022-12-16T17:49:46.077+0000] {processor.py:154} INFO - Started process (PID=6255) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:49:46.129+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:49:46.133+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:49:46.132+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:49:46.261+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:49:46.950+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:49:46.949+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:49:47.145+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:49:47.144+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:49:47.295+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.233 seconds
[2022-12-16T17:49:57.844+0000] {processor.py:154} INFO - Started process (PID=6271) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:49:57.857+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:49:57.873+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:49:57.872+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:49:58.067+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:49:58.411+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:49:58.410+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:49:58.715+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:49:58.714+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:49:59.133+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.341 seconds
[2022-12-16T17:50:09.870+0000] {processor.py:154} INFO - Started process (PID=6282) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:50:09.877+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:50:09.895+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:50:09.881+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:50:10.162+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:50:10.473+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:50:10.472+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:50:10.667+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:50:10.666+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:50:11.131+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.339 seconds
[2022-12-16T17:50:21.854+0000] {processor.py:154} INFO - Started process (PID=6292) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:50:21.883+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:50:21.887+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:50:21.886+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:50:22.098+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:50:22.288+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:50:22.287+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:50:22.421+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:50:22.420+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:50:22.636+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.859 seconds
[2022-12-16T17:50:32.964+0000] {processor.py:154} INFO - Started process (PID=6302) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:50:32.987+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:50:32.991+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:50:32.990+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:50:33.198+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:50:33.517+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:50:33.516+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:50:33.865+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:50:33.864+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:50:34.096+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.164 seconds
[2022-12-16T17:50:44.652+0000] {processor.py:154} INFO - Started process (PID=6319) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:50:44.659+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:50:44.663+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:50:44.662+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:50:44.808+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:50:45.465+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:50:45.464+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:50:45.761+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:50:45.760+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:50:46.127+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.523 seconds
[2022-12-16T17:50:56.350+0000] {processor.py:154} INFO - Started process (PID=6330) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:50:56.403+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:50:56.411+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:50:56.410+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:50:56.604+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:50:56.763+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:50:56.762+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:50:56.893+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:50:56.892+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:50:57.085+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.750 seconds
[2022-12-16T17:51:07.434+0000] {processor.py:154} INFO - Started process (PID=6340) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:51:07.482+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:51:07.487+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:51:07.485+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:51:07.656+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:51:08.048+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:51:08.047+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:51:08.254+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:51:08.245+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:51:08.637+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.224 seconds
[2022-12-16T17:51:19.093+0000] {processor.py:154} INFO - Started process (PID=6350) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:51:19.098+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:51:19.102+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:51:19.101+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:51:19.213+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:51:19.363+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:51:19.362+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:51:19.505+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:51:19.504+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:51:19.698+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.624 seconds
[2022-12-16T17:51:30.215+0000] {processor.py:154} INFO - Started process (PID=6368) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:51:30.236+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:51:30.245+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:51:30.244+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:51:30.451+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:51:31.512+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:51:31.511+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:51:31.807+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:51:31.806+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:51:32.050+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.870 seconds
[2022-12-16T17:51:42.623+0000] {processor.py:154} INFO - Started process (PID=6378) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:51:42.668+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:51:42.672+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:51:42.671+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:51:42.764+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:51:42.989+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:51:42.988+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:51:43.211+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:51:43.210+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:51:43.359+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.750 seconds
[2022-12-16T17:51:53.671+0000] {processor.py:154} INFO - Started process (PID=6388) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:51:53.683+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:51:53.687+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:51:53.686+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:51:54.002+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:51:54.561+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:51:54.559+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:51:54.755+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:51:54.754+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:51:54.941+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.318 seconds
[2022-12-16T17:52:05.264+0000] {processor.py:154} INFO - Started process (PID=6398) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:52:05.294+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:52:05.299+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:52:05.298+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:52:05.392+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:52:06.060+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:52:06.058+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:52:06.202+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:52:06.201+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:52:06.379+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.129 seconds
[2022-12-16T17:52:16.995+0000] {processor.py:154} INFO - Started process (PID=6416) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:52:17.023+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:52:17.027+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:52:17.026+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:52:17.241+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:52:17.768+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:52:17.767+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:52:17.978+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:52:17.977+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:52:18.222+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.270 seconds
[2022-12-16T17:52:28.904+0000] {processor.py:154} INFO - Started process (PID=6426) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:52:28.979+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:52:28.987+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:52:28.982+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:52:29.344+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:52:29.978+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:52:29.977+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:52:30.102+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:52:30.101+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:52:30.217+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.330 seconds
[2022-12-16T17:52:40.557+0000] {processor.py:154} INFO - Started process (PID=6436) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:52:40.705+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:52:40.729+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:52:40.720+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:52:40.865+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:52:41.103+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:52:41.102+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:52:41.227+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:52:41.226+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:52:41.362+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.849 seconds
[2022-12-16T17:52:51.824+0000] {processor.py:154} INFO - Started process (PID=6446) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:52:51.874+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:52:51.879+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:52:51.877+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:52:52.042+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:52:52.331+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:52:52.330+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:52:53.004+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:52:53.003+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:52:53.249+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.451 seconds
[2022-12-16T17:53:03.996+0000] {processor.py:154} INFO - Started process (PID=6463) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:53:04.010+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:53:04.028+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:53:04.027+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:53:04.235+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:53:05.591+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T17:53:16.555+0000] {processor.py:154} INFO - Started process (PID=6473) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:53:16.576+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:53:16.581+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:53:16.580+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:53:16.738+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:53:16.974+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:53:16.973+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:53:17.413+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:53:17.412+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:53:17.738+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.198 seconds
[2022-12-16T17:53:28.257+0000] {processor.py:154} INFO - Started process (PID=6483) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:53:28.264+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:53:28.268+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:53:28.267+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:53:28.405+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:53:29.044+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:53:29.043+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:53:29.192+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:53:29.191+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:53:29.459+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.223 seconds
[2022-12-16T17:53:39.808+0000] {processor.py:154} INFO - Started process (PID=6499) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:53:39.869+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:53:39.883+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:53:39.882+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:53:40.211+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:53:40.556+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:53:40.555+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:53:40.909+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:53:40.908+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:53:41.319+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.581 seconds
[2022-12-16T17:53:52.482+0000] {processor.py:154} INFO - Started process (PID=6510) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:53:52.498+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:53:52.502+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:53:52.500+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:53:52.731+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:53:53.957+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T17:54:04.931+0000] {processor.py:154} INFO - Started process (PID=6520) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:54:04.984+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:54:04.989+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:54:04.988+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:54:05.129+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:54:05.467+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:54:05.466+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:54:05.802+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:54:05.801+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:54:06.071+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.165 seconds
[2022-12-16T17:54:16.229+0000] {processor.py:154} INFO - Started process (PID=6530) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:54:16.253+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:54:16.260+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:54:16.259+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:54:16.374+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:54:16.570+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:54:16.569+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:54:16.688+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:54:16.688+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:54:16.833+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.628 seconds
[2022-12-16T17:54:27.304+0000] {processor.py:154} INFO - Started process (PID=6547) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:54:27.325+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:54:27.333+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:54:27.332+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:54:27.452+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:54:27.767+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:54:27.766+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:54:27.908+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:54:27.907+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:54:28.211+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.934 seconds
[2022-12-16T17:54:38.781+0000] {processor.py:154} INFO - Started process (PID=6557) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:54:38.826+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:54:38.836+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:54:38.834+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:54:39.147+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:54:39.506+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:54:39.501+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:54:39.829+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:54:39.828+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:54:40.088+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.325 seconds
[2022-12-16T17:54:51.062+0000] {processor.py:154} INFO - Started process (PID=6567) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:54:51.103+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:54:51.107+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:54:51.106+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:54:51.191+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:54:52.663+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:54:52.662+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:54:52.777+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:54:52.776+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:54:52.882+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.835 seconds
[2022-12-16T17:55:03.130+0000] {processor.py:154} INFO - Started process (PID=6577) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:55:03.157+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:55:03.161+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:55:03.160+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:55:03.247+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:55:03.376+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:55:03.375+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:55:03.497+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:55:03.496+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:55:03.639+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.524 seconds
[2022-12-16T17:55:14.078+0000] {processor.py:154} INFO - Started process (PID=6595) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:55:14.127+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:55:14.135+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:55:14.130+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:55:14.311+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:55:14.570+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:55:14.569+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:55:14.819+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:55:14.808+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:55:15.196+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.161 seconds
[2022-12-16T17:55:25.635+0000] {processor.py:154} INFO - Started process (PID=6605) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:55:25.672+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:55:25.676+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:55:25.675+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:55:25.847+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:55:26.040+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:55:26.038+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:55:26.214+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:55:26.212+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:55:26.631+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.009 seconds
[2022-12-16T17:55:37.195+0000] {processor.py:154} INFO - Started process (PID=6615) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:55:37.317+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:55:37.321+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:55:37.320+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:55:37.428+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:55:37.724+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:55:37.723+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:55:38.072+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:55:38.072+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:55:38.363+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.194 seconds
[2022-12-16T17:55:50.089+0000] {processor.py:154} INFO - Started process (PID=6631) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:55:50.195+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:55:50.220+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:55:50.219+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:55:51.318+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:55:52.088+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:55:52.087+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:55:52.539+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:55:52.538+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:55:53.232+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.378 seconds
[2022-12-16T17:56:04.408+0000] {processor.py:154} INFO - Started process (PID=6643) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:56:04.441+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:56:04.452+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:56:04.451+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:56:04.902+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:56:06.195+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:56:06.194+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:56:06.753+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:56:06.752+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:56:07.319+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.053 seconds
[2022-12-16T17:56:18.226+0000] {processor.py:154} INFO - Started process (PID=6653) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:56:18.351+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:56:18.383+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:56:18.382+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:56:18.705+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:56:19.175+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:56:19.174+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:56:19.525+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:56:19.524+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:56:19.846+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.698 seconds
[2022-12-16T17:56:30.532+0000] {processor.py:154} INFO - Started process (PID=6663) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:56:30.566+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:56:30.572+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:56:30.571+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:56:30.701+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:56:30.862+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:56:30.861+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:56:30.997+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:56:30.995+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:56:31.166+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.663 seconds
[2022-12-16T17:56:41.811+0000] {processor.py:154} INFO - Started process (PID=6673) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:56:41.831+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:56:41.835+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:56:41.834+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:56:42.061+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:56:42.702+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:56:42.701+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:56:42.830+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:56:42.829+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:56:42.966+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.198 seconds
[2022-12-16T17:56:53.464+0000] {processor.py:154} INFO - Started process (PID=6691) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:56:53.527+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:56:53.539+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:56:53.538+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:56:53.766+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:56:53.932+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:56:53.931+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:56:54.097+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:56:54.096+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:56:54.223+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.811 seconds
[2022-12-16T17:57:04.651+0000] {processor.py:154} INFO - Started process (PID=6701) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:57:04.690+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:57:04.693+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:57:04.692+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:57:04.909+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:57:05.214+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:57:05.200+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:57:05.488+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:57:05.487+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:57:05.670+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.039 seconds
[2022-12-16T17:57:16.017+0000] {processor.py:154} INFO - Started process (PID=6711) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:57:16.048+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:57:16.053+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:57:16.052+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:57:16.136+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:57:16.288+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:57:16.287+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:57:16.492+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:57:16.491+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:57:16.743+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.739 seconds
[2022-12-16T17:57:27.122+0000] {processor.py:154} INFO - Started process (PID=6721) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:57:27.171+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:57:27.182+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:57:27.181+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:57:27.297+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:57:27.452+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:57:27.451+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:57:27.579+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:57:27.578+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:57:27.717+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.630 seconds
[2022-12-16T17:57:38.226+0000] {processor.py:154} INFO - Started process (PID=6739) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:57:38.274+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:57:38.286+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:57:38.276+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:57:38.520+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:57:38.859+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:57:38.858+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:57:38.982+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:57:38.981+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:57:39.144+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.939 seconds
[2022-12-16T17:57:49.462+0000] {processor.py:154} INFO - Started process (PID=6749) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:57:49.509+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:57:49.513+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:57:49.512+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:57:49.601+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:57:50.666+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:57:50.665+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:57:50.784+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:57:50.783+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:57:50.913+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.465 seconds
[2022-12-16T17:58:01.652+0000] {processor.py:154} INFO - Started process (PID=6759) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:58:01.675+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:58:01.680+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:58:01.679+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:58:01.763+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:58:02.276+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:58:02.275+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:58:02.419+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:58:02.417+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:58:02.525+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.887 seconds
[2022-12-16T17:58:12.823+0000] {processor.py:154} INFO - Started process (PID=6776) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:58:12.864+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:58:12.869+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:58:12.868+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:58:13.029+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:58:13.756+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T17:58:24.258+0000] {processor.py:154} INFO - Started process (PID=6786) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:58:24.313+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:58:24.318+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:58:24.317+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:58:24.422+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:58:25.119+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:58:25.118+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:58:25.243+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:58:25.242+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:58:25.380+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.141 seconds
[2022-12-16T17:58:35.633+0000] {processor.py:154} INFO - Started process (PID=6796) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:58:35.682+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:58:35.686+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:58:35.685+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:58:35.768+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:58:36.328+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:58:36.327+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:58:36.439+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:58:36.439+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:58:36.599+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.980 seconds
[2022-12-16T17:58:46.969+0000] {processor.py:154} INFO - Started process (PID=6806) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:58:46.995+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:58:47.001+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:58:46.999+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:58:47.134+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:58:47.484+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:58:47.483+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:58:47.651+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:58:47.650+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:58:47.766+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.809 seconds
[2022-12-16T17:58:58.077+0000] {processor.py:154} INFO - Started process (PID=6823) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:58:58.151+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:58:58.159+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:58:58.158+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:58:58.398+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:58:59.957+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:58:59.956+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:59:00.143+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:59:00.142+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:59:00.304+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.249 seconds
[2022-12-16T17:59:10.613+0000] {processor.py:154} INFO - Started process (PID=6833) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:59:10.643+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:59:10.652+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:59:10.651+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:59:10.747+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:59:11.238+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:59:11.237+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:59:11.354+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:59:11.353+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:59:11.489+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.893 seconds
[2022-12-16T17:59:21.795+0000] {processor.py:154} INFO - Started process (PID=6843) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:59:21.841+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:59:21.846+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:59:21.844+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:59:21.930+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:59:22.072+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:59:22.071+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:59:22.185+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:59:22.184+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:59:22.330+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.548 seconds
[2022-12-16T17:59:32.720+0000] {processor.py:154} INFO - Started process (PID=6861) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:59:32.766+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:59:32.778+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:59:32.777+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:59:32.956+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:59:33.859+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T17:59:44.122+0000] {processor.py:154} INFO - Started process (PID=6871) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:59:44.166+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:59:44.170+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:59:44.169+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:59:44.259+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:59:44.437+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:59:44.436+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:59:44.549+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:59:44.548+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:59:44.684+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.580 seconds
[2022-12-16T17:59:54.993+0000] {processor.py:154} INFO - Started process (PID=6881) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:59:55.003+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T17:59:55.008+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:59:55.007+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:59:55.092+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T17:59:55.229+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:59:55.228+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T17:59:55.369+0000] {logging_mixin.py:137} INFO - [2022-12-16T17:59:55.368+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T17:59:55.477+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.498 seconds
[2022-12-16T18:00:05.750+0000] {processor.py:154} INFO - Started process (PID=6891) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:00:05.796+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:00:05.801+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:00:05.800+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:00:05.904+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:00:06.051+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:00:06.050+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:00:06.164+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:00:06.163+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:00:06.305+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.569 seconds
[2022-12-16T18:00:16.884+0000] {processor.py:154} INFO - Started process (PID=6909) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:00:16.934+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:00:16.953+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:00:16.944+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:00:17.309+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:00:17.811+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T18:00:28.356+0000] {processor.py:154} INFO - Started process (PID=6919) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:00:28.409+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:00:28.413+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:00:28.412+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:00:28.563+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:00:29.299+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:00:29.298+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:00:29.423+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:00:29.422+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:00:29.582+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.240 seconds
[2022-12-16T18:00:39.879+0000] {processor.py:154} INFO - Started process (PID=6929) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:00:39.921+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:00:39.926+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:00:39.925+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:00:40.066+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:00:40.805+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:00:40.804+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:00:40.921+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:00:40.920+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:00:41.038+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.174 seconds
[2022-12-16T18:00:51.391+0000] {processor.py:154} INFO - Started process (PID=6946) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:00:51.422+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:00:51.430+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:00:51.425+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:00:51.772+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:00:52.136+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:00:52.135+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:00:52.291+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:00:52.290+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:00:52.412+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.045 seconds
[2022-12-16T18:01:02.869+0000] {processor.py:154} INFO - Started process (PID=6957) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:01:02.912+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:01:02.918+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:01:02.916+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:01:03.011+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:01:03.472+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:01:03.471+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:01:03.636+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:01:03.635+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:01:03.781+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.926 seconds
[2022-12-16T18:01:13.968+0000] {processor.py:154} INFO - Started process (PID=6967) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:01:13.983+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:01:13.988+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:01:13.986+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:01:14.072+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:01:15.050+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T18:01:25.428+0000] {processor.py:154} INFO - Started process (PID=6977) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:01:25.477+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:01:25.482+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:01:25.481+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:01:25.564+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:01:26.141+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:01:26.140+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:01:26.424+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:01:26.423+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:01:26.715+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.300 seconds
[2022-12-16T18:01:37.178+0000] {processor.py:154} INFO - Started process (PID=6994) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:01:37.199+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:01:37.203+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:01:37.202+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:01:37.517+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:01:38.505+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:01:38.504+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:01:38.923+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:01:38.922+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:01:39.120+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.976 seconds
[2022-12-16T18:01:49.473+0000] {processor.py:154} INFO - Started process (PID=7004) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:01:49.520+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:01:49.524+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:01:49.523+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:01:49.611+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:01:49.747+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:01:49.746+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:01:49.864+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:01:49.864+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:01:50.031+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.571 seconds
[2022-12-16T18:02:00.239+0000] {processor.py:154} INFO - Started process (PID=7014) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:02:00.323+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:02:00.328+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:02:00.327+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:02:00.425+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:02:00.605+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:02:00.604+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:02:00.749+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:02:00.748+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:02:00.917+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.692 seconds
[2022-12-16T18:02:11.210+0000] {processor.py:154} INFO - Started process (PID=7024) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:02:11.239+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:02:11.243+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:02:11.242+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:02:11.326+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:02:11.639+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:02:11.638+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:02:11.753+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:02:11.752+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:02:11.890+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.698 seconds
[2022-12-16T18:02:22.379+0000] {processor.py:154} INFO - Started process (PID=7041) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:02:22.422+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:02:22.427+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:02:22.426+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:02:22.540+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:02:23.052+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:02:23.051+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:02:23.177+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:02:23.176+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:02:23.302+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.948 seconds
[2022-12-16T18:02:33.618+0000] {processor.py:154} INFO - Started process (PID=7051) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:02:33.643+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:02:33.647+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:02:33.647+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:02:33.741+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:02:34.005+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:02:34.004+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:02:34.147+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:02:34.146+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:02:34.277+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.674 seconds
[2022-12-16T18:02:44.622+0000] {processor.py:154} INFO - Started process (PID=7061) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:02:44.647+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:02:44.651+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:02:44.650+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:02:44.742+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:02:45.175+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:02:45.174+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:02:45.296+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:02:45.295+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:02:45.428+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.821 seconds
[2022-12-16T18:02:55.755+0000] {processor.py:154} INFO - Started process (PID=7079) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:02:55.782+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:02:55.787+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:02:55.786+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:02:55.909+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:02:56.738+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T18:03:07.287+0000] {processor.py:154} INFO - Started process (PID=7089) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:03:07.340+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:03:07.345+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:03:07.344+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:03:07.430+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:03:07.589+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:03:07.588+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:03:07.712+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:03:07.712+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:03:07.834+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.561 seconds
[2022-12-16T18:03:18.139+0000] {processor.py:154} INFO - Started process (PID=7099) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:03:18.354+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:03:18.358+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:03:18.357+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:03:18.449+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:03:18.649+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:03:18.647+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:03:18.831+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:03:18.830+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:03:18.946+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.822 seconds
[2022-12-16T18:03:29.304+0000] {processor.py:154} INFO - Started process (PID=7109) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:03:29.316+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:03:29.330+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:03:29.320+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:03:29.471+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:03:30.002+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:03:30.001+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:03:30.124+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:03:30.123+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:03:30.322+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.045 seconds
[2022-12-16T18:03:41.058+0000] {processor.py:154} INFO - Started process (PID=7127) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:03:41.079+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:03:41.090+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:03:41.083+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:03:41.413+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:03:41.844+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:03:41.838+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:03:42.401+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:03:42.361+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:03:42.601+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.608 seconds
[2022-12-16T18:03:53.019+0000] {processor.py:154} INFO - Started process (PID=7137) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:03:53.046+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:03:53.053+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:03:53.049+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:03:53.155+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:03:53.726+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:03:53.725+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:03:53.848+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:03:53.847+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:03:54.024+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.036 seconds
[2022-12-16T18:04:04.493+0000] {processor.py:154} INFO - Started process (PID=7147) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:04:04.587+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:04:04.604+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:04:04.603+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:04:04.979+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:04:05.813+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:04:05.811+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:04:06.145+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:04:06.144+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:04:06.535+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.123 seconds
[2022-12-16T18:04:17.341+0000] {processor.py:154} INFO - Started process (PID=7157) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:04:17.396+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:04:17.405+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:04:17.404+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:04:17.983+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:04:18.957+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:04:18.956+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:04:19.437+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:04:19.424+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:04:20.010+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.708 seconds
[2022-12-16T18:04:31.067+0000] {processor.py:154} INFO - Started process (PID=7175) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:04:31.114+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:04:31.127+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:04:31.120+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:04:31.351+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:04:31.827+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:04:31.824+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:04:32.037+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:04:32.036+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:04:32.373+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.388 seconds
[2022-12-16T18:04:42.719+0000] {processor.py:154} INFO - Started process (PID=7185) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:04:42.749+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:04:42.754+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:04:42.753+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:04:42.837+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:04:42.971+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:04:42.970+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:04:43.104+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:04:43.103+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:04:43.233+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.527 seconds
[2022-12-16T18:04:53.512+0000] {processor.py:154} INFO - Started process (PID=7195) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:04:53.552+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:04:53.558+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:04:53.556+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:04:53.639+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:04:54.121+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T18:05:04.748+0000] {processor.py:154} INFO - Started process (PID=7213) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:05:04.832+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:05:04.869+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:05:04.867+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:05:05.180+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:05:05.752+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:05:05.751+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:05:05.921+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:05:05.920+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:05:06.194+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.508 seconds
[2022-12-16T18:05:16.935+0000] {processor.py:154} INFO - Started process (PID=7224) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:05:16.964+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:05:16.968+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:05:16.967+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:05:17.208+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:05:17.416+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:05:17.415+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:05:17.554+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:05:17.553+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:05:17.686+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.770 seconds
[2022-12-16T18:05:28.466+0000] {processor.py:154} INFO - Started process (PID=7234) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:05:28.477+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:05:28.481+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:05:28.480+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:05:28.733+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:05:29.065+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:05:29.064+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:05:29.179+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:05:29.178+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:05:29.305+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.917 seconds
[2022-12-16T18:05:39.492+0000] {processor.py:154} INFO - Started process (PID=7244) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:05:39.554+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:05:39.559+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:05:39.557+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:05:39.707+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:05:40.290+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:05:40.285+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:05:40.521+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:05:40.520+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:05:40.991+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.523 seconds
[2022-12-16T18:05:51.626+0000] {processor.py:154} INFO - Started process (PID=7263) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:05:51.672+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:05:51.676+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:05:51.675+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:05:52.014+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:05:52.340+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:05:52.339+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:05:52.687+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:05:52.662+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:05:52.984+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.450 seconds
[2022-12-16T18:06:03.382+0000] {processor.py:154} INFO - Started process (PID=7273) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:06:03.388+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:06:03.392+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:06:03.391+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:06:03.492+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:06:03.680+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:06:03.679+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:06:03.855+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:06:03.854+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:06:03.982+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.625 seconds
[2022-12-16T18:06:14.625+0000] {processor.py:154} INFO - Started process (PID=7283) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:06:14.643+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:06:14.647+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:06:14.646+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:06:14.740+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:06:14.910+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:06:14.909+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:06:15.039+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:06:15.038+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:06:15.206+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.599 seconds
[2022-12-16T18:06:25.604+0000] {processor.py:154} INFO - Started process (PID=7293) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:06:25.611+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:06:25.618+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:06:25.617+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:06:25.708+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:06:25.964+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:06:25.963+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:06:26.162+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:06:26.161+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:06:26.269+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.683 seconds
[2022-12-16T18:06:37.073+0000] {processor.py:154} INFO - Started process (PID=7309) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:06:37.134+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:06:37.148+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:06:37.137+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:06:37.606+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:06:38.540+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:06:38.539+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:06:39.032+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:06:39.031+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:06:39.304+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.368 seconds
[2022-12-16T18:06:49.982+0000] {processor.py:154} INFO - Started process (PID=7319) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:06:49.999+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:06:50.012+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:06:50.011+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:06:50.337+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:06:50.716+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:06:50.715+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:06:51.369+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:06:51.357+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:06:51.575+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.644 seconds
[2022-12-16T18:07:02.342+0000] {processor.py:154} INFO - Started process (PID=7329) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:07:02.368+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:07:02.372+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:07:02.371+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:07:02.520+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:07:02.687+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:07:02.686+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:07:02.843+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:07:02.842+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:07:02.976+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.665 seconds
[2022-12-16T18:07:13.327+0000] {processor.py:154} INFO - Started process (PID=7339) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:07:13.331+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:07:13.363+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:07:13.339+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:07:13.694+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:07:13.932+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:07:13.931+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:07:14.066+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:07:14.065+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:07:14.263+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.954 seconds
[2022-12-16T18:07:25.236+0000] {processor.py:154} INFO - Started process (PID=7356) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:07:25.294+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:07:25.335+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:07:25.296+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:07:25.538+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:07:25.956+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:07:25.955+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:07:26.282+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:07:26.281+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:07:26.944+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.899 seconds
[2022-12-16T18:07:37.871+0000] {processor.py:154} INFO - Started process (PID=7366) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:07:37.879+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:07:37.888+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:07:37.887+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:07:38.079+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:07:38.391+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:07:38.390+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:07:38.630+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:07:38.629+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:07:38.946+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.099 seconds
[2022-12-16T18:07:49.454+0000] {processor.py:154} INFO - Started process (PID=7376) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:07:49.471+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:07:49.476+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:07:49.475+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:07:49.869+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:07:50.684+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:07:50.683+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:07:51.182+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:07:51.181+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:07:51.450+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.038 seconds
[2022-12-16T18:08:01.934+0000] {processor.py:154} INFO - Started process (PID=7386) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:08:01.941+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:08:01.945+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:08:01.944+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:08:02.180+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:08:03.883+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:08:03.882+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:08:04.360+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:08:04.359+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:08:04.767+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.902 seconds
[2022-12-16T18:08:15.946+0000] {processor.py:154} INFO - Started process (PID=7404) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:08:15.970+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:08:15.976+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:08:15.975+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:08:16.356+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:08:16.994+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:08:16.969+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:08:17.562+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:08:17.558+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:08:17.928+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.061 seconds
[2022-12-16T18:08:28.412+0000] {processor.py:154} INFO - Started process (PID=7414) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:08:28.445+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:08:28.449+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:08:28.448+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:08:28.558+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:08:29.079+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:08:29.079+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:08:29.195+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:08:29.194+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:08:29.306+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.911 seconds
[2022-12-16T18:08:39.727+0000] {processor.py:154} INFO - Started process (PID=7421) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:08:39.781+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:08:39.786+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:08:39.785+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:08:39.870+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:08:40.017+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:08:40.016+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:08:40.151+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:08:40.150+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:08:40.303+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.592 seconds
[2022-12-16T18:08:51.398+0000] {processor.py:154} INFO - Started process (PID=7434) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:08:51.407+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:08:51.416+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:08:51.415+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:08:51.516+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:08:51.777+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:08:51.776+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:08:51.967+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:08:51.965+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:08:52.193+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.809 seconds
[2022-12-16T18:09:03.019+0000] {processor.py:154} INFO - Started process (PID=7452) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:09:03.039+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:09:03.043+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:09:03.042+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:09:03.224+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:09:03.563+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:09:03.562+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:09:04.133+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:09:04.132+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:09:04.491+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.528 seconds
[2022-12-16T18:09:14.964+0000] {processor.py:154} INFO - Started process (PID=7462) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:09:14.993+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:09:14.997+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:09:14.996+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:09:15.249+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:09:16.216+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:09:16.215+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:09:16.410+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:09:16.408+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:09:16.550+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.603 seconds
[2022-12-16T18:09:26.830+0000] {processor.py:154} INFO - Started process (PID=7474) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:09:26.860+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:09:26.864+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:09:26.863+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:09:26.975+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:09:27.392+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:09:27.391+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:09:27.562+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:09:27.561+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:09:27.682+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.868 seconds
[2022-12-16T18:09:38.333+0000] {processor.py:154} INFO - Started process (PID=7489) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:09:38.340+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:09:38.352+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:09:38.351+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:09:38.576+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:09:38.856+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:09:38.855+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:09:39.152+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:09:39.149+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:09:39.543+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.269 seconds
[2022-12-16T18:09:50.056+0000] {processor.py:154} INFO - Started process (PID=7500) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:09:50.096+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:09:50.125+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:09:50.123+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:09:50.638+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:09:51.091+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:09:51.090+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:09:51.346+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:09:51.341+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:09:51.519+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.560 seconds
[2022-12-16T18:10:01.856+0000] {processor.py:154} INFO - Started process (PID=7510) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:10:01.873+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:10:01.878+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:10:01.877+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:10:01.988+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:10:03.130+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T18:10:13.516+0000] {processor.py:154} INFO - Started process (PID=7520) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:10:13.587+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:10:13.595+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:10:13.594+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:10:13.715+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:10:14.169+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:10:14.168+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:10:14.316+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:10:14.315+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:10:14.455+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.963 seconds
[2022-12-16T18:10:24.852+0000] {processor.py:154} INFO - Started process (PID=7538) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:10:24.892+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:10:24.906+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:10:24.905+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:10:25.155+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:10:26.884+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:10:26.870+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:10:27.451+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:10:27.450+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:10:27.808+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.981 seconds
[2022-12-16T18:10:38.134+0000] {processor.py:154} INFO - Started process (PID=7548) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:10:38.138+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:10:38.150+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:10:38.145+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:10:38.312+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:10:38.578+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:10:38.577+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:10:38.716+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:10:38.715+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:10:38.926+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.812 seconds
[2022-12-16T18:10:49.265+0000] {processor.py:154} INFO - Started process (PID=7558) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:10:49.311+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:10:49.319+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:10:49.318+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:10:49.553+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:10:49.843+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:10:49.842+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:10:50.007+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:10:50.006+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:10:50.203+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.961 seconds
[2022-12-16T18:11:00.560+0000] {processor.py:154} INFO - Started process (PID=7568) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:11:00.563+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:11:00.568+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:11:00.567+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:11:00.661+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:11:01.413+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:11:01.412+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:11:01.662+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:11:01.660+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:11:01.788+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.242 seconds
[2022-12-16T18:11:12.189+0000] {processor.py:154} INFO - Started process (PID=7586) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:11:12.213+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:11:12.220+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:11:12.219+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:11:12.383+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:11:12.620+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:11:12.619+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:11:13.001+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:11:13.000+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:11:13.223+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.052 seconds
[2022-12-16T18:11:23.639+0000] {processor.py:154} INFO - Started process (PID=7596) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:11:23.643+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:11:23.651+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:11:23.650+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:11:23.795+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:11:24.191+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:11:24.190+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:11:24.340+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:11:24.339+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:11:24.474+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.860 seconds
[2022-12-16T18:11:34.796+0000] {processor.py:154} INFO - Started process (PID=7606) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:11:34.846+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:11:34.850+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:11:34.849+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:11:34.950+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:11:35.348+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:11:35.347+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:11:35.505+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:11:35.502+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:11:35.646+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.863 seconds
[2022-12-16T18:11:46.154+0000] {processor.py:154} INFO - Started process (PID=7616) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:11:46.314+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:11:46.324+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:11:46.319+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:11:46.569+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:11:46.731+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:11:46.730+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:11:46.865+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:11:46.864+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:11:47.035+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.918 seconds
[2022-12-16T18:11:57.556+0000] {processor.py:154} INFO - Started process (PID=7631) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:11:57.568+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:11:57.572+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:11:57.571+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:11:57.876+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:11:58.768+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:11:58.767+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:11:59.472+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:11:59.471+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:11:59.862+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.337 seconds
[2022-12-16T18:12:10.358+0000] {processor.py:154} INFO - Started process (PID=7641) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:12:10.390+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:12:10.394+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:12:10.393+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:12:10.492+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:12:10.652+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:12:10.651+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:12:10.865+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:12:10.864+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:12:11.159+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.838 seconds
[2022-12-16T18:12:21.599+0000] {processor.py:154} INFO - Started process (PID=7651) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:12:21.693+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:12:21.699+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:12:21.697+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:12:21.845+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:12:23.116+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T18:12:33.378+0000] {processor.py:154} INFO - Started process (PID=7668) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:12:33.436+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:12:33.446+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:12:33.445+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:12:33.579+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:12:33.775+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:12:33.774+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:12:33.949+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:12:33.948+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:12:34.073+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.711 seconds
[2022-12-16T18:12:44.303+0000] {processor.py:154} INFO - Started process (PID=7679) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:12:44.324+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:12:44.328+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:12:44.327+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:12:44.580+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:12:44.890+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:12:44.889+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:12:45.035+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:12:45.034+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:12:45.211+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.941 seconds
[2022-12-16T18:12:55.776+0000] {processor.py:154} INFO - Started process (PID=7689) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:12:55.847+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:12:55.878+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:12:55.870+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:12:56.127+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:12:57.508+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T18:13:07.938+0000] {processor.py:154} INFO - Started process (PID=7699) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:13:07.980+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:13:07.988+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:13:07.987+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:13:08.092+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:13:08.291+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:13:08.290+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:13:08.449+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:13:08.445+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:13:08.577+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.685 seconds
[2022-12-16T18:13:14.566+0000] {processor.py:154} INFO - Started process (PID=7723) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:13:14.579+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:13:14.604+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:13:14.603+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:13:14.933+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:13:15.179+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:13:15.178+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:13:15.333+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:13:15.331+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:13:15.521+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.030 seconds
[2022-12-16T18:13:26.236+0000] {processor.py:154} INFO - Started process (PID=7735) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:13:26.270+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:13:26.286+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:13:26.281+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:13:26.460+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:13:26.652+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:13:26.651+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:13:26.912+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:13:26.911+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:13:27.037+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.815 seconds
[2022-12-16T18:13:37.663+0000] {processor.py:154} INFO - Started process (PID=7745) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:13:37.672+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:13:37.681+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:13:37.680+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:13:37.828+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:13:38.242+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:13:38.241+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:13:38.403+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:13:38.402+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:13:38.535+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.896 seconds
[2022-12-16T18:13:49.102+0000] {processor.py:154} INFO - Started process (PID=7755) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:13:49.142+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:13:49.155+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:13:49.144+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:13:49.316+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:13:49.924+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:13:49.923+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:13:50.174+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:13:50.173+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:13:50.555+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.490 seconds
[2022-12-16T18:14:01.231+0000] {processor.py:154} INFO - Started process (PID=7770) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:14:01.275+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:14:01.286+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:14:01.285+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:14:01.616+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:14:02.500+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:14:02.499+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:14:02.843+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:14:02.842+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:14:03.178+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.027 seconds
[2022-12-16T18:14:13.546+0000] {processor.py:154} INFO - Started process (PID=7780) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:14:13.561+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:14:13.573+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:14:13.565+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:14:13.797+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:14:14.138+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:14:14.133+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:14:14.453+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:14:14.443+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:14:14.839+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.347 seconds
[2022-12-16T18:14:25.403+0000] {processor.py:154} INFO - Started process (PID=7790) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:14:25.424+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:14:25.431+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:14:25.430+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:14:25.597+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:14:25.827+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:14:25.826+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:14:26.021+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:14:26.020+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:14:26.323+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.959 seconds
[2022-12-16T18:14:36.720+0000] {processor.py:154} INFO - Started process (PID=7800) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:14:36.725+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:14:36.732+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:14:36.730+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:14:36.936+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:14:37.218+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:14:37.216+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:14:37.429+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:14:37.428+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:14:37.605+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.899 seconds
[2022-12-16T18:14:48.343+0000] {processor.py:154} INFO - Started process (PID=7817) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:14:48.371+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:14:48.377+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:14:48.376+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:14:48.887+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:14:49.907+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:14:49.906+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:14:50.242+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:14:50.241+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:14:50.894+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.614 seconds
[2022-12-16T18:15:01.367+0000] {processor.py:154} INFO - Started process (PID=7830) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:15:01.389+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:15:01.395+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:15:01.394+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:15:01.566+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:15:01.851+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T18:15:12.303+0000] {processor.py:154} INFO - Started process (PID=7837) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:15:12.331+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:15:12.336+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:15:12.334+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:15:12.445+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:15:12.611+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:15:12.610+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:15:12.740+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:15:12.739+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:15:12.886+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.603 seconds
[2022-12-16T18:15:23.254+0000] {processor.py:154} INFO - Started process (PID=7847) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:15:23.278+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:15:23.283+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:15:23.282+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:15:23.483+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:15:23.868+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T18:15:34.279+0000] {processor.py:154} INFO - Started process (PID=7864) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:15:34.307+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:15:34.311+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:15:34.310+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:15:34.508+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:15:34.884+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:15:34.883+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:15:35.088+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:15:35.084+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:15:35.339+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.091 seconds
[2022-12-16T18:15:45.857+0000] {processor.py:154} INFO - Started process (PID=7874) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:15:45.879+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:15:45.883+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:15:45.882+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:15:46.070+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:15:46.752+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:15:46.751+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:15:47.014+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:15:47.013+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:15:47.249+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.407 seconds
[2022-12-16T18:15:57.499+0000] {processor.py:154} INFO - Started process (PID=7884) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:15:57.503+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:15:57.507+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:15:57.506+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:15:57.603+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:15:57.768+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:15:57.767+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:15:57.881+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:15:57.880+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:15:58.014+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.531 seconds
[2022-12-16T18:16:08.336+0000] {processor.py:154} INFO - Started process (PID=7894) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:16:08.383+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:16:08.388+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:16:08.387+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:16:08.494+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:16:08.688+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:16:08.687+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:16:08.802+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:16:08.802+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:16:08.908+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.588 seconds
[2022-12-16T18:16:19.545+0000] {processor.py:154} INFO - Started process (PID=7912) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:16:19.585+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:16:19.597+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:16:19.588+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:16:19.745+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:16:20.112+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:16:20.111+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:16:20.332+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:16:20.331+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:16:20.499+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.971 seconds
[2022-12-16T18:16:30.840+0000] {processor.py:154} INFO - Started process (PID=7922) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:16:30.858+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:16:30.862+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:16:30.861+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:16:30.981+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:16:31.171+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:16:31.170+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:16:31.297+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:16:31.296+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:16:31.443+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.617 seconds
[2022-12-16T18:16:41.783+0000] {processor.py:154} INFO - Started process (PID=7932) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:16:41.836+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:16:41.841+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:16:41.839+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:16:41.924+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:16:42.832+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:16:42.831+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:16:42.952+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:16:42.952+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:16:43.408+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.640 seconds
[2022-12-16T18:16:53.908+0000] {processor.py:154} INFO - Started process (PID=7942) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:16:53.924+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:16:53.932+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:16:53.931+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:16:54.209+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:16:54.974+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:16:54.973+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:16:55.331+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:16:55.329+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:16:55.701+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.826 seconds
[2022-12-16T18:17:06.874+0000] {processor.py:154} INFO - Started process (PID=7961) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:17:06.889+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:17:06.902+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:17:06.901+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:17:07.120+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:17:07.644+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:17:07.643+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:17:07.933+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:17:07.932+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:17:08.215+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.387 seconds
[2022-12-16T18:17:19.307+0000] {processor.py:154} INFO - Started process (PID=7971) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:17:19.310+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:17:19.322+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:17:19.321+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:17:19.487+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:17:19.676+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:17:19.675+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:17:19.827+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:17:19.826+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:17:20.020+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.748 seconds
[2022-12-16T18:17:30.708+0000] {processor.py:154} INFO - Started process (PID=7981) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:17:30.730+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:17:30.735+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:17:30.733+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:17:31.136+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:17:33.020+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:17:33.019+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:17:33.189+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:17:33.188+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:17:33.382+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.697 seconds
[2022-12-16T18:17:43.890+0000] {processor.py:154} INFO - Started process (PID=7991) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:17:43.896+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:17:43.901+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:17:43.900+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:17:44.120+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:17:44.874+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:17:44.873+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:17:45.157+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:17:45.152+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:17:45.434+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.577 seconds
[2022-12-16T18:17:55.979+0000] {processor.py:154} INFO - Started process (PID=8009) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:17:55.982+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:17:55.987+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:17:55.986+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:17:56.161+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:17:56.335+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:17:56.334+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:17:56.515+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:17:56.514+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:17:56.805+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.849 seconds
[2022-12-16T18:18:07.422+0000] {processor.py:154} INFO - Started process (PID=8019) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:18:07.499+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:18:07.503+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:18:07.502+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:18:07.631+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:18:08.542+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T18:18:18.972+0000] {processor.py:154} INFO - Started process (PID=8029) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:18:19.011+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:18:19.017+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:18:19.015+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:18:19.156+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:18:19.688+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:18:19.687+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:18:19.830+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:18:19.829+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:18:19.994+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.036 seconds
[2022-12-16T18:18:30.317+0000] {processor.py:154} INFO - Started process (PID=8039) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:18:30.320+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:18:30.324+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:18:30.323+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:18:30.417+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:18:31.432+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T18:18:42.121+0000] {processor.py:154} INFO - Started process (PID=8056) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:18:42.158+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:18:42.172+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:18:42.171+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:18:42.427+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:18:42.696+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:18:42.695+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:18:43.007+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:18:43.006+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:18:43.312+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.264 seconds
[2022-12-16T18:18:54.012+0000] {processor.py:154} INFO - Started process (PID=8066) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:18:54.068+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:18:54.093+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:18:54.092+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:18:54.371+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:18:55.133+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:18:55.132+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:18:55.312+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:18:55.311+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:18:55.665+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.720 seconds
[2022-12-16T18:19:06.336+0000] {processor.py:154} INFO - Started process (PID=8076) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:19:06.351+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:19:06.360+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:19:06.359+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:19:06.604+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:19:07.357+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:19:07.356+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:19:07.590+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:19:07.589+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:19:07.942+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.647 seconds
[2022-12-16T18:19:18.974+0000] {processor.py:154} INFO - Started process (PID=8086) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:19:19.074+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:19:19.079+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:19:19.078+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:19:19.462+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:19:20.569+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T18:19:31.350+0000] {processor.py:154} INFO - Started process (PID=8104) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:19:31.367+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:19:31.417+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:19:31.415+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:19:31.975+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:19:32.670+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:19:32.669+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:19:33.008+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:19:33.007+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:19:33.426+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.249 seconds
[2022-12-16T18:19:43.914+0000] {processor.py:154} INFO - Started process (PID=8114) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:19:43.922+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:19:43.926+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:19:43.925+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:19:44.079+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:19:44.446+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:19:44.444+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:19:44.678+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:19:44.677+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:19:44.843+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.956 seconds
[2022-12-16T18:19:55.408+0000] {processor.py:154} INFO - Started process (PID=8124) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:19:55.438+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:19:55.444+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:19:55.441+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:19:55.571+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:19:55.720+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:19:55.719+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:19:55.874+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:19:55.872+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:19:56.102+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.721 seconds
[2022-12-16T18:20:06.960+0000] {processor.py:154} INFO - Started process (PID=8141) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:20:07.028+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:20:07.065+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:20:07.064+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:20:07.476+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:20:08.451+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:20:08.450+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:20:08.773+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:20:08.772+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:20:09.085+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.206 seconds
[2022-12-16T18:20:19.397+0000] {processor.py:154} INFO - Started process (PID=8152) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:20:19.403+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:20:19.411+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:20:19.410+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:20:19.653+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:20:20.975+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T18:20:31.401+0000] {processor.py:154} INFO - Started process (PID=8162) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:20:31.453+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:20:31.458+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:20:31.457+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:20:31.548+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:20:32.288+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:20:32.287+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:20:32.590+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:20:32.589+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:20:32.830+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.450 seconds
[2022-12-16T18:20:43.716+0000] {processor.py:154} INFO - Started process (PID=8172) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:20:43.740+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:20:43.744+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:20:43.743+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:20:43.900+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:20:44.858+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:20:44.857+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:20:45.111+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:20:45.110+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:20:45.290+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.612 seconds
[2022-12-16T18:20:56.418+0000] {processor.py:154} INFO - Started process (PID=8191) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:20:56.449+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:20:56.453+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:20:56.452+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:20:56.668+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:20:57.052+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:20:57.051+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:20:57.489+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:20:57.488+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:20:57.791+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.423 seconds
[2022-12-16T18:21:08.630+0000] {processor.py:154} INFO - Started process (PID=8201) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:21:08.648+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:21:08.660+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:21:08.659+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:21:09.056+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:21:09.663+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:21:09.662+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:21:10.093+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:21:10.089+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:21:10.579+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.007 seconds
[2022-12-16T18:21:21.368+0000] {processor.py:154} INFO - Started process (PID=8211) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:21:21.385+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:21:21.390+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:21:21.388+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:21:21.619+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:21:22.192+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:21:22.191+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:21:22.364+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:21:22.363+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:21:22.546+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.218 seconds
[2022-12-16T18:21:32.934+0000] {processor.py:154} INFO - Started process (PID=8221) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:21:32.955+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:21:32.959+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:21:32.959+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:21:33.053+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:21:33.211+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:21:33.211+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:21:33.323+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:21:33.322+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:21:33.519+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.599 seconds
[2022-12-16T18:21:44.133+0000] {processor.py:154} INFO - Started process (PID=8238) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:21:44.183+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:21:44.187+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:21:44.186+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:21:44.299+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:21:44.448+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:21:44.447+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:21:44.596+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:21:44.595+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:21:44.742+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.671 seconds
[2022-12-16T18:21:55.261+0000] {processor.py:154} INFO - Started process (PID=8249) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:21:55.316+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:21:55.324+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:21:55.320+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:21:55.552+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:21:56.052+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:21:56.051+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:21:56.418+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:21:56.411+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:21:56.606+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.369 seconds
[2022-12-16T18:22:06.919+0000] {processor.py:154} INFO - Started process (PID=8259) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:22:06.962+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:22:06.966+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:22:06.965+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:22:07.052+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:22:07.188+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:22:07.187+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:22:07.324+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:22:07.323+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:22:07.655+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.754 seconds
[2022-12-16T18:22:18.930+0000] {processor.py:154} INFO - Started process (PID=8269) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:22:18.938+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:22:18.946+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:22:18.941+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:22:19.159+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:22:19.869+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:22:19.854+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:22:20.388+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:22:20.387+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:22:20.766+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.873 seconds
[2022-12-16T18:22:31.400+0000] {processor.py:154} INFO - Started process (PID=8287) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:22:31.416+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:22:31.436+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:22:31.434+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:22:31.715+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:22:33.244+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T18:22:43.951+0000] {processor.py:154} INFO - Started process (PID=8297) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:22:44.031+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:22:44.036+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:22:44.034+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:22:44.125+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:22:44.256+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:22:44.256+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:22:44.367+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:22:44.366+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:22:44.562+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.627 seconds
[2022-12-16T18:22:54.963+0000] {processor.py:154} INFO - Started process (PID=8307) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:22:54.989+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:22:55.002+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:22:54.997+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:22:55.170+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:22:56.457+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T18:23:06.807+0000] {processor.py:154} INFO - Started process (PID=8317) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:23:06.866+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:23:06.874+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:23:06.869+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:23:07.027+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:23:08.276+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T18:23:18.877+0000] {processor.py:154} INFO - Started process (PID=8335) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:23:18.923+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:23:18.927+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:23:18.926+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:23:19.098+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:23:19.395+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:23:19.383+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:23:19.842+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:23:19.828+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:23:20.048+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.196 seconds
[2022-12-16T18:23:30.639+0000] {processor.py:154} INFO - Started process (PID=8345) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:23:30.684+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:23:30.691+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:23:30.687+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:23:30.953+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:23:31.474+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:23:31.472+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:23:31.709+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:23:31.708+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:23:32.308+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.714 seconds
[2022-12-16T18:23:42.634+0000] {processor.py:154} INFO - Started process (PID=8355) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:23:42.642+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:23:42.653+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:23:42.652+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:23:42.909+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:23:43.125+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:23:43.123+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:23:43.329+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:23:43.328+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:23:43.513+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.912 seconds
[2022-12-16T18:23:53.908+0000] {processor.py:154} INFO - Started process (PID=8371) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:23:53.935+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:23:53.943+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:23:53.941+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:23:54.299+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:23:55.072+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:23:55.071+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:23:55.486+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:23:55.477+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:23:56.247+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.404 seconds
[2022-12-16T18:24:07.020+0000] {processor.py:154} INFO - Started process (PID=8382) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:24:07.063+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:24:07.078+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:24:07.076+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:24:07.317+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:24:07.571+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:24:07.569+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:24:08.003+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:24:08.002+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:24:08.300+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.295 seconds
[2022-12-16T18:24:18.655+0000] {processor.py:154} INFO - Started process (PID=8392) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:24:18.685+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:24:18.692+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:24:18.688+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:24:18.830+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:24:19.023+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:24:19.022+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:24:19.154+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:24:19.153+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:24:19.337+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.698 seconds
[2022-12-16T18:24:29.579+0000] {processor.py:154} INFO - Started process (PID=8402) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:24:29.594+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:24:29.600+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:24:29.599+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:24:29.733+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:24:30.730+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T18:24:41.434+0000] {processor.py:154} INFO - Started process (PID=8421) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:24:41.458+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:24:41.466+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:24:41.465+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:24:41.832+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:24:42.540+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:24:42.528+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:24:42.854+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:24:42.848+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:24:43.125+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.780 seconds
[2022-12-16T18:24:53.435+0000] {processor.py:154} INFO - Started process (PID=8431) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:24:53.448+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:24:53.457+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:24:53.456+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:24:53.555+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:24:54.887+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T18:25:05.233+0000] {processor.py:154} INFO - Started process (PID=8441) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:25:05.259+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:25:05.267+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:25:05.266+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:25:05.347+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:25:06.475+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:25:06.474+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:25:06.662+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:25:06.661+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:25:07.071+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.852 seconds
[2022-12-16T18:25:17.828+0000] {processor.py:154} INFO - Started process (PID=8454) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:25:17.836+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:25:17.841+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:25:17.840+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:25:17.977+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:25:19.198+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:25:19.188+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:25:19.610+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:25:19.605+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:25:20.246+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.429 seconds
[2022-12-16T18:25:31.519+0000] {processor.py:154} INFO - Started process (PID=8469) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:25:31.522+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:25:31.532+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:25:31.531+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:25:31.974+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:25:33.476+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:25:33.474+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:25:34.041+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:25:34.040+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:25:34.498+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.055 seconds
[2022-12-16T18:25:45.260+0000] {processor.py:154} INFO - Started process (PID=8484) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:25:45.291+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:25:45.314+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:25:45.306+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:25:45.679+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:25:45.983+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:25:45.982+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:25:46.209+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:25:46.208+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:25:46.386+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.160 seconds
[2022-12-16T18:25:56.705+0000] {processor.py:154} INFO - Started process (PID=8494) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:25:56.710+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:25:56.714+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:25:56.713+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:25:56.811+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:25:58.231+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:25:58.230+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:25:58.369+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:25:58.368+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:25:58.512+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.846 seconds
[2022-12-16T18:26:09.029+0000] {processor.py:154} INFO - Started process (PID=8511) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:26:09.042+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:26:09.058+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:26:09.045+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:26:09.300+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:26:10.321+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:26:10.319+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:26:10.587+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:26:10.586+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:26:10.779+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.791 seconds
[2022-12-16T18:26:21.304+0000] {processor.py:154} INFO - Started process (PID=8522) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:26:21.329+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:26:21.333+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:26:21.332+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:26:21.423+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:26:22.255+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T18:26:32.710+0000] {processor.py:154} INFO - Started process (PID=8532) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:26:32.719+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:26:32.723+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:26:32.722+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:26:32.911+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:26:33.667+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T18:26:44.189+0000] {processor.py:154} INFO - Started process (PID=8542) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:26:44.192+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:26:44.197+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:26:44.196+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:26:44.319+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:26:45.088+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:26:45.087+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:26:45.420+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:26:45.419+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:26:45.584+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.420 seconds
[2022-12-16T18:26:56.175+0000] {processor.py:154} INFO - Started process (PID=8560) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:26:56.211+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:26:56.242+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:26:56.241+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:26:56.579+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:26:56.934+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:26:56.933+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:26:57.220+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:26:57.219+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:26:57.485+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.469 seconds
[2022-12-16T18:27:07.764+0000] {processor.py:154} INFO - Started process (PID=8570) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:27:07.769+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:27:07.778+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:27:07.772+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:27:07.919+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:27:08.633+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:27:08.632+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:27:08.914+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:27:08.913+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:27:09.180+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.435 seconds
[2022-12-16T18:27:19.576+0000] {processor.py:154} INFO - Started process (PID=8580) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:27:19.924+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:27:19.931+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:27:19.930+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:27:20.032+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:27:20.648+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:27:20.647+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:27:20.910+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:27:20.909+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:27:21.121+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.562 seconds
[2022-12-16T18:27:31.483+0000] {processor.py:154} INFO - Started process (PID=8590) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:27:31.486+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:27:31.491+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:27:31.490+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:27:31.615+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:27:31.899+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:27:31.898+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:27:32.366+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:27:32.365+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:27:32.619+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.157 seconds
[2022-12-16T18:27:43.271+0000] {processor.py:154} INFO - Started process (PID=8608) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:27:43.329+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:27:43.359+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:27:43.344+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:27:43.899+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:27:44.688+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:27:44.687+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:27:45.289+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:27:45.288+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:27:45.575+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.397 seconds
[2022-12-16T18:27:56.008+0000] {processor.py:154} INFO - Started process (PID=8618) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:27:56.011+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:27:56.020+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:27:56.019+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:27:56.198+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:27:57.025+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:27:57.021+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:27:57.375+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:27:57.374+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:27:57.639+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.668 seconds
[2022-12-16T18:28:08.429+0000] {processor.py:154} INFO - Started process (PID=8628) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:28:08.433+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:28:08.437+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:28:08.436+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:28:08.576+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:28:09.201+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:28:09.200+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:28:09.485+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:28:09.484+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:28:09.778+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.366 seconds
[2022-12-16T18:28:20.303+0000] {processor.py:154} INFO - Started process (PID=8645) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:28:20.348+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:28:20.365+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:28:20.353+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:28:20.510+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:28:20.895+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:28:20.894+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:28:21.430+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:28:21.429+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:28:21.863+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.607 seconds
[2022-12-16T18:28:32.195+0000] {processor.py:154} INFO - Started process (PID=8656) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:28:32.240+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:28:32.245+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:28:32.244+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:28:32.336+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:28:33.598+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T18:28:44.274+0000] {processor.py:154} INFO - Started process (PID=8666) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:28:44.320+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:28:44.323+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:28:44.323+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:28:44.420+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:28:44.600+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:28:44.599+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:28:44.842+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:28:44.837+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:28:45.134+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.874 seconds
[2022-12-16T18:28:55.736+0000] {processor.py:154} INFO - Started process (PID=8676) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:28:55.769+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:28:55.782+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:28:55.772+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:28:56.032+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:28:56.313+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:28:56.312+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:28:56.921+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:28:56.920+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:28:57.520+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.825 seconds
[2022-12-16T18:29:08.116+0000] {processor.py:154} INFO - Started process (PID=8694) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:29:08.182+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:29:08.188+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:29:08.185+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:29:08.299+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:29:08.455+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:29:08.454+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:29:08.606+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:29:08.605+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:29:08.754+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.682 seconds
[2022-12-16T18:29:19.521+0000] {processor.py:154} INFO - Started process (PID=8704) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:29:19.565+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:29:19.571+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:29:19.570+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:29:19.677+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:29:19.815+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:29:19.814+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:29:19.936+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:29:19.935+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:29:20.061+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.588 seconds
[2022-12-16T18:29:30.362+0000] {processor.py:154} INFO - Started process (PID=8714) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:29:30.460+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:29:30.465+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:29:30.464+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:29:30.552+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:29:31.681+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:29:31.680+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:29:31.805+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:29:31.804+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:29:31.961+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.615 seconds
[2022-12-16T18:29:42.636+0000] {processor.py:154} INFO - Started process (PID=8724) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:29:42.684+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:29:42.688+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:29:42.687+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:29:42.836+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:29:44.772+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:29:44.749+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:29:45.427+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:29:45.426+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:29:45.603+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.989 seconds
[2022-12-16T18:29:55.998+0000] {processor.py:154} INFO - Started process (PID=8742) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:29:56.001+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:29:56.005+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:29:56.004+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:29:56.354+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:29:57.331+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:29:57.329+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:29:57.776+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:29:57.775+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:29:58.208+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.253 seconds
[2022-12-16T18:30:08.948+0000] {processor.py:154} INFO - Started process (PID=8752) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:30:08.986+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:30:09.004+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:30:09.003+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:30:09.289+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:30:09.495+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:30:09.495+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:30:09.614+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:30:09.613+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:30:09.774+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.905 seconds
[2022-12-16T18:30:20.203+0000] {processor.py:154} INFO - Started process (PID=8762) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:30:20.255+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:30:20.259+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:30:20.257+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:30:20.521+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:30:21.208+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:30:21.206+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:30:21.347+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:30:21.347+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:30:21.464+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.300 seconds
[2022-12-16T18:30:31.865+0000] {processor.py:154} INFO - Started process (PID=8779) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:30:31.878+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:30:31.887+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:30:31.881+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:30:32.079+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:30:32.542+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:30:32.537+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:30:32.974+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:30:32.964+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:30:33.490+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.642 seconds
[2022-12-16T18:30:44.267+0000] {processor.py:154} INFO - Started process (PID=8790) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:30:44.298+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:30:44.304+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:30:44.303+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:30:44.524+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:30:45.026+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:30:45.025+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:30:45.749+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:30:45.733+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:30:46.183+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.953 seconds
[2022-12-16T18:30:56.762+0000] {processor.py:154} INFO - Started process (PID=8800) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:30:56.768+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:30:56.778+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:30:56.777+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:30:56.901+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:30:58.092+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:30:58.091+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:30:58.292+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:30:58.291+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:30:58.417+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.689 seconds
[2022-12-16T18:31:08.743+0000] {processor.py:154} INFO - Started process (PID=8810) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:31:08.747+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:31:08.752+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:31:08.751+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:31:08.859+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:31:10.085+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:31:10.084+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:31:10.292+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:31:10.291+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:31:10.404+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.675 seconds
[2022-12-16T18:31:21.156+0000] {processor.py:154} INFO - Started process (PID=8831) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:31:21.316+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:31:21.329+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:31:21.328+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:31:21.660+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:31:21.906+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:31:21.905+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:31:22.089+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:31:22.087+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:31:22.233+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.149 seconds
[2022-12-16T18:31:32.704+0000] {processor.py:154} INFO - Started process (PID=8841) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:31:32.731+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:31:32.736+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:31:32.734+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:31:33.167+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:31:33.600+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:31:33.599+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:31:33.869+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:31:33.868+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:31:34.014+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.331 seconds
[2022-12-16T18:31:44.383+0000] {processor.py:154} INFO - Started process (PID=8848) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:31:44.399+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:31:44.407+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:31:44.406+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:31:44.556+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:31:45.074+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:31:45.073+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:31:45.265+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:31:45.264+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:31:45.468+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.110 seconds
[2022-12-16T18:31:55.723+0000] {processor.py:154} INFO - Started process (PID=8858) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:31:55.750+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:31:55.768+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:31:55.766+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:31:55.867+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:31:56.187+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:31:56.186+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:31:56.369+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:31:56.368+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:31:56.506+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.796 seconds
[2022-12-16T18:32:07.072+0000] {processor.py:154} INFO - Started process (PID=8877) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:32:07.107+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:32:07.119+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:32:07.118+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:32:07.257+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:32:08.399+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:32:08.398+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:32:08.569+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:32:08.568+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:32:08.739+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.711 seconds
[2022-12-16T18:32:19.078+0000] {processor.py:154} INFO - Started process (PID=8887) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:32:19.131+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:32:19.135+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:32:19.134+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:32:19.235+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:32:20.260+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:32:20.259+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:32:20.418+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:32:20.417+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:32:20.528+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.467 seconds
[2022-12-16T18:32:30.910+0000] {processor.py:154} INFO - Started process (PID=8900) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:32:30.934+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:32:30.937+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:32:30.937+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:32:31.045+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:32:31.598+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:32:31.597+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:32:31.749+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:32:31.748+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:32:32.096+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.233 seconds
[2022-12-16T18:32:42.698+0000] {processor.py:154} INFO - Started process (PID=8914) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:32:42.758+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:32:42.766+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:32:42.761+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:32:43.241+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:32:44.872+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:32:44.871+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:32:45.788+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:32:45.781+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:32:46.118+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.468 seconds
[2022-12-16T18:32:56.486+0000] {processor.py:154} INFO - Started process (PID=8928) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:32:56.491+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:32:56.500+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:32:56.499+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:32:56.686+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:32:57.219+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:32:57.218+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:32:57.424+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:32:57.422+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:32:57.652+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.193 seconds
[2022-12-16T18:33:08.024+0000] {processor.py:154} INFO - Started process (PID=8935) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:33:08.067+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:33:08.072+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:33:08.071+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:33:08.160+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:33:08.339+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:33:08.338+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:33:08.731+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:33:08.730+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:33:09.224+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.219 seconds
[2022-12-16T18:33:19.810+0000] {processor.py:154} INFO - Started process (PID=8945) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:33:19.836+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:33:19.841+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:33:19.840+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:33:19.961+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:33:20.862+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T18:33:31.433+0000] {processor.py:154} INFO - Started process (PID=8963) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:33:31.496+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:33:31.500+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:33:31.499+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:33:31.670+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:33:32.324+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:33:32.323+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:33:32.654+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:33:32.653+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:33:32.898+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.482 seconds
[2022-12-16T18:33:43.466+0000] {processor.py:154} INFO - Started process (PID=8973) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:33:43.520+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:33:43.525+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:33:43.524+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:33:43.641+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:33:43.834+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:33:43.833+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:33:43.999+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:33:43.998+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:33:44.110+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.660 seconds
[2022-12-16T18:33:55.069+0000] {processor.py:154} INFO - Started process (PID=8983) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:33:55.102+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:33:55.106+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:33:55.105+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:33:55.264+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:33:55.450+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:33:55.449+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:33:55.601+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:33:55.600+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:33:55.741+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.707 seconds
[2022-12-16T18:34:06.130+0000] {processor.py:154} INFO - Started process (PID=8993) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:34:06.174+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:34:06.179+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:34:06.178+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:34:06.339+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:34:07.363+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:34:07.362+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:34:07.705+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:34:07.704+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:34:07.954+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.842 seconds
[2022-12-16T18:34:18.417+0000] {processor.py:154} INFO - Started process (PID=9011) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:34:18.428+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:34:18.433+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:34:18.432+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:34:18.533+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:34:18.688+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:34:18.687+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:34:18.825+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:34:18.824+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:34:19.112+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.712 seconds
[2022-12-16T18:34:29.664+0000] {processor.py:154} INFO - Started process (PID=9021) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:34:29.713+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:34:29.721+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:34:29.716+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:34:29.847+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:34:30.457+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:34:30.455+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:34:30.592+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:34:30.592+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:34:30.702+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.055 seconds
[2022-12-16T18:34:41.188+0000] {processor.py:154} INFO - Started process (PID=9031) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:34:41.217+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:34:41.222+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:34:41.220+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:34:41.331+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:34:42.176+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:34:42.175+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:34:42.370+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:34:42.370+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:34:42.494+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.334 seconds
[2022-12-16T18:34:52.713+0000] {processor.py:154} INFO - Started process (PID=9041) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:34:52.743+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:34:52.747+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:34:52.746+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:34:52.928+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:34:53.395+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:34:53.393+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:34:53.725+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:34:53.724+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:34:54.022+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.327 seconds
[2022-12-16T18:35:05.113+0000] {processor.py:154} INFO - Started process (PID=9059) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:35:05.136+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:35:05.148+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:35:05.147+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:35:05.447+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:35:05.847+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:35:05.846+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:35:06.400+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:35:06.388+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:35:06.776+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.722 seconds
[2022-12-16T18:35:17.434+0000] {processor.py:154} INFO - Started process (PID=9069) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:35:17.440+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:35:17.445+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:35:17.444+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:35:17.582+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:35:18.264+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:35:18.262+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:35:18.446+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:35:18.445+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:35:18.632+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.216 seconds
[2022-12-16T18:35:29.032+0000] {processor.py:154} INFO - Started process (PID=9079) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:35:29.061+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:35:29.067+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:35:29.065+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:35:29.173+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:35:30.593+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:35:30.592+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:35:30.805+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:35:30.804+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:35:31.147+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.145 seconds
[2022-12-16T18:35:41.680+0000] {processor.py:154} INFO - Started process (PID=9089) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:35:41.683+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:35:41.689+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:35:41.688+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:35:41.796+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:35:42.121+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:35:42.120+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:35:42.245+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:35:42.244+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:35:42.399+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.734 seconds
[2022-12-16T18:35:53.216+0000] {processor.py:154} INFO - Started process (PID=9107) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:35:53.262+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:35:53.287+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:35:53.286+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:35:53.521+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:35:53.896+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:35:53.895+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:35:54.104+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:35:54.103+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:35:54.373+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.280 seconds
[2022-12-16T18:36:04.840+0000] {processor.py:154} INFO - Started process (PID=9117) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:36:04.942+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:36:04.946+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:36:04.945+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:36:05.036+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:36:05.250+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:36:05.249+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:36:05.383+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:36:05.382+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:36:05.564+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.748 seconds
[2022-12-16T18:36:15.964+0000] {processor.py:154} INFO - Started process (PID=9127) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:36:15.992+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:36:16.003+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:36:16.002+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:36:16.096+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:36:16.815+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:36:16.814+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:36:17.121+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:36:17.112+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:36:17.405+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.460 seconds
[2022-12-16T18:36:27.735+0000] {processor.py:154} INFO - Started process (PID=9137) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:36:27.774+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:36:27.786+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:36:27.781+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:36:27.872+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:36:29.913+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:36:29.912+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:36:30.291+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:36:30.290+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:36:30.723+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.001 seconds
[2022-12-16T18:36:41.517+0000] {processor.py:154} INFO - Started process (PID=9155) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:36:41.552+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:36:41.556+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:36:41.555+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:36:42.005+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:36:42.538+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:36:42.537+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:36:43.064+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:36:43.063+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:36:43.325+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.918 seconds
[2022-12-16T18:36:53.868+0000] {processor.py:154} INFO - Started process (PID=9165) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:36:53.905+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:36:53.910+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:36:53.908+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:36:53.993+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:36:54.394+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:36:54.393+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:36:54.509+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:36:54.508+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:36:54.642+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.787 seconds
[2022-12-16T18:37:05.467+0000] {processor.py:154} INFO - Started process (PID=9175) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:37:05.497+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:37:05.502+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:37:05.500+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:37:05.620+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:37:06.625+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:37:06.624+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:37:06.846+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:37:06.840+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:37:07.145+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.699 seconds
[2022-12-16T18:37:18.149+0000] {processor.py:154} INFO - Started process (PID=9192) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:37:18.167+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:37:18.179+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:37:18.178+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:37:18.434+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:37:18.778+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:37:18.777+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:37:19.227+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:37:19.220+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:37:19.778+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.677 seconds
[2022-12-16T18:37:30.545+0000] {processor.py:154} INFO - Started process (PID=9203) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:37:30.584+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:37:30.624+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:37:30.623+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:37:31.110+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:37:31.841+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:37:31.837+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:37:32.214+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:37:32.209+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:37:32.494+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.074 seconds
[2022-12-16T18:37:42.787+0000] {processor.py:154} INFO - Started process (PID=9213) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:37:42.791+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:37:42.799+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:37:42.798+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:37:42.999+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:37:43.607+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:37:43.606+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:37:43.743+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:37:43.742+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:37:43.908+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.150 seconds
[2022-12-16T18:37:54.404+0000] {processor.py:154} INFO - Started process (PID=9223) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:37:54.449+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:37:54.455+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:37:54.455+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:37:54.631+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:37:54.967+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:37:54.966+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:37:55.189+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:37:55.188+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:37:55.609+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.238 seconds
[2022-12-16T18:38:06.001+0000] {processor.py:154} INFO - Started process (PID=9233) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:38:06.153+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:38:06.159+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:38:06.156+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:38:06.282+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:38:07.484+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T18:38:18.259+0000] {processor.py:154} INFO - Started process (PID=9250) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:38:18.297+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:38:18.301+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:38:18.300+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:38:18.814+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:38:19.372+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:38:19.371+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:38:19.510+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:38:19.509+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:38:19.625+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.413 seconds
[2022-12-16T18:38:29.964+0000] {processor.py:154} INFO - Started process (PID=9260) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:38:29.990+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:38:29.998+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:38:29.997+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:38:30.099+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:38:30.321+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:38:30.320+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:38:30.549+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:38:30.548+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:38:30.737+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.788 seconds
[2022-12-16T18:38:41.193+0000] {processor.py:154} INFO - Started process (PID=9270) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:38:41.275+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:38:41.280+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:38:41.279+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:38:41.404+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:38:41.542+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:38:41.541+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:38:41.653+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:38:41.652+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:38:41.835+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.664 seconds
[2022-12-16T18:38:52.616+0000] {processor.py:154} INFO - Started process (PID=9287) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:38:52.647+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:38:52.652+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:38:52.651+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:38:52.935+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:38:53.308+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:38:53.307+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:38:53.539+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:38:53.533+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:38:53.871+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.322 seconds
[2022-12-16T18:39:04.434+0000] {processor.py:154} INFO - Started process (PID=9297) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:39:04.445+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:39:04.449+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:39:04.448+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:39:04.581+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:39:05.509+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:39:05.507+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:39:05.659+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:39:05.658+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:39:05.828+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.415 seconds
[2022-12-16T18:39:16.076+0000] {processor.py:154} INFO - Started process (PID=9307) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:39:16.088+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:39:16.098+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:39:16.096+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:39:16.258+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:39:16.580+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:39:16.575+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:39:16.996+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:39:16.995+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:39:17.203+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.160 seconds
[2022-12-16T18:39:27.637+0000] {processor.py:154} INFO - Started process (PID=9317) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:39:27.651+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:39:27.658+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:39:27.657+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:39:27.874+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:39:28.242+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:39:28.241+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:39:28.677+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:39:28.667+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:39:29.011+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.405 seconds
[2022-12-16T18:39:39.607+0000] {processor.py:154} INFO - Started process (PID=9336) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:39:39.634+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:39:39.643+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:39:39.642+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:39:39.817+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:39:40.835+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:39:40.828+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:39:41.201+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:39:41.195+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:39:41.478+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.901 seconds
[2022-12-16T18:39:52.123+0000] {processor.py:154} INFO - Started process (PID=9346) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:39:52.130+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:39:52.139+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:39:52.133+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:39:52.268+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:39:52.500+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:39:52.499+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:39:52.699+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:39:52.698+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:39:52.834+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.757 seconds
[2022-12-16T18:40:03.208+0000] {processor.py:154} INFO - Started process (PID=9356) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:40:03.217+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:40:03.221+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:40:03.220+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:40:03.360+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:40:03.576+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:40:03.575+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:40:03.743+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:40:03.741+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:40:03.944+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.770 seconds
[2022-12-16T18:40:14.444+0000] {processor.py:154} INFO - Started process (PID=9366) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:40:14.493+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:40:14.498+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:40:14.497+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:40:14.684+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:40:15.038+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:40:15.037+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:40:15.252+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:40:15.251+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:40:15.469+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.049 seconds
[2022-12-16T18:40:26.191+0000] {processor.py:154} INFO - Started process (PID=9385) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:40:26.204+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:40:26.208+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:40:26.207+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:40:26.398+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:40:27.475+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T18:40:38.315+0000] {processor.py:154} INFO - Started process (PID=9395) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:40:38.320+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:40:38.324+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:40:38.323+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:40:38.436+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:40:38.607+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:40:38.607+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:40:38.726+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:40:38.725+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:40:38.963+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.675 seconds
[2022-12-16T18:40:49.768+0000] {processor.py:154} INFO - Started process (PID=9405) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:40:49.819+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:40:49.824+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:40:49.823+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:40:50.187+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:40:50.477+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:40:50.476+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:40:50.699+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:40:50.698+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:40:50.918+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.184 seconds
[2022-12-16T18:41:01.293+0000] {processor.py:154} INFO - Started process (PID=9415) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:41:01.316+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:41:01.320+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:41:01.319+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:41:01.434+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:41:01.972+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T18:41:12.630+0000] {processor.py:154} INFO - Started process (PID=9433) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:41:12.667+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:41:12.687+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:41:12.686+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:41:12.961+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:41:14.132+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:41:14.131+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:41:14.676+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:41:14.675+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:41:15.147+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.596 seconds
[2022-12-16T18:41:25.619+0000] {processor.py:154} INFO - Started process (PID=9443) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:41:25.677+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:41:25.681+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:41:25.680+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:41:25.818+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:41:27.081+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:41:27.080+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:41:27.209+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:41:27.208+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:41:27.319+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.714 seconds
[2022-12-16T18:41:37.440+0000] {processor.py:154} INFO - Started process (PID=9453) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:41:37.470+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:41:37.476+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:41:37.475+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:41:37.677+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:41:38.111+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:41:38.110+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:41:38.238+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:41:38.237+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:41:38.384+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.958 seconds
[2022-12-16T18:41:48.877+0000] {processor.py:154} INFO - Started process (PID=9463) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:41:48.925+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:41:48.930+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:41:48.929+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:41:49.028+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:41:49.170+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:41:49.169+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:41:49.282+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:41:49.281+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:41:49.433+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.575 seconds
[2022-12-16T18:42:00.054+0000] {processor.py:154} INFO - Started process (PID=9481) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:42:00.107+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:42:00.119+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:42:00.118+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:42:00.278+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:42:01.013+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:42:01.012+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:42:01.153+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:42:01.152+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:42:01.310+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.304 seconds
[2022-12-16T18:42:11.574+0000] {processor.py:154} INFO - Started process (PID=9491) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:42:11.614+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:42:11.622+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:42:11.621+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:42:11.914+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:42:13.349+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:42:13.348+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:42:13.538+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:42:13.536+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:42:13.734+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.199 seconds
[2022-12-16T18:42:24.042+0000] {processor.py:154} INFO - Started process (PID=9501) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:42:24.066+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:42:24.072+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:42:24.071+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:42:24.151+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:42:24.583+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:42:24.582+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:42:24.948+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:42:24.947+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:42:25.187+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.160 seconds
[2022-12-16T18:42:35.906+0000] {processor.py:154} INFO - Started process (PID=9518) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:42:35.943+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:42:36.009+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:42:35.992+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:42:36.344+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:42:36.919+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:42:36.918+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:42:37.064+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:42:37.063+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:42:37.509+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.692 seconds
[2022-12-16T18:42:48.073+0000] {processor.py:154} INFO - Started process (PID=9529) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:42:48.076+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:42:48.082+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:42:48.080+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:42:48.253+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:42:48.821+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:42:48.819+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:42:49.088+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:42:49.087+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:42:49.328+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.276 seconds
[2022-12-16T18:42:59.648+0000] {processor.py:154} INFO - Started process (PID=9539) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:42:59.673+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:42:59.677+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:42:59.676+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:42:59.916+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:43:00.218+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:43:00.217+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:43:00.397+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:43:00.396+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:43:00.522+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.896 seconds
[2022-12-16T18:43:10.892+0000] {processor.py:154} INFO - Started process (PID=9549) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:43:10.942+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:43:10.946+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:43:10.945+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:43:11.032+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:43:11.329+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:43:11.325+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:43:11.579+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:43:11.578+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:43:11.716+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.851 seconds
[2022-12-16T18:43:22.396+0000] {processor.py:154} INFO - Started process (PID=9567) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:43:22.399+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:43:22.419+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:43:22.418+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:43:22.652+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:43:22.906+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:43:22.905+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:43:23.064+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:43:23.063+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:43:23.475+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.131 seconds
[2022-12-16T18:43:34.019+0000] {processor.py:154} INFO - Started process (PID=9577) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:43:34.044+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:43:34.049+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:43:34.048+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:43:34.129+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:43:34.515+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:43:34.514+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:43:34.635+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:43:34.634+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:43:34.750+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.746 seconds
[2022-12-16T18:43:45.062+0000] {processor.py:154} INFO - Started process (PID=9587) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:43:45.091+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:43:45.096+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:43:45.095+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:43:45.193+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:43:46.038+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-16T18:43:56.311+0000] {processor.py:154} INFO - Started process (PID=9597) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:43:56.337+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:43:56.341+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:43:56.340+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:43:56.563+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:43:57.067+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:43:57.066+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:43:57.224+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:43:57.223+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:43:57.435+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.143 seconds
[2022-12-16T18:44:07.918+0000] {processor.py:154} INFO - Started process (PID=9615) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:44:07.921+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:44:07.926+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:44:07.924+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:44:08.081+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:44:08.982+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:44:08.981+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:44:09.381+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:44:09.380+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:44:09.702+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.801 seconds
[2022-12-16T18:44:20.357+0000] {processor.py:154} INFO - Started process (PID=9625) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:44:20.380+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-16T18:44:20.384+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:44:20.383+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:44:20.568+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-16T18:44:21.227+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:44:21.226+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-16T18:44:21.389+0000] {logging_mixin.py:137} INFO - [2022-12-16T18:44:21.388+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-16T18:44:21.532+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.195 seconds
