[2022-12-11T14:43:07.729+0000] {processor.py:154} INFO - Started process (PID=394) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:43:07.739+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:43:07.752+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:43:07.750+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:43:08.756+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:43:11.055+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:43:11.053+0000] {manager.py:507} INFO - Created Permission View: can delete on DAG:spark_etl
[2022-12-11T14:43:11.143+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:43:11.142+0000] {manager.py:507} INFO - Created Permission View: can read on DAG:spark_etl
[2022-12-11T14:43:11.209+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:43:11.208+0000] {manager.py:507} INFO - Created Permission View: can edit on DAG:spark_etl
[2022-12-11T14:43:11.211+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:43:11.210+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:43:11.318+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:43:11.317+0000] {dag.py:2606} INFO - Creating ORM DAG for spark_etl
[2022-12-11T14:43:11.409+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:43:11.407+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:43:11.596+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.915 seconds
[2022-12-11T14:43:22.019+0000] {processor.py:154} INFO - Started process (PID=399) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:43:22.034+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:43:22.038+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:43:22.037+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:43:22.426+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:43:23.248+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:43:23.246+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:43:23.500+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:43:23.499+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:43:23.681+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.726 seconds
[2022-12-11T14:43:34.319+0000] {processor.py:154} INFO - Started process (PID=412) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:43:34.323+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:43:34.340+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:43:34.339+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:43:34.836+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:43:35.677+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:43:35.676+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:43:36.496+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:43:36.478+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:43:36.769+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.498 seconds
[2022-12-11T14:43:47.078+0000] {processor.py:154} INFO - Started process (PID=417) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:43:47.111+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:43:47.117+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:43:47.116+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:43:47.216+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:43:47.449+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:43:47.448+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:43:47.588+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:43:47.587+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:43:47.703+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.643 seconds
[2022-12-11T14:43:58.036+0000] {processor.py:154} INFO - Started process (PID=422) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:43:58.062+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:43:58.066+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:43:58.065+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:43:58.176+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:43:58.726+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:43:58.725+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:43:59.040+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:43:59.038+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:43:59.220+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.204 seconds
[2022-12-11T14:44:09.588+0000] {processor.py:154} INFO - Started process (PID=427) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:44:09.593+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:44:09.597+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:44:09.596+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:44:09.725+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:44:10.509+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:44:10.507+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:44:10.755+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:44:10.754+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:44:10.874+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.304 seconds
[2022-12-11T14:44:21.387+0000] {processor.py:154} INFO - Started process (PID=440) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:44:21.431+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:44:21.435+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:44:21.434+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:44:21.557+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:44:23.052+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T14:44:33.498+0000] {processor.py:154} INFO - Started process (PID=445) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:44:33.524+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:44:33.531+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:44:33.530+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:44:33.713+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:44:34.680+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:44:34.679+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:44:34.939+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:44:34.935+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:44:35.201+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.726 seconds
[2022-12-11T14:44:45.428+0000] {processor.py:154} INFO - Started process (PID=450) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:44:45.435+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:44:45.440+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:44:45.439+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:44:45.714+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:44:47.306+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T14:44:57.757+0000] {processor.py:154} INFO - Started process (PID=455) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:44:57.793+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:44:57.806+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:44:57.805+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:44:58.060+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:44:58.900+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:44:58.892+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:44:59.322+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:44:59.321+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:44:59.737+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.002 seconds
[2022-12-11T14:45:05.173+0000] {processor.py:154} INFO - Started process (PID=467) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:45:05.189+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:45:05.206+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:45:05.205+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:45:05.441+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:45:06.711+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:45:06.710+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:45:06.968+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:45:06.967+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:45:07.278+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.172 seconds
[2022-12-11T14:45:17.580+0000] {processor.py:154} INFO - Started process (PID=472) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:45:17.584+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:45:17.590+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:45:17.589+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:45:17.704+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:45:17.951+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:45:17.950+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:45:18.149+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:45:18.148+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:45:18.346+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.791 seconds
[2022-12-11T14:45:28.699+0000] {processor.py:154} INFO - Started process (PID=477) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:45:28.724+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:45:28.731+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:45:28.730+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:45:28.838+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:45:30.384+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T14:45:40.814+0000] {processor.py:154} INFO - Started process (PID=482) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:45:40.842+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:45:40.846+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:45:40.845+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:45:40.970+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:45:41.731+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:45:41.730+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:45:42.090+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:45:42.089+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:45:42.396+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.602 seconds
[2022-12-11T14:45:53.309+0000] {processor.py:154} INFO - Started process (PID=494) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:45:53.313+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:45:53.318+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:45:53.317+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:45:53.438+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:45:55.095+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:45:55.094+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:45:56.726+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:45:56.725+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:45:57.837+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 4.577 seconds
[2022-12-11T14:46:09.361+0000] {processor.py:154} INFO - Started process (PID=499) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:46:09.381+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:46:09.438+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:46:09.437+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:46:10.548+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:46:13.559+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:46:13.558+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:46:14.582+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:46:14.571+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:46:15.091+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 5.884 seconds
[2022-12-11T14:46:25.711+0000] {processor.py:154} INFO - Started process (PID=504) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:46:25.715+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:46:25.719+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:46:25.718+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:46:25.809+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:46:26.042+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:46:26.041+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:46:26.230+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:46:26.229+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:46:26.370+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.674 seconds
[2022-12-11T14:46:36.917+0000] {processor.py:154} INFO - Started process (PID=509) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:46:36.957+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:46:36.967+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:46:36.965+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:46:37.401+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:46:38.991+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:46:38.990+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:46:39.394+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:46:39.393+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:46:39.616+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.744 seconds
[2022-12-11T14:46:50.252+0000] {processor.py:154} INFO - Started process (PID=522) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:46:50.276+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:46:50.289+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:46:50.279+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:46:50.592+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:46:52.011+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:46:52.010+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:46:53.186+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:46:53.185+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:46:54.239+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 4.028 seconds
[2022-12-11T14:47:05.303+0000] {processor.py:154} INFO - Started process (PID=527) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:47:05.329+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:47:05.352+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:47:05.343+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:47:05.911+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:47:08.138+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:47:08.124+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:47:08.471+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:47:08.470+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:47:08.731+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.569 seconds
[2022-12-11T14:47:19.229+0000] {processor.py:154} INFO - Started process (PID=532) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:47:19.269+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:47:19.274+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:47:19.273+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:47:19.406+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:47:19.679+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:47:19.678+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:47:19.819+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:47:19.818+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:47:19.958+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.796 seconds
[2022-12-11T14:47:30.840+0000] {processor.py:154} INFO - Started process (PID=538) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:47:30.914+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:47:30.973+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:47:30.972+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:47:31.935+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:47:34.038+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:47:34.036+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:47:34.428+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:47:34.427+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:47:34.675+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 4.065 seconds
[2022-12-11T14:47:45.132+0000] {processor.py:154} INFO - Started process (PID=551) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:47:45.163+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:47:45.168+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:47:45.167+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:47:45.280+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:47:45.534+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:47:45.534+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:47:45.710+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:47:45.709+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:47:45.858+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.743 seconds
[2022-12-11T14:47:56.417+0000] {processor.py:154} INFO - Started process (PID=556) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:47:56.443+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:47:56.449+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:47:56.447+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:47:56.607+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:47:57.176+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:47:57.175+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:47:57.314+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:47:57.313+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:47:57.475+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.078 seconds
[2022-12-11T14:48:08.140+0000] {processor.py:154} INFO - Started process (PID=561) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:48:08.189+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:48:08.198+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:48:08.197+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:48:08.439+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:48:09.003+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:48:09.002+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:48:09.268+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:48:09.259+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:48:09.556+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.459 seconds
[2022-12-11T14:48:20.034+0000] {processor.py:154} INFO - Started process (PID=566) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:48:20.038+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:48:20.044+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:48:20.043+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:48:20.166+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:48:20.605+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:48:20.601+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:48:20.969+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:48:20.968+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:48:21.355+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.345 seconds
[2022-12-11T14:48:32.354+0000] {processor.py:154} INFO - Started process (PID=580) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:48:32.435+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:48:32.443+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:48:32.442+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:48:33.366+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:48:34.608+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:48:34.599+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:48:34.952+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:48:34.947+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:48:35.257+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.076 seconds
[2022-12-11T14:48:45.943+0000] {processor.py:154} INFO - Started process (PID=585) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:48:45.957+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:48:45.962+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:48:45.961+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:48:46.145+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:48:46.649+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:48:46.648+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:48:46.791+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:48:46.790+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:48:46.921+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.034 seconds
[2022-12-11T14:48:57.125+0000] {processor.py:154} INFO - Started process (PID=590) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:48:57.131+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:48:57.138+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:48:57.134+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:48:57.239+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:48:58.034+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T14:49:08.369+0000] {processor.py:154} INFO - Started process (PID=595) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:49:08.391+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:49:08.395+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:49:08.394+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:49:08.498+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:49:09.425+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:49:09.424+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:49:09.685+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:49:09.684+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:49:10.025+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.677 seconds
[2022-12-11T14:49:20.501+0000] {processor.py:154} INFO - Started process (PID=608) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:49:20.524+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:49:20.529+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:49:20.528+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:49:20.640+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:49:20.935+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:49:20.934+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:49:21.195+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:49:21.194+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:49:21.326+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.843 seconds
[2022-12-11T14:49:31.839+0000] {processor.py:154} INFO - Started process (PID=613) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:49:31.842+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:49:31.847+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:49:31.846+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:49:31.996+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:49:33.693+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T14:49:44.141+0000] {processor.py:154} INFO - Started process (PID=618) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:49:44.210+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:49:44.215+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:49:44.214+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:49:44.702+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:49:45.795+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:49:45.794+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:49:46.131+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:49:46.130+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:49:46.357+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.244 seconds
[2022-12-11T14:49:57.414+0000] {processor.py:154} INFO - Started process (PID=632) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:49:57.527+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:49:57.531+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:49:57.530+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:49:58.344+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:50:00.091+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:50:00.090+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:50:00.812+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:50:00.807+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:50:01.163+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.987 seconds
[2022-12-11T14:50:12.106+0000] {processor.py:154} INFO - Started process (PID=638) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:50:12.120+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:50:12.127+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:50:12.123+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:50:12.453+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:50:13.486+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:50:13.485+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:50:13.887+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:50:13.886+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:50:14.181+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.143 seconds
[2022-12-11T14:50:24.590+0000] {processor.py:154} INFO - Started process (PID=644) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:50:24.597+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:50:24.602+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:50:24.601+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:50:24.775+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:50:25.449+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:50:25.447+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:50:25.643+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:50:25.642+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:50:25.813+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.252 seconds
[2022-12-11T14:50:36.296+0000] {processor.py:154} INFO - Started process (PID=649) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:50:36.328+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:50:36.335+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:50:36.333+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:50:36.480+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:50:37.646+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T14:50:48.102+0000] {processor.py:154} INFO - Started process (PID=654) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:50:48.129+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:50:48.139+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:50:48.138+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:50:48.324+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:50:50.190+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:50:50.189+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:50:50.787+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:50:50.786+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:50:51.334+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.259 seconds
[2022-12-11T14:51:01.697+0000] {processor.py:154} INFO - Started process (PID=668) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:51:01.708+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:51:01.715+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:51:01.714+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:51:01.965+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:51:02.579+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:51:02.578+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:51:03.271+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:51:03.270+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:51:03.566+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.912 seconds
[2022-12-11T14:51:14.403+0000] {processor.py:154} INFO - Started process (PID=673) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:51:14.456+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:51:14.483+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:51:14.481+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:51:14.761+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:51:17.337+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:51:17.332+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:51:18.049+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:51:18.039+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:51:18.726+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 4.419 seconds
[2022-12-11T14:51:29.477+0000] {processor.py:154} INFO - Started process (PID=680) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:51:29.493+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:51:29.512+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:51:29.496+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:51:29.868+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:51:31.595+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:51:31.594+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:51:31.888+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:51:31.883+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:51:32.117+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.757 seconds
[2022-12-11T14:51:42.702+0000] {processor.py:154} INFO - Started process (PID=693) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:51:42.798+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:51:42.815+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:51:42.814+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:51:43.206+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:51:43.663+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:51:43.662+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:51:44.002+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:51:44.001+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:51:44.370+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.746 seconds
[2022-12-11T14:51:55.858+0000] {processor.py:154} INFO - Started process (PID=699) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:51:55.947+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:51:56.021+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:51:56.007+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:51:56.986+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:51:59.465+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:51:59.464+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:51:59.930+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:51:59.928+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:52:00.189+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 4.455 seconds
[2022-12-11T14:52:10.453+0000] {processor.py:154} INFO - Started process (PID=704) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:52:10.483+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:52:10.487+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:52:10.486+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:52:10.587+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:52:11.404+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:52:11.403+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:52:11.561+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:52:11.560+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:52:11.690+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.254 seconds
[2022-12-11T14:52:21.901+0000] {processor.py:154} INFO - Started process (PID=709) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:52:21.913+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:52:21.918+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:52:21.917+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:52:22.016+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:52:22.300+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:52:22.295+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:52:22.564+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:52:22.563+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:52:22.713+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.839 seconds
[2022-12-11T14:52:33.378+0000] {processor.py:154} INFO - Started process (PID=721) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:52:33.510+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:52:33.523+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:52:33.521+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:52:33.846+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:52:34.449+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:52:34.448+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:52:34.696+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:52:34.683+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:52:34.845+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.638 seconds
[2022-12-11T14:52:45.622+0000] {processor.py:154} INFO - Started process (PID=727) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:52:45.638+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:52:45.655+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:52:45.654+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:52:46.127+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:52:47.170+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:52:47.169+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:52:47.423+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:52:47.422+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:52:47.597+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.064 seconds
[2022-12-11T14:52:57.980+0000] {processor.py:154} INFO - Started process (PID=732) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:52:58.008+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:52:58.013+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:52:58.011+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:52:58.152+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:52:59.026+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:52:59.025+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:52:59.233+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:52:59.232+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:52:59.391+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.443 seconds
[2022-12-11T14:53:07.509+0000] {processor.py:154} INFO - Started process (PID=742) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:53:07.529+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:53:07.558+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:53:07.546+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:53:07.775+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:53:08.458+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:53:08.457+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:53:08.620+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:53:08.619+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:53:08.741+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.296 seconds
[2022-12-11T14:53:19.438+0000] {processor.py:154} INFO - Started process (PID=755) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:53:19.478+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:53:19.495+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:53:19.494+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:53:20.022+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:53:22.579+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:53:22.577+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:53:22.955+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:53:22.954+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:53:23.677+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 4.434 seconds
[2022-12-11T14:53:34.490+0000] {processor.py:154} INFO - Started process (PID=761) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:53:34.519+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:53:34.523+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:53:34.522+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:53:34.572+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:53:34.962+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:53:34.959+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:53:35.277+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:53:35.274+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:53:35.538+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.095 seconds
[2022-12-11T14:53:46.102+0000] {processor.py:154} INFO - Started process (PID=766) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:53:46.120+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:53:46.124+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:53:46.123+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:53:46.169+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:53:47.334+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T14:53:57.694+0000] {processor.py:154} INFO - Started process (PID=771) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:53:57.723+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:53:57.728+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:53:57.727+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:53:57.768+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:53:58.034+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:53:58.034+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:53:58.182+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:53:58.181+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:53:58.293+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.614 seconds
[2022-12-11T14:54:08.949+0000] {processor.py:154} INFO - Started process (PID=783) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:54:08.993+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:54:09.009+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:54:09.007+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:54:09.098+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:54:09.691+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:54:09.690+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:54:09.861+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:54:09.860+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:54:10.042+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.186 seconds
[2022-12-11T14:54:20.674+0000] {processor.py:154} INFO - Started process (PID=789) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:54:20.715+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:54:20.721+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:54:20.719+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:54:20.812+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:54:22.210+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:54:22.209+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:54:22.326+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:54:22.325+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:54:22.443+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.817 seconds
[2022-12-11T14:54:32.827+0000] {processor.py:154} INFO - Started process (PID=794) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:54:32.859+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:54:32.873+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:54:32.872+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:54:32.950+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:54:33.114+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:54:33.113+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:54:33.247+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:54:33.246+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:54:33.364+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.557 seconds
[2022-12-11T14:54:43.650+0000] {processor.py:154} INFO - Started process (PID=799) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:54:43.675+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:54:43.679+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:54:43.679+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:54:43.757+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:54:44.208+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:54:44.206+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:54:44.566+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:54:44.565+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:54:44.901+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.268 seconds
[2022-12-11T14:54:55.294+0000] {processor.py:154} INFO - Started process (PID=812) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:54:55.321+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:54:55.326+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:54:55.325+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:54:55.374+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:54:55.731+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:54:55.724+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:54:56.583+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:54:56.582+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:54:56.944+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.672 seconds
[2022-12-11T14:55:07.258+0000] {processor.py:154} INFO - Started process (PID=817) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:55:07.263+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:55:07.267+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:55:07.266+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:55:07.306+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:55:08.089+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T14:55:18.558+0000] {processor.py:154} INFO - Started process (PID=822) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:55:18.588+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:55:18.593+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:55:18.591+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:55:18.631+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:55:20.022+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:55:20.021+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:55:20.706+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:55:20.705+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:55:20.999+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.459 seconds
[2022-12-11T14:55:31.390+0000] {processor.py:154} INFO - Started process (PID=827) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:55:31.418+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:55:31.422+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:55:31.421+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:55:31.475+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:55:32.630+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:55:32.629+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:55:33.395+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:55:33.394+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:55:33.595+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.228 seconds
[2022-12-11T14:55:44.161+0000] {processor.py:154} INFO - Started process (PID=840) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:55:44.193+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:55:44.206+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:55:44.205+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:55:44.348+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:55:44.752+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:55:44.751+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:55:45.192+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:55:45.191+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:55:45.477+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.373 seconds
[2022-12-11T14:55:55.831+0000] {processor.py:154} INFO - Started process (PID=845) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:55:55.861+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:55:55.871+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:55:55.869+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:55:55.965+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:55:56.689+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:55:56.687+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:55:56.983+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:55:56.982+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:55:57.185+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.401 seconds
[2022-12-11T14:56:07.489+0000] {processor.py:154} INFO - Started process (PID=850) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:56:07.519+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:56:07.524+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:56:07.523+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:56:07.566+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:56:07.735+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:56:07.734+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:56:08.175+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:56:08.174+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:56:08.632+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.159 seconds
[2022-12-11T14:56:19.186+0000] {processor.py:154} INFO - Started process (PID=863) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:56:19.209+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:56:19.214+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:56:19.213+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:56:19.254+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:56:19.672+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:56:19.671+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:56:20.094+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:56:20.093+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:56:20.367+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.228 seconds
[2022-12-11T14:56:30.992+0000] {processor.py:154} INFO - Started process (PID=868) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:56:31.015+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:56:31.020+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:56:31.018+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:56:31.070+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:56:31.235+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:56:31.234+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:56:31.352+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:56:31.351+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:56:31.472+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.497 seconds
[2022-12-11T14:56:41.653+0000] {processor.py:154} INFO - Started process (PID=873) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:56:41.657+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:56:41.661+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:56:41.660+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:56:41.707+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:56:41.851+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:56:41.851+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:56:41.965+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:56:41.964+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:56:42.083+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.447 seconds
[2022-12-11T14:56:52.452+0000] {processor.py:154} INFO - Started process (PID=878) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:56:52.480+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:56:52.485+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:56:52.484+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:56:52.522+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:56:53.225+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T14:57:03.786+0000] {processor.py:154} INFO - Started process (PID=890) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:57:03.817+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:57:03.821+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:57:03.820+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:57:03.869+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:57:04.302+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:57:04.301+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:57:04.443+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:57:04.442+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:57:04.591+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.823 seconds
[2022-12-11T14:57:15.107+0000] {processor.py:154} INFO - Started process (PID=895) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:57:15.142+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:57:15.155+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:57:15.149+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:57:15.233+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:57:15.398+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:57:15.397+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:57:15.527+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:57:15.525+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:57:15.733+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.643 seconds
[2022-12-11T14:57:26.563+0000] {processor.py:154} INFO - Started process (PID=900) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:57:26.589+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:57:26.604+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:57:26.603+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:57:26.713+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:57:27.955+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T14:57:38.207+0000] {processor.py:154} INFO - Started process (PID=905) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:57:38.244+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:57:38.296+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:57:38.247+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:57:38.516+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:57:39.233+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T14:57:49.458+0000] {processor.py:154} INFO - Started process (PID=917) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:57:49.484+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:57:49.490+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:57:49.489+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:57:49.537+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:57:49.864+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:57:49.846+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:57:50.847+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:57:50.846+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:57:51.086+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.645 seconds
[2022-12-11T14:58:01.694+0000] {processor.py:154} INFO - Started process (PID=922) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:58:01.726+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:58:01.731+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:58:01.730+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:58:01.822+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:58:02.087+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:58:02.086+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:58:02.341+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:58:02.335+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:58:03.076+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.423 seconds
[2022-12-11T14:58:13.549+0000] {processor.py:154} INFO - Started process (PID=927) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:58:13.572+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:58:13.578+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:58:13.576+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:58:13.620+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:58:13.772+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:58:13.771+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:58:13.941+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:58:13.939+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:58:14.161+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.642 seconds
[2022-12-11T14:58:24.597+0000] {processor.py:154} INFO - Started process (PID=940) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:58:24.610+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:58:24.618+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:58:24.616+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:58:24.851+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:58:25.508+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:58:25.507+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:58:25.739+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:58:25.738+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:58:25.874+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.297 seconds
[2022-12-11T14:58:36.831+0000] {processor.py:154} INFO - Started process (PID=945) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:58:36.852+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:58:36.909+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:58:36.908+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:58:37.219+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:58:38.335+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T14:58:49.570+0000] {processor.py:154} INFO - Started process (PID=951) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:58:49.594+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:58:49.598+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:58:49.597+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:58:49.665+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:58:50.814+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:58:50.813+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:58:50.975+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:58:50.974+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:58:51.160+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.650 seconds
[2022-12-11T14:59:01.705+0000] {processor.py:154} INFO - Started process (PID=956) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:59:01.729+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:59:01.739+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:59:01.737+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:59:01.906+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:59:03.415+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T14:59:14.569+0000] {processor.py:154} INFO - Started process (PID=969) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:59:14.589+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:59:14.637+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:59:14.630+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:59:14.894+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:59:16.229+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:59:16.228+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:59:17.295+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:59:17.294+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:59:18.135+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.732 seconds
[2022-12-11T14:59:29.047+0000] {processor.py:154} INFO - Started process (PID=974) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:59:29.055+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:59:29.083+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:59:29.082+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:59:29.197+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:59:30.850+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:59:30.835+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:59:31.603+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:59:31.601+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:59:31.857+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.884 seconds
[2022-12-11T14:59:42.545+0000] {processor.py:154} INFO - Started process (PID=979) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:59:42.557+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:59:42.562+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:59:42.561+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:59:42.639+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:59:43.110+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:59:43.109+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T14:59:43.405+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:59:43.404+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T14:59:43.760+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.254 seconds
[2022-12-11T14:59:54.233+0000] {processor.py:154} INFO - Started process (PID=985) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:59:54.287+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T14:59:54.302+0000] {logging_mixin.py:137} INFO - [2022-12-11T14:59:54.293+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:59:54.501+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T14:59:56.249+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T15:00:07.282+0000] {processor.py:154} INFO - Started process (PID=998) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:00:07.298+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:00:07.330+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:00:07.329+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:00:07.503+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:00:07.880+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:00:07.879+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:00:08.186+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:00:08.185+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:00:08.561+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.385 seconds
[2022-12-11T15:00:19.117+0000] {processor.py:154} INFO - Started process (PID=1003) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:00:19.147+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:00:19.157+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:00:19.156+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:00:19.393+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:00:20.643+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T15:00:31.611+0000] {processor.py:154} INFO - Started process (PID=1008) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:00:31.635+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:00:31.639+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:00:31.638+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:00:31.912+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:00:33.309+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T15:00:43.870+0000] {processor.py:154} INFO - Started process (PID=1013) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:00:43.893+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:00:43.907+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:00:43.906+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:00:44.096+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:00:44.604+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:00:44.599+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:00:44.967+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:00:44.966+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:00:45.257+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.469 seconds
[2022-12-11T15:00:55.690+0000] {processor.py:154} INFO - Started process (PID=1018) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:00:55.695+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:00:55.701+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:00:55.699+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:00:55.748+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:00:57.173+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T15:01:07.471+0000] {processor.py:154} INFO - Started process (PID=1031) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:01:07.481+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:01:07.486+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:01:07.486+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:01:07.559+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:01:07.700+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:01:07.699+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:01:07.819+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:01:07.818+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:01:07.948+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.498 seconds
[2022-12-11T15:01:18.278+0000] {processor.py:154} INFO - Started process (PID=1036) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:01:18.299+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:01:18.306+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:01:18.303+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:01:18.443+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:01:19.294+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:01:19.293+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:01:19.416+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:01:19.415+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:01:19.543+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.287 seconds
[2022-12-11T15:01:29.818+0000] {processor.py:154} INFO - Started process (PID=1041) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:01:29.845+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:01:29.851+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:01:29.850+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:01:29.911+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:01:30.381+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:01:30.380+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:01:30.542+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:01:30.541+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:01:30.718+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.921 seconds
[2022-12-11T15:01:41.230+0000] {processor.py:154} INFO - Started process (PID=1054) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:01:41.246+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:01:41.251+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:01:41.250+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:01:41.299+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:01:41.556+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:01:41.555+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:01:41.815+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:01:41.803+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:01:42.295+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.132 seconds
[2022-12-11T15:01:52.560+0000] {processor.py:154} INFO - Started process (PID=1059) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:01:52.573+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:01:52.579+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:01:52.578+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:01:52.634+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:01:52.887+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:01:52.886+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:01:53.004+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:01:53.003+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:01:53.119+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.576 seconds
[2022-12-11T15:02:03.420+0000] {processor.py:154} INFO - Started process (PID=1064) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:02:03.440+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:02:03.445+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:02:03.444+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:02:03.485+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:02:03.837+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:02:03.836+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:02:03.954+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:02:03.953+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:02:04.064+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.660 seconds
[2022-12-11T15:02:14.404+0000] {processor.py:154} INFO - Started process (PID=1069) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:02:14.413+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:02:14.418+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:02:14.417+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:02:14.464+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:02:15.529+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:02:15.528+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:02:15.726+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:02:15.725+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:02:16.028+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.646 seconds
[2022-12-11T15:02:26.436+0000] {processor.py:154} INFO - Started process (PID=1082) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:02:26.461+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:02:26.464+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:02:26.464+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:02:26.501+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:02:26.647+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:02:26.646+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:02:26.766+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:02:26.765+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:02:26.897+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.480 seconds
[2022-12-11T15:02:37.231+0000] {processor.py:154} INFO - Started process (PID=1087) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:02:37.258+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:02:37.263+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:02:37.262+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:02:37.305+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:02:37.471+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:02:37.470+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:02:37.586+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:02:37.586+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:02:37.696+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.505 seconds
[2022-12-11T15:02:47.957+0000] {processor.py:154} INFO - Started process (PID=1092) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:02:47.962+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:02:47.966+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:02:47.965+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:02:48.008+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:02:49.018+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:02:49.017+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:02:49.139+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:02:49.138+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:02:49.335+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.395 seconds
[2022-12-11T15:02:59.725+0000] {processor.py:154} INFO - Started process (PID=1105) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:02:59.761+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:02:59.769+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:02:59.768+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:02:59.873+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:03:00.303+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:03:00.302+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:03:00.583+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:03:00.582+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:03:00.743+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.042 seconds
[2022-12-11T15:03:11.179+0000] {processor.py:154} INFO - Started process (PID=1110) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:03:11.183+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:03:11.187+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:03:11.186+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:03:11.246+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:03:11.384+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:03:11.383+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:03:11.502+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:03:11.501+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:03:11.613+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.450 seconds
[2022-12-11T15:03:21.880+0000] {processor.py:154} INFO - Started process (PID=1115) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:03:21.910+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:03:21.914+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:03:21.913+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:03:21.955+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:03:22.313+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:03:22.312+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:03:22.433+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:03:22.432+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:03:22.558+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.694 seconds
[2022-12-11T15:03:32.744+0000] {processor.py:154} INFO - Started process (PID=1120) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:03:32.773+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:03:32.778+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:03:32.777+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:03:32.830+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:03:33.078+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:03:33.076+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:03:33.237+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:03:33.235+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:03:33.513+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.805 seconds
[2022-12-11T15:03:44.355+0000] {processor.py:154} INFO - Started process (PID=1133) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:03:44.379+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:03:44.396+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:03:44.394+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:03:44.523+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:03:44.813+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:03:44.812+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:03:45.182+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:03:45.181+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:03:45.444+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.241 seconds
[2022-12-11T15:03:56.766+0000] {processor.py:154} INFO - Started process (PID=1138) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:03:56.847+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:03:56.889+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:03:56.888+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:03:57.077+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:03:58.725+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:03:58.718+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:03:59.073+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:03:59.072+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:03:59.371+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.784 seconds
[2022-12-11T15:04:09.844+0000] {processor.py:154} INFO - Started process (PID=1143) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:04:09.853+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:04:09.857+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:04:09.856+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:04:09.920+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:04:10.161+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:04:10.159+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:04:10.330+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:04:10.327+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:04:10.509+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.682 seconds
[2022-12-11T15:04:20.850+0000] {processor.py:154} INFO - Started process (PID=1148) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:04:20.883+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:04:20.888+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:04:20.887+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:04:20.939+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:04:21.194+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:04:21.193+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:04:21.311+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:04:21.310+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:04:21.421+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.587 seconds
[2022-12-11T15:04:32.002+0000] {processor.py:154} INFO - Started process (PID=1161) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:04:32.043+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:04:32.051+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:04:32.049+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:04:32.219+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:04:32.839+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:04:32.838+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:04:33.133+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:04:33.132+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:04:33.489+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.558 seconds
[2022-12-11T15:04:44.058+0000] {processor.py:154} INFO - Started process (PID=1166) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:04:44.070+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:04:44.082+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:04:44.081+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:04:44.193+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:04:44.831+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:04:44.827+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:04:45.691+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:04:45.690+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:04:46.021+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.016 seconds
[2022-12-11T15:04:56.294+0000] {processor.py:154} INFO - Started process (PID=1171) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:04:56.324+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:04:56.329+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:04:56.328+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:04:56.367+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:04:57.088+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:04:57.087+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:04:57.253+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:04:57.252+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:04:57.436+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.164 seconds
[2022-12-11T15:05:07.574+0000] {processor.py:154} INFO - Started process (PID=1176) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:05:07.582+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:05:07.587+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:05:07.586+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:05:07.635+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:05:08.563+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:05:08.562+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:05:08.929+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:05:08.928+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:05:09.153+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.599 seconds
[2022-12-11T15:05:19.670+0000] {processor.py:154} INFO - Started process (PID=1188) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:05:19.707+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:05:19.713+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:05:19.711+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:05:19.813+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:05:20.303+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:05:20.301+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:05:20.775+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:05:20.774+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:05:21.093+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.460 seconds
[2022-12-11T15:05:31.492+0000] {processor.py:154} INFO - Started process (PID=1193) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:05:31.503+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:05:31.514+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:05:31.513+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:05:31.563+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:05:31.982+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:05:31.981+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:05:32.739+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:05:32.738+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:05:33.246+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.805 seconds
[2022-12-11T15:05:43.731+0000] {processor.py:154} INFO - Started process (PID=1198) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:05:43.764+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:05:43.772+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:05:43.771+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:05:43.833+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:05:44.181+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:05:44.179+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:05:44.542+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:05:44.541+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:05:44.971+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.265 seconds
[2022-12-11T15:05:55.426+0000] {processor.py:154} INFO - Started process (PID=1210) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:05:55.453+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:05:55.472+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:05:55.464+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:05:55.572+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:05:56.826+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:05:56.825+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:05:57.445+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:05:57.435+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:05:57.709+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.342 seconds
[2022-12-11T15:06:08.186+0000] {processor.py:154} INFO - Started process (PID=1216) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:06:08.210+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:06:08.215+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:06:08.213+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:06:08.282+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:06:09.906+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T15:06:20.225+0000] {processor.py:154} INFO - Started process (PID=1221) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:06:20.233+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:06:20.238+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:06:20.237+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:06:20.355+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:06:20.892+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:06:20.891+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:06:21.251+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:06:21.250+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:06:21.672+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.477 seconds
[2022-12-11T15:06:32.065+0000] {processor.py:154} INFO - Started process (PID=1226) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:06:32.093+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:06:32.108+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:06:32.107+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:06:32.180+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:06:32.885+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T15:06:43.567+0000] {processor.py:154} INFO - Started process (PID=1239) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:06:43.610+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:06:43.615+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:06:43.614+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:06:43.737+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:06:44.342+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:06:44.341+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:06:44.715+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:06:44.714+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:06:44.994+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.466 seconds
[2022-12-11T15:06:55.691+0000] {processor.py:154} INFO - Started process (PID=1244) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:06:55.700+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:06:55.705+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:06:55.704+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:06:55.763+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:06:56.506+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:06:56.505+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:06:56.679+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:06:56.679+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:06:56.802+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.150 seconds
[2022-12-11T15:07:07.219+0000] {processor.py:154} INFO - Started process (PID=1249) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:07:07.239+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:07:07.243+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:07:07.242+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:07:07.298+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:07:08.505+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T15:07:18.869+0000] {processor.py:154} INFO - Started process (PID=1254) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:07:18.895+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:07:18.900+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:07:18.899+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:07:18.959+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:07:19.238+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:07:19.237+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:07:19.614+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:07:19.613+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:07:20.085+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.242 seconds
[2022-12-11T15:07:30.865+0000] {processor.py:154} INFO - Started process (PID=1266) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:07:30.889+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:07:30.898+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:07:30.897+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:07:31.018+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:07:31.409+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:07:31.408+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:07:31.743+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:07:31.742+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:07:32.027+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.217 seconds
[2022-12-11T15:07:42.534+0000] {processor.py:154} INFO - Started process (PID=1271) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:07:42.554+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:07:42.561+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:07:42.560+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:07:42.609+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:07:42.847+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:07:42.846+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:07:43.090+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:07:43.089+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:07:43.277+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.771 seconds
[2022-12-11T15:07:53.637+0000] {processor.py:154} INFO - Started process (PID=1276) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:07:53.666+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:07:53.671+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:07:53.670+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:07:53.736+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:07:54.845+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:07:54.843+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:07:55.031+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:07:55.030+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:07:55.174+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.564 seconds
[2022-12-11T15:08:05.586+0000] {processor.py:154} INFO - Started process (PID=1281) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:08:05.609+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:08:05.614+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:08:05.613+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:08:05.653+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:08:06.043+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:08:06.041+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:08:06.210+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:08:06.209+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:08:06.359+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.789 seconds
[2022-12-11T15:08:17.274+0000] {processor.py:154} INFO - Started process (PID=1299) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:08:17.295+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:08:17.321+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:08:17.315+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:08:17.508+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:08:17.939+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:08:17.938+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:08:18.430+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:08:18.426+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:08:18.979+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.782 seconds
[2022-12-11T15:08:29.933+0000] {processor.py:154} INFO - Started process (PID=1309) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:08:29.973+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:08:30.011+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:08:30.010+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:08:30.275+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:08:31.780+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T15:08:42.051+0000] {processor.py:154} INFO - Started process (PID=1319) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:08:42.079+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:08:42.088+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:08:42.087+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:08:42.158+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:08:42.415+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:08:42.414+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:08:42.682+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:08:42.681+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:08:42.934+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.907 seconds
[2022-12-11T15:08:53.485+0000] {processor.py:154} INFO - Started process (PID=1329) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:08:53.518+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:08:53.523+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:08:53.522+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:08:53.618+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:08:53.805+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:08:53.803+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:08:53.964+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:08:53.961+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:08:54.145+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.784 seconds
[2022-12-11T15:09:04.708+0000] {processor.py:154} INFO - Started process (PID=1347) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:09:04.891+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:09:04.895+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:09:04.894+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:09:05.055+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:09:05.821+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:09:05.808+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:09:06.347+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:09:06.346+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:09:06.618+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.971 seconds
[2022-12-11T15:09:17.519+0000] {processor.py:154} INFO - Started process (PID=1357) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:09:17.530+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:09:17.536+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:09:17.535+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:09:17.776+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:09:18.783+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:09:18.782+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:09:19.142+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:09:19.141+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:09:19.351+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.870 seconds
[2022-12-11T15:09:29.724+0000] {processor.py:154} INFO - Started process (PID=1367) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:09:29.731+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:09:29.740+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:09:29.739+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:09:29.795+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:09:30.101+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:09:30.100+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:09:30.643+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:09:30.642+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:09:31.066+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.376 seconds
[2022-12-11T15:09:41.590+0000] {processor.py:154} INFO - Started process (PID=1377) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:09:41.614+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:09:41.619+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:09:41.617+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:09:41.680+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:09:42.162+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:09:42.161+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:09:42.446+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:09:42.445+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:09:42.709+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.147 seconds
[2022-12-11T15:09:53.098+0000] {processor.py:154} INFO - Started process (PID=1395) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:09:53.135+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:09:53.143+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:09:53.138+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:09:53.255+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:09:54.009+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:09:54.008+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:09:54.472+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:09:54.466+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:09:54.961+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.926 seconds
[2022-12-11T15:10:05.879+0000] {processor.py:154} INFO - Started process (PID=1405) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:10:05.886+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:10:05.892+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:10:05.891+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:10:06.037+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:10:06.438+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:10:06.437+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:10:06.619+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:10:06.618+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:10:06.766+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.918 seconds
[2022-12-11T15:10:17.587+0000] {processor.py:154} INFO - Started process (PID=1417) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:10:17.609+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:10:17.613+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:10:17.612+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:10:17.706+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:10:18.092+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:10:18.087+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:10:18.406+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:10:18.405+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:10:18.634+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.077 seconds
[2022-12-11T15:10:29.278+0000] {processor.py:154} INFO - Started process (PID=1425) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:10:29.305+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:10:29.310+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:10:29.309+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:10:29.375+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:10:29.638+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:10:29.637+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:10:29.964+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:10:29.963+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:10:30.307+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.050 seconds
[2022-12-11T15:10:41.613+0000] {processor.py:154} INFO - Started process (PID=1444) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:10:41.637+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:10:41.712+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:10:41.683+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:10:42.405+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:10:45.398+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:10:45.365+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:10:46.356+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:10:46.355+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:10:46.749+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 5.283 seconds
[2022-12-11T15:10:57.889+0000] {processor.py:154} INFO - Started process (PID=1455) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:10:57.907+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:10:57.947+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:10:57.918+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:10:58.215+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:10:59.442+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T15:11:10.434+0000] {processor.py:154} INFO - Started process (PID=1465) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:11:10.501+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:11:10.521+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:11:10.520+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:11:10.915+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:11:11.519+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:11:11.511+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:11:12.257+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:11:12.227+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:11:12.941+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.689 seconds
[2022-12-11T15:11:23.497+0000] {processor.py:154} INFO - Started process (PID=1472) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:11:23.528+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:11:23.533+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:11:23.531+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:11:23.573+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:11:23.792+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:11:23.791+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:11:23.948+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:11:23.947+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:11:24.174+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.692 seconds
[2022-12-11T15:11:34.614+0000] {processor.py:154} INFO - Started process (PID=1489) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:11:34.648+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:11:34.670+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:11:34.651+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:11:34.880+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:11:36.882+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:11:36.875+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:11:37.377+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:11:37.367+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:11:37.734+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.197 seconds
[2022-12-11T15:11:48.118+0000] {processor.py:154} INFO - Started process (PID=1500) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:11:48.153+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:11:48.173+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:11:48.163+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:11:48.427+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:11:50.177+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T15:12:01.079+0000] {processor.py:154} INFO - Started process (PID=1511) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:12:01.083+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:12:01.087+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:12:01.086+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:12:01.127+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:12:01.277+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:12:01.276+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:12:01.424+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:12:01.423+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:12:01.577+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.514 seconds
[2022-12-11T15:12:12.109+0000] {processor.py:154} INFO - Started process (PID=1521) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:12:12.229+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:12:12.238+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:12:12.237+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:12:12.362+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:12:14.273+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:12:14.263+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:12:14.748+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:12:14.743+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:12:14.941+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.920 seconds
[2022-12-11T15:12:26.676+0000] {processor.py:154} INFO - Started process (PID=1540) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:12:26.765+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:12:26.829+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:12:26.819+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:12:27.088+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:12:28.853+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:12:28.852+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:12:29.922+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:12:29.921+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:12:30.365+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.869 seconds
[2022-12-11T15:12:41.003+0000] {processor.py:154} INFO - Started process (PID=1550) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:12:41.045+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:12:41.054+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:12:41.053+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:12:41.194+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:12:42.304+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:12:42.291+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:12:42.768+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:12:42.758+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:12:43.209+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.252 seconds
[2022-12-11T15:12:54.591+0000] {processor.py:154} INFO - Started process (PID=1564) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:12:54.693+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:12:54.738+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:12:54.737+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:12:55.073+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:12:56.607+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:12:56.605+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:12:57.446+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:12:57.440+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:12:57.825+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.442 seconds
[2022-12-11T15:13:09.375+0000] {processor.py:154} INFO - Started process (PID=1575) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:13:09.393+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:13:09.421+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:13:09.420+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:13:09.701+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:13:10.936+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:13:10.935+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:13:11.494+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:13:11.493+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:13:11.814+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.503 seconds
[2022-12-11T15:13:22.663+0000] {processor.py:154} INFO - Started process (PID=1594) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:13:22.696+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:13:22.706+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:13:22.705+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:13:23.212+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:13:23.892+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:13:23.891+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:13:24.317+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:13:24.300+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:13:24.649+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.133 seconds
[2022-12-11T15:13:35.042+0000] {processor.py:154} INFO - Started process (PID=1604) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:13:35.057+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:13:35.062+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:13:35.061+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:13:35.272+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:13:35.495+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:13:35.494+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:13:35.611+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:13:35.611+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:13:35.740+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.735 seconds
[2022-12-11T15:13:46.854+0000] {processor.py:154} INFO - Started process (PID=1614) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:13:46.878+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:13:46.891+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:13:46.890+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:13:47.164+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:13:47.651+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:13:47.647+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:13:47.859+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:13:47.858+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:13:48.000+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.295 seconds
[2022-12-11T15:13:58.801+0000] {processor.py:154} INFO - Started process (PID=1624) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:13:58.824+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:13:58.835+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:13:58.834+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:13:59.095+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:14:00.431+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:14:00.429+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:14:01.148+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:14:01.143+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:14:01.937+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.198 seconds
[2022-12-11T15:14:13.572+0000] {processor.py:154} INFO - Started process (PID=1644) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:14:13.597+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:14:13.643+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:14:13.628+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:14:14.011+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:14:14.892+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:14:14.891+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:14:16.048+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:14:15.999+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:14:16.781+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.445 seconds
[2022-12-11T15:14:28.471+0000] {processor.py:154} INFO - Started process (PID=1654) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:14:28.526+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:14:28.530+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:14:28.529+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:14:28.903+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:14:32.231+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:14:32.230+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:14:33.698+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:14:33.697+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:14:34.571+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 6.241 seconds
[2022-12-11T15:14:45.555+0000] {processor.py:154} INFO - Started process (PID=1667) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:14:45.567+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:14:45.572+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:14:45.571+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:14:45.671+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:14:47.219+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:14:47.218+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:14:47.627+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:14:47.626+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:14:47.837+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.313 seconds
[2022-12-11T15:14:58.247+0000] {processor.py:154} INFO - Started process (PID=1677) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:14:58.285+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:14:58.297+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:14:58.296+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:14:58.439+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:14:59.121+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:14:59.120+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:14:59.361+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:14:59.360+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:14:59.535+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.332 seconds
[2022-12-11T15:15:10.526+0000] {processor.py:154} INFO - Started process (PID=1695) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:15:10.561+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:15:10.580+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:15:10.573+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:15:10.860+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:15:11.975+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:15:11.974+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:15:12.602+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:15:12.601+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:15:12.898+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.451 seconds
[2022-12-11T15:15:23.862+0000] {processor.py:154} INFO - Started process (PID=1706) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:15:23.887+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:15:23.891+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:15:23.890+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:15:24.034+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:15:24.415+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:15:24.414+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:15:24.734+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:15:24.733+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:15:24.958+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.151 seconds
[2022-12-11T15:15:35.285+0000] {processor.py:154} INFO - Started process (PID=1716) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:15:35.361+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:15:35.385+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:15:35.384+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:15:35.635+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:15:36.422+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:15:36.420+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:15:36.699+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:15:36.698+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:15:36.905+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.660 seconds
[2022-12-11T15:15:48.010+0000] {processor.py:154} INFO - Started process (PID=1726) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:15:48.050+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:15:48.054+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:15:48.053+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:15:48.351+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:15:50.378+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:15:50.377+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:15:50.790+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:15:50.789+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:15:51.141+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.224 seconds
[2022-12-11T15:16:01.759+0000] {processor.py:154} INFO - Started process (PID=1744) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:16:01.791+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:16:01.816+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:16:01.806+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:16:02.052+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:16:03.259+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:16:03.257+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:16:03.438+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:16:03.437+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:16:03.583+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.845 seconds
[2022-12-11T15:16:15.065+0000] {processor.py:154} INFO - Started process (PID=1755) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:16:15.089+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:16:15.114+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:16:15.113+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:16:15.608+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:16:17.647+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:16:17.636+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:16:18.564+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:16:18.563+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:16:18.858+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.980 seconds
[2022-12-11T15:16:29.388+0000] {processor.py:154} INFO - Started process (PID=1765) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:16:29.406+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:16:29.418+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:16:29.416+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:16:29.513+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:16:29.806+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:16:29.805+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:16:29.925+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:16:29.924+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:16:30.042+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.676 seconds
[2022-12-11T15:16:40.503+0000] {processor.py:154} INFO - Started process (PID=1775) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:16:40.524+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:16:40.529+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:16:40.528+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:16:40.588+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:16:40.775+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:16:40.774+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:16:41.018+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:16:41.017+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:16:41.440+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.953 seconds
[2022-12-11T15:16:51.978+0000] {processor.py:154} INFO - Started process (PID=1792) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:16:52.013+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:16:52.022+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:16:52.021+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:16:52.197+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:16:53.887+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:16:53.886+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:16:56.079+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:16:56.078+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:16:58.456+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 6.552 seconds
[2022-12-11T15:17:10.070+0000] {processor.py:154} INFO - Started process (PID=1807) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:17:10.106+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:17:10.111+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:17:10.110+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:17:10.240+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:17:10.783+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:17:10.782+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:17:11.359+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:17:11.358+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:17:11.731+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.724 seconds
[2022-12-11T15:17:22.749+0000] {processor.py:154} INFO - Started process (PID=1817) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:17:22.762+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:17:22.772+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:17:22.770+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:17:22.904+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:17:23.974+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:17:23.973+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:17:24.235+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:17:24.234+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:17:24.504+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.840 seconds
[2022-12-11T15:17:35.329+0000] {processor.py:154} INFO - Started process (PID=1827) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:17:35.336+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:17:35.357+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:17:35.356+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:17:35.485+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:17:35.830+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:17:35.829+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:17:36.055+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:17:36.054+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:17:36.226+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.942 seconds
[2022-12-11T15:17:47.242+0000] {processor.py:154} INFO - Started process (PID=1842) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:17:47.308+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:17:47.350+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:17:47.311+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:17:47.796+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:17:48.950+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:17:48.943+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:17:49.556+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:17:49.555+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:17:50.039+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.860 seconds
[2022-12-11T15:18:00.711+0000] {processor.py:154} INFO - Started process (PID=1861) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:18:00.780+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:18:00.799+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:18:00.783+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:18:01.146+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:18:03.483+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:18:03.466+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:18:04.023+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:18:04.022+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:18:04.481+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.862 seconds
[2022-12-11T15:18:15.711+0000] {processor.py:154} INFO - Started process (PID=1872) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:18:15.729+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:18:15.750+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:18:15.748+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:18:15.915+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:18:16.352+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:18:16.351+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:18:16.775+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:18:16.774+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:18:17.078+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.410 seconds
[2022-12-11T15:18:27.547+0000] {processor.py:154} INFO - Started process (PID=1882) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:18:27.569+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:18:27.573+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:18:27.572+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:18:27.638+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:18:27.862+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:18:27.861+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:18:28.025+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:18:28.024+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:18:28.235+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.715 seconds
[2022-12-11T15:18:39.266+0000] {processor.py:154} INFO - Started process (PID=1893) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:18:39.308+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:18:39.352+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:18:39.311+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:18:39.998+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:18:41.410+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:18:41.409+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:18:41.962+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:18:41.961+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:18:42.435+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.386 seconds
[2022-12-11T15:18:53.390+0000] {processor.py:154} INFO - Started process (PID=1909) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:18:53.408+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:18:53.434+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:18:53.411+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:18:53.622+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:18:54.790+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:18:54.789+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:18:55.186+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:18:55.185+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:18:55.595+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.290 seconds
[2022-12-11T15:19:06.399+0000] {processor.py:154} INFO - Started process (PID=1920) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:19:06.464+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:19:06.478+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:19:06.477+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:19:06.752+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:19:08.112+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T15:19:19.317+0000] {processor.py:154} INFO - Started process (PID=1930) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:19:19.325+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:19:19.338+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:19:19.337+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:19:19.502+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:19:20.568+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:19:20.555+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:19:21.017+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:19:21.011+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:19:21.321+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.085 seconds
[2022-12-11T15:19:31.847+0000] {processor.py:154} INFO - Started process (PID=1940) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:19:31.871+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:19:31.879+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:19:31.875+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:19:31.952+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:19:32.193+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:19:32.191+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:19:32.452+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:19:32.451+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:19:32.563+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.746 seconds
[2022-12-11T15:19:43.708+0000] {processor.py:154} INFO - Started process (PID=1950) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:19:43.746+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:19:43.762+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:19:43.760+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:19:43.871+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:19:45.114+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:19:45.097+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:19:45.559+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:19:45.558+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:19:46.187+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.625 seconds
[2022-12-11T15:19:57.209+0000] {processor.py:154} INFO - Started process (PID=1968) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:19:57.240+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:19:57.270+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:19:57.261+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:19:57.357+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:19:57.542+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:19:57.541+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:19:57.750+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:19:57.749+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:19:58.180+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.010 seconds
[2022-12-11T15:20:09.043+0000] {processor.py:154} INFO - Started process (PID=1978) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:20:09.061+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:20:09.079+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:20:09.078+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:20:09.419+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:20:09.984+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:20:09.983+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:20:10.280+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:20:10.279+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:20:10.533+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.683 seconds
[2022-12-11T15:20:21.481+0000] {processor.py:154} INFO - Started process (PID=1988) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:20:21.505+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:20:21.520+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:20:21.518+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:20:21.631+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:20:22.590+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:20:22.590+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:20:23.023+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:20:23.022+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:20:23.540+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.160 seconds
[2022-12-11T15:20:34.076+0000] {processor.py:154} INFO - Started process (PID=1998) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:20:34.101+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:20:34.106+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:20:34.105+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:20:34.147+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:20:34.299+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:20:34.298+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:20:34.452+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:20:34.451+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:20:34.602+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.546 seconds
[2022-12-11T15:20:45.203+0000] {processor.py:154} INFO - Started process (PID=2015) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:20:45.247+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:20:45.265+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:20:45.256+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:20:45.366+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:20:45.844+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:20:45.835+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:20:46.133+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:20:46.132+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:20:46.418+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.317 seconds
[2022-12-11T15:20:56.921+0000] {processor.py:154} INFO - Started process (PID=2026) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:20:56.954+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:20:56.963+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:20:56.962+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:20:57.113+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:20:57.832+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:20:57.831+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:20:57.957+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:20:57.957+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:20:58.103+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.239 seconds
[2022-12-11T15:21:08.585+0000] {processor.py:154} INFO - Started process (PID=2036) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:21:08.604+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:21:08.612+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:21:08.607+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:21:08.752+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:21:09.157+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:21:09.156+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:21:09.387+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:21:09.386+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:21:09.813+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.261 seconds
[2022-12-11T15:21:20.385+0000] {processor.py:154} INFO - Started process (PID=2046) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:21:20.417+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:21:20.422+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:21:20.421+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:21:20.533+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:21:20.839+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:21:20.838+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:21:21.110+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:21:21.109+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:21:21.436+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.152 seconds
[2022-12-11T15:21:32.521+0000] {processor.py:154} INFO - Started process (PID=2064) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:21:32.527+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:21:32.531+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:21:32.530+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:21:32.675+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:21:33.075+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:21:33.071+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:21:33.394+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:21:33.393+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:21:33.900+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.420 seconds
[2022-12-11T15:21:45.510+0000] {processor.py:154} INFO - Started process (PID=2074) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:21:45.532+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:21:45.550+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:21:45.537+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:21:45.711+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:21:45.980+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:21:45.979+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:21:46.805+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:21:46.804+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:21:47.526+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.076 seconds
[2022-12-11T15:21:58.083+0000] {processor.py:154} INFO - Started process (PID=2084) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:21:58.108+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:21:58.115+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:21:58.110+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:21:58.177+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:21:58.750+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:21:58.749+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:21:59.078+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:21:59.071+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:21:59.379+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.323 seconds
[2022-12-11T15:22:10.328+0000] {processor.py:154} INFO - Started process (PID=2093) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:22:10.362+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:22:10.386+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:22:10.385+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:22:10.858+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:22:11.458+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:22:11.457+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:22:12.130+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:22:12.103+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:22:12.771+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.520 seconds
[2022-12-11T15:22:23.358+0000] {processor.py:154} INFO - Started process (PID=2112) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:22:23.390+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:22:23.398+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:22:23.397+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:22:23.667+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:22:24.531+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:22:24.530+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:22:24.868+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:22:24.867+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:22:25.157+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.817 seconds
[2022-12-11T15:22:35.979+0000] {processor.py:154} INFO - Started process (PID=2123) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:22:36.012+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:22:36.019+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:22:36.018+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:22:36.409+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:22:37.084+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:22:37.083+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:22:37.804+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:22:37.803+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:22:38.229+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.339 seconds
[2022-12-11T15:22:49.208+0000] {processor.py:154} INFO - Started process (PID=2134) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:22:49.220+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:22:49.248+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:22:49.234+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:22:49.433+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:22:50.876+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:22:50.875+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:22:51.585+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:22:51.584+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:22:51.993+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.861 seconds
[2022-12-11T15:23:02.508+0000] {processor.py:154} INFO - Started process (PID=2144) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:23:02.513+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:23:02.520+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:23:02.518+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:23:02.568+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:23:03.510+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:23:03.509+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:23:03.713+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:23:03.711+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:23:03.848+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.357 seconds
[2022-12-11T15:23:14.162+0000] {processor.py:154} INFO - Started process (PID=2154) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:23:14.193+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:23:14.197+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:23:14.196+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:23:14.238+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:23:15.928+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:23:15.927+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:23:16.128+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:23:16.126+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:23:16.306+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.161 seconds
[2022-12-11T15:23:27.354+0000] {processor.py:154} INFO - Started process (PID=2172) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:23:27.476+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:23:27.485+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:23:27.484+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:23:27.658+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:23:28.673+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:23:28.623+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:23:30.383+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:23:30.286+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:23:33.693+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 6.479 seconds
[2022-12-11T15:23:44.640+0000] {processor.py:154} INFO - Started process (PID=2186) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:23:44.644+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:23:44.648+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:23:44.647+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:23:44.688+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:23:45.347+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:23:45.346+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:23:45.569+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:23:45.568+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:23:45.701+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.077 seconds
[2022-12-11T15:23:56.339+0000] {processor.py:154} INFO - Started process (PID=2196) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:23:56.355+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:23:56.359+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:23:56.358+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:23:56.438+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:23:57.102+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:23:57.101+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:23:57.771+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:23:57.759+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:23:58.736+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.428 seconds
[2022-12-11T15:24:09.821+0000] {processor.py:154} INFO - Started process (PID=2208) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:24:09.854+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:24:09.871+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:24:09.870+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:24:09.937+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:24:11.479+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T15:24:21.198+0000] {processor.py:154} INFO - Started process (PID=2225) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:24:21.244+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:24:21.260+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:24:21.247+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:24:21.506+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:24:22.435+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:24:22.434+0000] {taskinstance.py:1853} ERROR - {'DAG Id': 'spark_etl', 'Task Id': 'mysql_to_postgres', 'Run Id': 'manual__2022-12-11T14:59:51.957888+00:00', 'Hostname': '90ebd458857e'}
[2022-12-11T15:24:23.089+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:24:23.088+0000] {taskinstance.py:1401} INFO - Marking task as FAILED. dag_id=spark_etl, task_id=mysql_to_postgres, execution_date=20221211T145951, start_date=20221211T152407, end_date=20221211T152422
[2022-12-11T15:24:23.325+0000] {processor.py:725} INFO - Executed failure callback for <TaskInstance: spark_etl.mysql_to_postgres manual__2022-12-11T14:59:51.957888+00:00 [failed]> in state failed
[2022-12-11T15:24:23.622+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:24:23.615+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:24:24.165+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:24:24.159+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:24:24.493+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.397 seconds
[2022-12-11T15:24:35.201+0000] {processor.py:154} INFO - Started process (PID=2236) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:24:35.244+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:24:35.281+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:24:35.247+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:24:35.680+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:24:37.395+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:24:37.394+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:24:38.418+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:24:38.417+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:24:39.190+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 4.191 seconds
[2022-12-11T15:24:50.408+0000] {processor.py:154} INFO - Started process (PID=2246) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:24:50.418+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:24:50.423+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:24:50.422+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:24:50.491+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:24:50.722+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:24:50.721+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:24:50.929+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:24:50.928+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:24:51.330+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.949 seconds
[2022-12-11T15:25:01.815+0000] {processor.py:154} INFO - Started process (PID=2256) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:25:01.830+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:25:01.842+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:25:01.841+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:25:02.290+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:25:02.844+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:25:02.839+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:25:03.205+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:25:03.195+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:25:03.569+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.842 seconds
[2022-12-11T15:25:13.759+0000] {processor.py:154} INFO - Started process (PID=2266) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:25:13.780+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:25:13.784+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:25:13.783+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:25:13.830+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:25:14.185+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:25:14.184+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:25:14.412+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:25:14.411+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:25:14.568+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.826 seconds
[2022-12-11T15:25:24.830+0000] {processor.py:154} INFO - Started process (PID=2284) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:25:24.868+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:25:24.878+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:25:24.877+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:25:25.037+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:25:25.535+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:25:25.534+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:25:26.029+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:25:26.028+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:25:27.320+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.530 seconds
[2022-12-11T15:25:38.335+0000] {processor.py:154} INFO - Started process (PID=2294) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:25:38.365+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:25:38.382+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:25:38.381+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:25:38.604+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:25:39.226+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T15:25:49.769+0000] {processor.py:154} INFO - Started process (PID=2304) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:25:49.785+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:25:49.790+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:25:49.789+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:25:49.835+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:25:50.011+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:25:50.010+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:25:50.165+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:25:50.164+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:25:50.273+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.523 seconds
[2022-12-11T15:26:00.561+0000] {processor.py:154} INFO - Started process (PID=2314) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:26:00.591+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:26:00.596+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:26:00.595+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:26:00.674+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:26:01.068+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:26:01.058+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:26:01.425+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:26:01.424+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:26:01.607+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.077 seconds
[2022-12-11T15:26:11.919+0000] {processor.py:154} INFO - Started process (PID=2331) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:26:11.937+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:26:11.948+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:26:11.947+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:26:12.067+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:26:12.383+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:26:12.382+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:26:12.543+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:26:12.542+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:26:12.769+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.885 seconds
[2022-12-11T15:26:23.578+0000] {processor.py:154} INFO - Started process (PID=2342) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:26:23.621+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:26:23.641+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:26:23.640+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:26:23.826+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:26:24.785+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:26:24.784+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:26:25.114+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:26:25.113+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:26:25.394+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.904 seconds
[2022-12-11T15:26:36.422+0000] {processor.py:154} INFO - Started process (PID=2352) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:26:36.426+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:26:36.430+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:26:36.429+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:26:36.487+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:26:36.949+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:26:36.948+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:26:37.125+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:26:37.122+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:26:37.295+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.889 seconds
[2022-12-11T15:26:47.683+0000] {processor.py:154} INFO - Started process (PID=2362) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:26:47.704+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:26:47.709+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:26:47.708+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:26:47.811+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:26:47.979+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:26:47.977+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:26:48.096+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:26:48.095+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:26:48.252+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.598 seconds
[2022-12-11T15:26:58.663+0000] {processor.py:154} INFO - Started process (PID=2380) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:26:58.674+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:26:58.684+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:26:58.681+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:26:58.768+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:26:59.526+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:26:59.525+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:26:59.767+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:26:59.766+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:27:00.013+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.395 seconds
[2022-12-11T15:27:10.467+0000] {processor.py:154} INFO - Started process (PID=2391) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:27:10.496+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:27:10.500+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:27:10.499+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:27:10.542+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:27:10.698+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:27:10.697+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:27:10.821+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:27:10.821+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:27:11.014+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.563 seconds
[2022-12-11T15:27:21.592+0000] {processor.py:154} INFO - Started process (PID=2401) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:27:21.597+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:27:21.602+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:27:21.601+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:27:21.654+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:27:21.856+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:27:21.855+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:27:22.010+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:27:22.009+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:27:22.140+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.566 seconds
[2022-12-11T15:27:32.506+0000] {processor.py:154} INFO - Started process (PID=2411) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:27:32.521+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:27:32.525+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:27:32.524+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:27:32.563+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:27:32.992+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:27:32.980+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:27:33.231+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:27:33.230+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:27:33.382+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.893 seconds
[2022-12-11T15:27:44.420+0000] {processor.py:154} INFO - Started process (PID=2429) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:27:44.450+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:27:44.455+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:27:44.453+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:27:44.500+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:27:45.301+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:27:45.300+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:27:45.608+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:27:45.603+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:27:45.831+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.429 seconds
[2022-12-11T15:27:56.993+0000] {processor.py:154} INFO - Started process (PID=2439) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:27:57.006+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:27:57.021+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:27:57.009+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:27:57.114+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:27:57.404+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:27:57.402+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:27:57.600+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:27:57.599+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:27:57.773+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.843 seconds
[2022-12-11T15:28:08.294+0000] {processor.py:154} INFO - Started process (PID=2449) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:28:08.298+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:28:08.305+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:28:08.303+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:28:08.353+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:28:09.559+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T15:28:20.086+0000] {processor.py:154} INFO - Started process (PID=2459) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:28:20.090+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:28:20.095+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:28:20.093+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:28:20.176+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:28:20.764+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:28:20.763+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:28:20.885+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:28:20.884+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:28:21.029+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.961 seconds
[2022-12-11T15:28:31.491+0000] {processor.py:154} INFO - Started process (PID=2478) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:28:31.520+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:28:31.533+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:28:31.523+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:28:31.646+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:28:31.987+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:28:31.986+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:28:32.283+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:28:32.283+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:28:32.650+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.218 seconds
[2022-12-11T15:28:43.029+0000] {processor.py:154} INFO - Started process (PID=2488) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:28:43.033+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:28:43.038+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:28:43.037+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:28:43.088+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:28:43.302+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:28:43.301+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:28:43.459+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:28:43.458+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:28:43.608+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.598 seconds
[2022-12-11T15:28:54.242+0000] {processor.py:154} INFO - Started process (PID=2498) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:28:54.263+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:28:54.271+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:28:54.270+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:28:54.316+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:28:54.752+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:28:54.751+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:28:54.909+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:28:54.908+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:28:55.053+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.828 seconds
[2022-12-11T15:29:05.967+0000] {processor.py:154} INFO - Started process (PID=2514) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:29:05.980+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:29:05.986+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:29:05.983+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:29:06.052+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:29:06.443+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:29:06.442+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:29:06.618+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:29:06.617+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:29:06.816+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.869 seconds
[2022-12-11T15:29:17.415+0000] {processor.py:154} INFO - Started process (PID=2525) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:29:17.422+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:29:17.426+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:29:17.425+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:29:17.466+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:29:18.478+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T15:29:28.684+0000] {processor.py:154} INFO - Started process (PID=2535) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:29:28.705+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:29:28.709+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:29:28.708+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:29:28.748+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:29:28.889+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:29:28.889+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:29:29.028+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:29:29.027+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:29:29.164+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.496 seconds
[2022-12-11T15:29:40.081+0000] {processor.py:154} INFO - Started process (PID=2545) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:29:40.085+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:29:40.092+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:29:40.089+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:29:40.162+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:29:41.321+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T15:29:51.812+0000] {processor.py:154} INFO - Started process (PID=2563) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:29:51.828+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:29:51.849+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:29:51.846+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:29:51.949+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:29:52.725+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:29:52.724+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:29:52.889+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:29:52.888+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:29:53.017+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.258 seconds
[2022-12-11T15:30:03.656+0000] {processor.py:154} INFO - Started process (PID=2573) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:30:03.687+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:30:03.691+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:30:03.690+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:30:03.880+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:30:04.153+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:30:04.152+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:30:04.349+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:30:04.348+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:30:04.560+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.989 seconds
[2022-12-11T15:30:14.885+0000] {processor.py:154} INFO - Started process (PID=2583) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:30:14.900+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:30:14.908+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:30:14.903+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:30:14.949+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:30:15.143+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:30:15.142+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:30:15.465+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:30:15.457+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:30:15.762+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.896 seconds
[2022-12-11T15:30:26.110+0000] {processor.py:154} INFO - Started process (PID=2593) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:30:26.132+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:30:26.139+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:30:26.138+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:30:26.205+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:30:26.405+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:30:26.404+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:30:26.555+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:30:26.554+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:30:26.694+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.609 seconds
[2022-12-11T15:30:37.088+0000] {processor.py:154} INFO - Started process (PID=2611) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:30:37.116+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:30:37.120+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:30:37.119+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:30:37.216+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:30:37.539+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:30:37.538+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:30:37.800+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:30:37.799+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:30:38.001+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.975 seconds
[2022-12-11T15:30:48.515+0000] {processor.py:154} INFO - Started process (PID=2621) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:30:48.519+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:30:48.523+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:30:48.522+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:30:48.563+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:30:48.757+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:30:48.756+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:30:48.938+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:30:48.937+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:30:49.068+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.569 seconds
[2022-12-11T15:30:59.396+0000] {processor.py:154} INFO - Started process (PID=2631) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:30:59.419+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:30:59.424+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:30:59.423+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:30:59.464+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:30:59.671+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:30:59.670+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:30:59.849+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:30:59.848+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:31:00.038+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.659 seconds
[2022-12-11T15:31:10.401+0000] {processor.py:154} INFO - Started process (PID=2641) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:31:10.425+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:31:10.430+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:31:10.429+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:31:10.469+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:31:11.065+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:31:11.064+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:31:11.341+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:31:11.323+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:31:11.665+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.280 seconds
[2022-12-11T15:31:22.076+0000] {processor.py:154} INFO - Started process (PID=2658) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:31:22.080+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:31:22.084+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:31:22.083+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:31:22.143+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:31:22.300+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:31:22.299+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:31:22.416+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:31:22.415+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:31:22.530+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.470 seconds
[2022-12-11T15:31:32.990+0000] {processor.py:154} INFO - Started process (PID=2666) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:31:32.995+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:31:33.005+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:31:33.004+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:31:33.073+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:31:33.442+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:31:33.441+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:31:33.600+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:31:33.599+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:31:33.748+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.794 seconds
[2022-12-11T15:31:44.120+0000] {processor.py:154} INFO - Started process (PID=2676) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:31:44.150+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:31:44.155+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:31:44.154+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:31:44.238+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:31:44.936+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:31:44.935+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:31:45.364+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:31:45.343+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:31:45.678+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.584 seconds
[2022-12-11T15:31:56.233+0000] {processor.py:154} INFO - Started process (PID=2694) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:31:56.241+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:31:56.253+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:31:56.243+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:31:56.342+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:31:57.839+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:31:57.838+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:31:58.042+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:31:58.040+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:31:58.285+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.118 seconds
[2022-12-11T15:32:08.694+0000] {processor.py:154} INFO - Started process (PID=2701) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:32:08.718+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:32:08.722+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:32:08.721+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:32:08.815+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:32:09.335+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:32:09.334+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:32:09.619+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:32:09.618+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:32:09.800+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.135 seconds
[2022-12-11T15:32:20.227+0000] {processor.py:154} INFO - Started process (PID=2711) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:32:20.294+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:32:20.305+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:32:20.304+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:32:20.407+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:32:20.892+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:32:20.891+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:32:21.051+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:32:21.050+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:32:21.237+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.038 seconds
[2022-12-11T15:32:31.715+0000] {processor.py:154} INFO - Started process (PID=2721) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:32:31.737+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:32:31.741+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:32:31.740+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:32:31.801+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:32:32.560+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:32:32.559+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:32:32.702+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:32:32.701+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:32:32.838+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.159 seconds
[2022-12-11T15:32:43.598+0000] {processor.py:154} INFO - Started process (PID=2738) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:32:43.604+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:32:43.616+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:32:43.607+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:32:43.742+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:32:44.013+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:32:44.012+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:32:44.281+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:32:44.280+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:32:44.579+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.012 seconds
[2022-12-11T15:32:55.278+0000] {processor.py:154} INFO - Started process (PID=2748) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:32:55.309+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:32:55.321+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:32:55.320+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:32:55.457+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:32:56.241+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:32:56.237+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:32:56.474+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:32:56.473+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:32:56.649+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.397 seconds
[2022-12-11T15:33:07.225+0000] {processor.py:154} INFO - Started process (PID=2758) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:33:07.255+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:33:07.272+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:33:07.265+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:33:07.519+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:33:08.184+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:33:08.183+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:33:08.457+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:33:08.456+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:33:08.696+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.546 seconds
[2022-12-11T15:33:19.363+0000] {processor.py:154} INFO - Started process (PID=2768) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:33:19.373+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:33:19.384+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:33:19.381+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:33:19.548+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:33:20.449+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:33:20.448+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:33:20.623+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:33:20.622+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:33:20.741+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.424 seconds
[2022-12-11T15:33:31.319+0000] {processor.py:154} INFO - Started process (PID=2786) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:33:31.344+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:33:31.361+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:33:31.357+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:33:31.521+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:33:32.130+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:33:32.129+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:33:32.544+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:33:32.543+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:33:32.926+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.657 seconds
[2022-12-11T15:33:43.444+0000] {processor.py:154} INFO - Started process (PID=2796) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:33:43.452+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:33:43.458+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:33:43.456+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:33:43.506+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:33:44.902+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T15:33:55.198+0000] {processor.py:154} INFO - Started process (PID=2806) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:33:55.213+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:33:55.218+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:33:55.217+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:33:55.286+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:33:55.764+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:33:55.763+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:33:56.010+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:33:56.009+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:33:56.218+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.061 seconds
[2022-12-11T15:34:06.940+0000] {processor.py:154} INFO - Started process (PID=2817) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:34:06.948+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:34:06.952+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:34:06.951+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:34:07.018+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:34:07.724+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:34:07.724+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:34:08.390+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:34:08.383+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:34:08.669+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.760 seconds
[2022-12-11T15:34:19.702+0000] {processor.py:154} INFO - Started process (PID=2835) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:34:19.741+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:34:19.776+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:34:19.760+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:34:19.965+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:34:20.991+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:34:20.962+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:34:21.884+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:34:21.883+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:34:22.717+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.121 seconds
[2022-12-11T15:34:33.825+0000] {processor.py:154} INFO - Started process (PID=2845) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:34:33.840+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:34:33.844+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:34:33.843+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:34:33.922+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:34:34.538+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:34:34.537+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:34:34.825+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:34:34.818+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:34:35.303+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.507 seconds
[2022-12-11T15:34:45.726+0000] {processor.py:154} INFO - Started process (PID=2855) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:34:45.730+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:34:45.733+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:34:45.733+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:34:45.771+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:34:47.046+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:34:47.045+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:34:47.347+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:34:47.346+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:34:47.615+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.908 seconds
[2022-12-11T15:34:58.240+0000] {processor.py:154} INFO - Started process (PID=2865) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:34:58.263+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:34:58.267+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:34:58.266+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:34:58.306+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:34:59.684+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:34:59.683+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:34:59.923+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:34:59.922+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:35:00.161+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.936 seconds
[2022-12-11T15:35:10.916+0000] {processor.py:154} INFO - Started process (PID=2883) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:35:10.927+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:35:10.930+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:35:10.930+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:35:11.095+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:35:12.317+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T15:35:22.861+0000] {processor.py:154} INFO - Started process (PID=2893) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:35:22.866+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:35:22.882+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:35:22.881+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:35:22.992+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:35:23.655+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:35:23.654+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:35:23.771+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:35:23.770+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:35:23.894+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.079 seconds
[2022-12-11T15:35:34.310+0000] {processor.py:154} INFO - Started process (PID=2903) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:35:34.319+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:35:34.327+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:35:34.323+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:35:34.395+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:35:35.407+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:35:35.395+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:35:35.732+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:35:35.726+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:35:35.996+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.730 seconds
[2022-12-11T15:35:46.835+0000] {processor.py:154} INFO - Started process (PID=2921) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:35:46.877+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:35:46.890+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:35:46.889+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:35:47.127+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:35:48.221+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T15:35:58.856+0000] {processor.py:154} INFO - Started process (PID=2934) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:35:58.894+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:35:58.914+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:35:58.913+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:35:59.185+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:35:59.695+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:35:59.694+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:35:59.839+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:35:59.838+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:36:00.077+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.353 seconds
[2022-12-11T15:36:10.688+0000] {processor.py:154} INFO - Started process (PID=2941) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:36:10.734+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:36:10.751+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:36:10.750+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:36:10.936+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:36:11.371+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:36:11.368+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:36:11.516+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:36:11.515+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:36:11.661+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.000 seconds
[2022-12-11T15:36:21.791+0000] {processor.py:154} INFO - Started process (PID=2951) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:36:21.811+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:36:21.816+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:36:21.815+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:36:21.854+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:36:22.405+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:36:22.390+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:36:22.745+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:36:22.744+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:36:22.987+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.211 seconds
[2022-12-11T15:36:33.587+0000] {processor.py:154} INFO - Started process (PID=2971) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:36:33.600+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:36:33.605+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:36:33.604+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:36:33.651+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:36:33.843+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:36:33.842+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:36:34.144+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:36:34.127+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:36:34.926+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.364 seconds
[2022-12-11T15:36:45.412+0000] {processor.py:154} INFO - Started process (PID=2978) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:36:45.426+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:36:45.430+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:36:45.429+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:36:45.475+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:36:45.643+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:36:45.642+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:36:45.776+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:36:45.775+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:36:45.925+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.532 seconds
[2022-12-11T15:36:56.118+0000] {processor.py:154} INFO - Started process (PID=2988) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:36:56.143+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:36:56.147+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:36:56.146+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:36:56.196+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:36:57.172+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T15:37:07.704+0000] {processor.py:154} INFO - Started process (PID=2998) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:37:07.734+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:37:07.738+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:37:07.737+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:37:07.780+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:37:08.343+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:37:08.342+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:37:08.607+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:37:08.606+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:37:08.938+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.259 seconds
[2022-12-11T15:37:19.699+0000] {processor.py:154} INFO - Started process (PID=3016) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:37:19.715+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:37:19.745+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:37:19.744+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:37:19.918+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:37:20.202+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:37:20.201+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:37:20.531+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:37:20.530+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:37:20.777+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.156 seconds
[2022-12-11T15:37:31.457+0000] {processor.py:154} INFO - Started process (PID=3026) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:37:31.461+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:37:31.471+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:37:31.470+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:37:31.513+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:37:32.260+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:37:32.259+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:37:32.390+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:37:32.389+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:37:32.577+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.151 seconds
[2022-12-11T15:37:43.149+0000] {processor.py:154} INFO - Started process (PID=3036) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:37:43.153+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:37:43.156+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:37:43.156+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:37:43.194+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:37:43.336+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:37:43.335+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:37:43.458+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:37:43.457+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:37:43.590+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.457 seconds
[2022-12-11T15:37:53.864+0000] {processor.py:154} INFO - Started process (PID=3046) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:37:53.911+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:37:53.916+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:37:53.915+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:37:53.956+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:37:54.558+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T15:38:05.146+0000] {processor.py:154} INFO - Started process (PID=3063) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:38:05.174+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:38:05.190+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:38:05.189+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:38:05.294+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:38:06.752+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:38:06.751+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:38:07.143+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:38:07.142+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:38:07.501+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.398 seconds
[2022-12-11T15:38:17.750+0000] {processor.py:154} INFO - Started process (PID=3073) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:38:17.757+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:38:17.761+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:38:17.760+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:38:17.804+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:38:18.195+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:38:18.194+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:38:18.412+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:38:18.411+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:38:18.604+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.873 seconds
[2022-12-11T15:38:28.986+0000] {processor.py:154} INFO - Started process (PID=3083) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:38:28.991+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:38:28.995+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:38:28.994+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:38:29.039+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:38:29.210+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:38:29.209+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:38:29.325+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:38:29.324+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:38:29.706+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.739 seconds
[2022-12-11T15:38:40.438+0000] {processor.py:154} INFO - Started process (PID=3094) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:38:40.479+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:38:40.483+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:38:40.482+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:38:40.935+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:38:42.900+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T15:38:53.723+0000] {processor.py:154} INFO - Started process (PID=3113) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:38:53.735+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:38:53.746+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:38:53.745+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:38:53.889+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:38:54.661+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:38:54.660+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:38:55.309+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:38:55.298+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:38:55.837+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.158 seconds
[2022-12-11T15:39:07.033+0000] {processor.py:154} INFO - Started process (PID=3123) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:39:07.058+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:39:07.093+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:39:07.086+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:39:07.387+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:39:08.198+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:39:08.196+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:39:08.904+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:39:08.883+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:39:09.278+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.431 seconds
[2022-12-11T15:39:19.947+0000] {processor.py:154} INFO - Started process (PID=3133) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:39:19.977+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:39:19.986+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:39:19.985+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:39:20.088+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:39:21.287+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:39:21.282+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:39:21.567+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:39:21.566+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:39:21.889+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.010 seconds
[2022-12-11T15:39:32.421+0000] {processor.py:154} INFO - Started process (PID=3143) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:39:32.431+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:39:32.435+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:39:32.434+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:39:32.567+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:39:33.825+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:39:33.824+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:39:34.219+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:39:34.218+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:39:34.601+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.236 seconds
[2022-12-11T15:39:45.254+0000] {processor.py:154} INFO - Started process (PID=3159) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:39:45.285+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:39:45.295+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:39:45.294+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:39:45.430+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:39:45.806+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:39:45.805+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:39:46.234+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:39:46.214+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:39:46.470+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.266 seconds
[2022-12-11T15:39:57.301+0000] {processor.py:154} INFO - Started process (PID=3170) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:39:57.334+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:39:57.380+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:39:57.357+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:39:57.639+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:39:58.777+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:39:58.776+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:39:58.932+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:39:58.931+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:39:59.254+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.107 seconds
[2022-12-11T15:40:09.643+0000] {processor.py:154} INFO - Started process (PID=3180) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:40:09.676+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:40:09.681+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:40:09.680+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:40:09.720+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:40:10.500+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T15:40:21.044+0000] {processor.py:154} INFO - Started process (PID=3190) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:40:21.052+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:40:21.064+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:40:21.055+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:40:21.115+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:40:21.318+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:40:21.317+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:40:21.471+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:40:21.470+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:40:21.606+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.595 seconds
[2022-12-11T15:40:32.021+0000] {processor.py:154} INFO - Started process (PID=3200) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:40:32.029+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:40:32.034+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:40:32.033+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:40:32.071+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:40:32.496+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:40:32.495+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:40:32.855+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:40:32.854+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:40:33.181+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.207 seconds
[2022-12-11T15:40:43.830+0000] {processor.py:154} INFO - Started process (PID=3218) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:40:43.836+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:40:43.840+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:40:43.839+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:40:43.884+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:40:45.310+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:40:45.309+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:40:45.499+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:40:45.498+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:40:45.748+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.946 seconds
[2022-12-11T15:40:56.421+0000] {processor.py:154} INFO - Started process (PID=3228) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:40:56.446+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:40:56.461+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:40:56.451+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:40:56.635+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:40:57.482+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:40:57.435+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:40:57.763+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:40:57.762+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:40:57.992+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.594 seconds
[2022-12-11T15:41:08.958+0000] {processor.py:154} INFO - Started process (PID=3238) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:41:08.996+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:41:09.001+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:41:09.000+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:41:09.102+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:41:09.294+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:41:09.292+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:41:09.439+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:41:09.439+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:41:09.552+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.630 seconds
[2022-12-11T15:41:19.958+0000] {processor.py:154} INFO - Started process (PID=3254) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:41:19.980+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:41:20.004+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:41:19.994+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:41:20.058+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:41:20.822+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:41:20.798+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:41:21.400+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:41:21.399+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:41:21.522+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.581 seconds
[2022-12-11T15:41:31.794+0000] {processor.py:154} INFO - Started process (PID=3265) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:41:31.821+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:41:31.825+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:41:31.824+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:41:31.863+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:41:32.041+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:41:32.040+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:41:32.394+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:41:32.392+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:41:32.683+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.904 seconds
[2022-12-11T15:41:43.415+0000] {processor.py:154} INFO - Started process (PID=3275) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:41:43.547+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:41:43.555+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:41:43.554+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:41:43.784+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:41:44.064+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:41:44.063+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:41:44.355+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:41:44.343+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:41:44.637+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.254 seconds
[2022-12-11T15:41:55.525+0000] {processor.py:154} INFO - Started process (PID=3285) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:41:55.689+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:41:55.701+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:41:55.693+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:41:55.924+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:41:57.011+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:41:57.010+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:41:57.367+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:41:57.367+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:41:57.901+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.447 seconds
[2022-12-11T15:42:08.311+0000] {processor.py:154} INFO - Started process (PID=3303) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:42:08.318+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:42:08.329+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:42:08.327+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:42:08.390+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:42:09.135+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T15:42:19.647+0000] {processor.py:154} INFO - Started process (PID=3313) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:42:19.668+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:42:19.673+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:42:19.672+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:42:19.710+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:42:20.322+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:42:20.318+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:42:20.492+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:42:20.490+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:42:20.765+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.131 seconds
[2022-12-11T15:42:31.425+0000] {processor.py:154} INFO - Started process (PID=3324) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:42:31.461+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:42:31.545+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:42:31.543+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:42:31.828+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:42:32.333+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:42:32.332+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:42:32.659+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:42:32.658+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:42:32.929+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.554 seconds
[2022-12-11T15:42:43.523+0000] {processor.py:154} INFO - Started process (PID=3334) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:42:43.549+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:42:43.569+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:42:43.563+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:42:43.742+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:42:44.172+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:42:44.171+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:42:44.453+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:42:44.452+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:42:44.737+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.248 seconds
[2022-12-11T15:42:55.164+0000] {processor.py:154} INFO - Started process (PID=3352) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:42:55.167+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:42:55.186+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:42:55.185+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:42:55.290+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:42:55.702+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:42:55.701+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:42:56.063+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:42:56.062+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:42:56.427+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.315 seconds
[2022-12-11T15:43:07.927+0000] {processor.py:154} INFO - Started process (PID=3363) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:43:08.051+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:43:08.067+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:43:08.066+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:43:08.267+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:43:08.923+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:43:08.922+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:43:09.467+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:43:09.466+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:43:09.674+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.854 seconds
[2022-12-11T15:43:20.562+0000] {processor.py:154} INFO - Started process (PID=3373) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:43:20.592+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:43:20.602+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:43:20.600+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:43:20.862+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:43:21.521+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:43:21.515+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:43:22.175+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:43:22.174+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:43:22.350+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.875 seconds
[2022-12-11T15:43:32.705+0000] {processor.py:154} INFO - Started process (PID=3383) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:43:32.709+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:43:32.722+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:43:32.721+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:43:32.765+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:43:33.038+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:43:33.037+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:43:33.231+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:43:33.230+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:43:33.465+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.845 seconds
[2022-12-11T15:43:44.186+0000] {processor.py:154} INFO - Started process (PID=3399) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:43:44.201+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:43:44.211+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:43:44.210+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:43:44.336+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:43:45.074+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:43:45.073+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:43:45.508+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:43:45.503+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:43:46.005+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.860 seconds
[2022-12-11T15:43:56.717+0000] {processor.py:154} INFO - Started process (PID=3410) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:43:56.726+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:43:56.740+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:43:56.729+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:43:56.846+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:43:57.445+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:43:57.444+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:43:58.438+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:43:58.415+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:43:58.762+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.103 seconds
[2022-12-11T15:44:09.341+0000] {processor.py:154} INFO - Started process (PID=3420) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:44:09.346+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:44:09.351+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:44:09.350+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:44:09.395+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:44:09.834+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:44:09.833+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:44:10.270+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:44:10.268+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:44:10.574+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.249 seconds
[2022-12-11T15:44:21.110+0000] {processor.py:154} INFO - Started process (PID=3430) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:44:21.128+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:44:21.132+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:44:21.131+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:44:21.191+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:44:21.377+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:44:21.376+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:44:21.569+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:44:21.568+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:44:21.855+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.764 seconds
[2022-12-11T15:44:32.609+0000] {processor.py:154} INFO - Started process (PID=3440) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:44:32.621+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:44:32.641+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:44:32.631+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:44:32.754+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:44:33.232+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:44:33.231+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:44:33.544+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:44:33.530+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:44:34.084+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.523 seconds
[2022-12-11T15:44:45.014+0000] {processor.py:154} INFO - Started process (PID=3458) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:44:45.037+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:44:45.047+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:44:45.045+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:44:45.150+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:44:45.430+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:44:45.429+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:44:45.723+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:44:45.722+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:44:46.293+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.348 seconds
[2022-12-11T15:44:56.933+0000] {processor.py:154} INFO - Started process (PID=3468) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:44:56.960+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:44:56.965+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:44:56.964+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:44:57.015+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:44:57.201+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:44:57.200+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:44:57.380+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:44:57.371+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:44:57.647+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.730 seconds
[2022-12-11T15:45:08.117+0000] {processor.py:154} INFO - Started process (PID=3478) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:45:08.141+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:45:08.150+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:45:08.149+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:45:08.222+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:45:09.108+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T15:45:19.519+0000] {processor.py:154} INFO - Started process (PID=3488) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:45:19.545+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:45:19.550+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:45:19.549+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:45:19.619+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:45:20.925+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T15:45:31.921+0000] {processor.py:154} INFO - Started process (PID=3506) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:45:31.931+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:45:31.938+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:45:31.935+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:45:32.024+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:45:33.170+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T15:45:43.708+0000] {processor.py:154} INFO - Started process (PID=3516) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:45:43.712+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:45:43.716+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:45:43.715+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:45:43.759+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:45:44.098+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T15:45:54.989+0000] {processor.py:154} INFO - Started process (PID=3526) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:45:55.011+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:45:55.015+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:45:55.014+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:45:55.122+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:45:56.763+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:45:56.762+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:45:57.110+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:45:57.103+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:45:57.325+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.355 seconds
[2022-12-11T15:46:07.754+0000] {processor.py:154} INFO - Started process (PID=3543) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:46:07.774+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:46:07.779+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:46:07.778+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:46:07.932+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:46:08.302+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:46:08.301+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:46:08.595+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:46:08.587+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:46:09.021+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.330 seconds
[2022-12-11T15:46:19.572+0000] {processor.py:154} INFO - Started process (PID=3554) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:46:19.577+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:46:19.581+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:46:19.580+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:46:19.618+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:46:19.761+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:46:19.760+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:46:19.875+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:46:19.875+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:46:19.990+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.438 seconds
[2022-12-11T15:46:30.313+0000] {processor.py:154} INFO - Started process (PID=3564) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:46:30.331+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:46:30.335+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:46:30.334+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:46:30.383+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:46:30.569+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:46:30.568+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:46:30.720+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:46:30.719+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:46:30.836+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.539 seconds
[2022-12-11T15:46:41.085+0000] {processor.py:154} INFO - Started process (PID=3574) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:46:41.114+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:46:41.118+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:46:41.117+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:46:41.157+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:46:41.798+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:46:41.797+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:46:41.943+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:46:41.942+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:46:42.069+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.014 seconds
[2022-12-11T15:46:52.423+0000] {processor.py:154} INFO - Started process (PID=3592) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:46:52.452+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:46:52.461+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:46:52.460+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:46:52.507+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:46:53.359+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:46:53.358+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:46:53.679+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:46:53.678+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:46:53.790+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.384 seconds
[2022-12-11T15:47:04.212+0000] {processor.py:154} INFO - Started process (PID=3602) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:47:04.222+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:47:04.226+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:47:04.225+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:47:04.268+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:47:05.324+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T15:47:15.596+0000] {processor.py:154} INFO - Started process (PID=3612) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:47:15.602+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:47:15.606+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:47:15.605+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:47:15.643+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:47:16.137+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:47:16.136+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:47:16.342+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:47:16.341+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:47:16.495+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.921 seconds
[2022-12-11T15:47:26.931+0000] {processor.py:154} INFO - Started process (PID=3622) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:47:26.954+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:47:26.958+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:47:26.957+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:47:27.035+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:47:28.052+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:47:28.051+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:47:28.191+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:47:28.190+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:47:28.326+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.416 seconds
[2022-12-11T15:47:38.881+0000] {processor.py:154} INFO - Started process (PID=3640) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:47:38.908+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:47:38.915+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:47:38.911+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:47:39.033+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:47:39.941+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T15:47:50.135+0000] {processor.py:154} INFO - Started process (PID=3650) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:47:50.157+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:47:50.161+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:47:50.160+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:47:50.218+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:47:51.063+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T15:48:01.469+0000] {processor.py:154} INFO - Started process (PID=3660) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:48:01.499+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:48:01.503+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:48:01.502+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:48:01.557+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:48:02.663+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T15:48:13.159+0000] {processor.py:154} INFO - Started process (PID=3678) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:48:13.191+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:48:13.196+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:48:13.195+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:48:13.240+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:48:13.441+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:48:13.440+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:48:13.599+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:48:13.598+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:48:13.748+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.605 seconds
[2022-12-11T15:48:24.390+0000] {processor.py:154} INFO - Started process (PID=3688) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:48:24.395+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:48:24.401+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:48:24.400+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:48:24.442+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:48:24.622+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:48:24.621+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:48:24.882+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:48:24.881+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:48:24.994+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.625 seconds
[2022-12-11T15:48:35.333+0000] {processor.py:154} INFO - Started process (PID=3698) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:48:35.341+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:48:35.346+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:48:35.345+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:48:35.405+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:48:35.585+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:48:35.584+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:48:35.734+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:48:35.732+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:48:35.982+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.678 seconds
[2022-12-11T15:48:46.375+0000] {processor.py:154} INFO - Started process (PID=3708) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:48:46.385+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:48:46.390+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:48:46.389+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:48:46.431+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:48:47.041+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T15:48:57.822+0000] {processor.py:154} INFO - Started process (PID=3725) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:48:57.845+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:48:57.850+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:48:57.849+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:48:57.889+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:48:58.146+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:48:58.145+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:48:58.291+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:48:58.290+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:48:58.401+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.595 seconds
[2022-12-11T15:49:08.717+0000] {processor.py:154} INFO - Started process (PID=3735) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:49:08.721+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:49:08.724+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:49:08.724+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:49:08.765+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:49:09.311+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:49:09.310+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:49:09.428+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:49:09.428+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:49:09.536+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.837 seconds
[2022-12-11T15:49:19.862+0000] {processor.py:154} INFO - Started process (PID=3745) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:49:19.871+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:49:19.876+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:49:19.875+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:49:19.963+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:49:20.326+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:49:20.324+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:49:20.511+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:49:20.510+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:49:20.649+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.818 seconds
[2022-12-11T15:49:31.162+0000] {processor.py:154} INFO - Started process (PID=3755) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:49:31.166+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:49:31.170+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:49:31.169+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:49:31.236+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:49:31.776+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:49:31.775+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:49:31.891+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:49:31.890+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:49:32.020+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.873 seconds
[2022-12-11T15:49:43.133+0000] {processor.py:154} INFO - Started process (PID=3773) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:49:43.143+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:49:43.157+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:49:43.156+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:49:43.299+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:49:43.515+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:49:43.514+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:49:43.906+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:49:43.905+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:49:44.184+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.090 seconds
[2022-12-11T15:49:54.539+0000] {processor.py:154} INFO - Started process (PID=3783) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:49:54.555+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:49:54.566+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:49:54.565+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:49:54.613+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:49:54.866+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:49:54.865+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:49:54.983+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:49:54.982+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:49:55.105+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.581 seconds
[2022-12-11T15:50:05.432+0000] {processor.py:154} INFO - Started process (PID=3793) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:50:05.459+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:50:05.464+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:50:05.463+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:50:05.510+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:50:05.997+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:50:05.996+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:50:06.113+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:50:06.112+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:50:06.243+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.830 seconds
[2022-12-11T15:50:16.565+0000] {processor.py:154} INFO - Started process (PID=3811) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:50:16.818+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:50:16.823+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:50:16.822+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:50:16.948+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:50:17.374+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:50:17.373+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:50:17.625+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:50:17.624+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:50:17.751+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.215 seconds
[2022-12-11T15:50:28.247+0000] {processor.py:154} INFO - Started process (PID=3822) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:50:28.251+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:50:28.255+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:50:28.254+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:50:28.294+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:50:28.447+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:50:28.446+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:50:28.567+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:50:28.566+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:50:28.689+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.456 seconds
[2022-12-11T15:50:38.986+0000] {processor.py:154} INFO - Started process (PID=3832) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:50:38.994+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:50:38.998+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:50:38.997+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:50:39.050+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:50:39.299+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:50:39.299+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:50:39.438+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:50:39.437+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:50:39.550+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.598 seconds
[2022-12-11T15:50:49.763+0000] {processor.py:154} INFO - Started process (PID=3842) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:50:49.785+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:50:49.790+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:50:49.789+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:50:49.829+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:50:50.254+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T15:51:00.748+0000] {processor.py:154} INFO - Started process (PID=3861) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:51:00.768+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:51:00.779+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:51:00.771+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:51:00.938+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:51:01.237+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:51:01.236+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:51:01.509+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:51:01.508+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:51:01.674+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.989 seconds
[2022-12-11T15:51:12.330+0000] {processor.py:154} INFO - Started process (PID=3869) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:51:12.373+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:51:12.417+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:51:12.416+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:51:12.495+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:51:13.183+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:51:13.182+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:51:13.308+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:51:13.307+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:51:13.427+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.264 seconds
[2022-12-11T15:51:23.852+0000] {processor.py:154} INFO - Started process (PID=3879) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:51:23.941+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:51:23.949+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:51:23.948+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:51:24.104+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:51:24.937+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:51:24.936+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:51:25.117+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:51:25.115+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:51:25.247+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.482 seconds
[2022-12-11T15:51:35.699+0000] {processor.py:154} INFO - Started process (PID=3889) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:51:35.729+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:51:35.738+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:51:35.737+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:51:35.827+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:51:36.175+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:51:36.174+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:51:36.391+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:51:36.390+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:51:36.523+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.872 seconds
[2022-12-11T15:51:46.975+0000] {processor.py:154} INFO - Started process (PID=3907) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:51:46.981+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:51:46.989+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:51:46.983+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:51:47.038+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:51:48.459+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:51:48.458+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:51:48.747+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:51:48.746+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:51:48.917+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.964 seconds
[2022-12-11T15:51:59.262+0000] {processor.py:154} INFO - Started process (PID=3914) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:51:59.270+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:51:59.275+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:51:59.274+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:51:59.322+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:52:00.151+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:52:00.150+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:52:00.459+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:52:00.458+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:52:00.632+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.387 seconds
[2022-12-11T15:52:10.852+0000] {processor.py:154} INFO - Started process (PID=3927) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:52:10.865+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:52:10.868+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:52:10.868+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:52:10.911+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:52:11.494+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T15:52:22.326+0000] {processor.py:154} INFO - Started process (PID=3941) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:52:22.377+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:52:22.406+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:52:22.405+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:52:22.642+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:52:23.069+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:52:23.068+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:52:23.209+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:52:23.208+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:52:23.434+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.204 seconds
[2022-12-11T15:52:34.018+0000] {processor.py:154} INFO - Started process (PID=3951) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:52:34.040+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:52:34.046+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:52:34.043+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:52:34.157+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:52:35.322+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T15:52:45.743+0000] {processor.py:154} INFO - Started process (PID=3964) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:52:45.775+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:52:45.781+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:52:45.779+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:52:45.862+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:52:46.194+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:52:46.193+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:52:46.843+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:52:46.842+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:52:47.065+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.342 seconds
[2022-12-11T15:52:57.600+0000] {processor.py:154} INFO - Started process (PID=3974) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:52:57.613+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:52:57.631+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:52:57.630+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:52:57.720+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:52:57.974+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:52:57.973+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:52:58.090+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:52:58.089+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:52:58.196+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.621 seconds
[2022-12-11T15:54:06.161+0000] {processor.py:154} INFO - Started process (PID=179) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:54:06.175+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:54:06.205+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:54:06.187+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:54:06.416+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:54:06.967+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:54:06.966+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:54:07.307+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:54:07.303+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:54:07.594+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.485 seconds
[2022-12-11T15:54:17.883+0000] {processor.py:154} INFO - Started process (PID=189) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:54:17.887+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:54:17.891+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:54:17.890+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:54:18.076+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:54:18.718+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:54:18.716+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:54:18.905+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:54:18.903+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:54:19.063+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.197 seconds
[2022-12-11T15:54:29.404+0000] {processor.py:154} INFO - Started process (PID=206) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:54:29.409+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:54:29.414+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:54:29.413+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:54:29.535+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:54:30.362+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:54:30.361+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:54:30.632+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:54:30.631+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:54:30.747+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.374 seconds
[2022-12-11T15:54:41.080+0000] {processor.py:154} INFO - Started process (PID=217) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:54:41.096+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:54:41.101+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:54:41.100+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:54:41.354+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:54:41.537+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:54:41.535+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:54:41.675+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:54:41.674+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:54:41.804+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.749 seconds
[2022-12-11T15:54:52.000+0000] {processor.py:154} INFO - Started process (PID=227) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:54:52.059+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:54:52.063+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:54:52.062+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:54:52.218+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:54:52.381+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:54:52.380+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:54:52.525+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:54:52.524+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:54:52.652+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.677 seconds
[2022-12-11T15:55:02.973+0000] {processor.py:154} INFO - Started process (PID=237) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:55:02.993+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:55:02.998+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:55:02.996+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:55:03.122+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:55:03.313+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:55:03.312+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:55:03.483+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:55:03.482+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:55:03.644+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.688 seconds
[2022-12-11T15:55:14.209+0000] {processor.py:154} INFO - Started process (PID=255) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:55:14.260+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:55:14.264+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:55:14.263+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:55:14.473+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:55:14.775+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:55:14.774+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:55:15.029+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:55:15.027+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:55:15.224+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.036 seconds
[2022-12-11T15:55:25.912+0000] {processor.py:154} INFO - Started process (PID=265) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:55:25.916+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:55:25.920+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:55:25.919+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:55:26.027+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:55:26.526+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:55:26.525+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:55:26.650+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:55:26.649+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:55:27.012+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.120 seconds
[2022-12-11T15:55:38.121+0000] {processor.py:154} INFO - Started process (PID=277) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:55:38.124+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:55:38.129+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:55:38.128+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:55:38.223+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:55:38.367+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:55:38.367+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:55:38.508+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:55:38.507+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:55:38.691+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.586 seconds
[2022-12-11T15:55:48.960+0000] {processor.py:154} INFO - Started process (PID=287) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:55:48.963+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:55:48.967+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:55:48.966+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:55:49.056+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:55:49.222+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:55:49.221+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:55:49.357+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:55:49.356+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:55:49.508+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.562 seconds
[2022-12-11T15:56:00.229+0000] {processor.py:154} INFO - Started process (PID=304) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:56:00.260+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:56:00.264+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:56:00.263+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:56:00.349+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:56:00.495+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:56:00.494+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:56:00.618+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:56:00.617+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:56:00.737+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.526 seconds
[2022-12-11T15:56:10.947+0000] {processor.py:154} INFO - Started process (PID=314) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:56:10.962+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:56:10.967+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:56:10.966+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:56:11.087+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:56:11.996+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:56:11.995+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:56:12.122+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:56:12.121+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:56:12.253+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.324 seconds
[2022-12-11T15:56:22.515+0000] {processor.py:154} INFO - Started process (PID=324) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:56:22.529+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:56:22.532+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:56:22.531+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:56:22.618+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:56:23.262+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:56:23.261+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:56:23.401+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:56:23.400+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:56:23.537+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.038 seconds
[2022-12-11T15:56:33.730+0000] {processor.py:154} INFO - Started process (PID=340) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:56:33.742+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:56:33.748+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:56:33.747+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:56:33.870+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:56:35.080+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:56:35.079+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:56:35.423+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:56:35.422+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:56:35.677+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.963 seconds
[2022-12-11T15:56:46.178+0000] {processor.py:154} INFO - Started process (PID=352) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:56:46.182+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:56:46.186+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:56:46.185+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:56:46.275+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:56:46.422+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:56:46.421+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:56:46.548+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:56:46.547+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:56:46.660+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.497 seconds
[2022-12-11T15:56:57.365+0000] {processor.py:154} INFO - Started process (PID=362) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:56:57.369+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:56:57.373+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:56:57.372+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:56:57.458+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:56:57.942+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:56:57.941+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:56:58.059+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:56:58.058+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:56:58.241+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.891 seconds
[2022-12-11T15:57:08.732+0000] {processor.py:154} INFO - Started process (PID=372) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:57:08.736+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:57:08.740+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:57:08.739+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:57:08.847+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:57:09.005+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:57:09.004+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:57:09.123+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:57:09.122+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:57:09.252+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.536 seconds
[2022-12-11T15:57:19.641+0000] {processor.py:154} INFO - Started process (PID=390) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:57:19.677+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:57:19.683+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:57:19.682+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:57:19.835+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:57:20.065+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:57:20.064+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:57:20.196+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:57:20.195+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:57:20.309+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.688 seconds
[2022-12-11T15:57:30.568+0000] {processor.py:154} INFO - Started process (PID=400) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:57:30.578+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:57:30.583+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:57:30.582+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:57:30.671+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:57:31.637+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:57:31.636+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:57:31.756+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:57:31.755+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:57:31.876+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.323 seconds
[2022-12-11T15:57:42.219+0000] {processor.py:154} INFO - Started process (PID=410) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:57:42.245+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:57:42.249+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:57:42.247+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:57:42.345+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:57:42.809+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:57:42.808+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:57:42.928+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:57:42.927+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:57:43.141+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.936 seconds
[2022-12-11T15:57:53.556+0000] {processor.py:154} INFO - Started process (PID=426) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:57:53.579+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:57:53.583+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:57:53.582+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:57:53.762+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:57:54.107+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:57:54.106+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:57:54.271+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:57:54.270+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:57:54.381+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.857 seconds
[2022-12-11T15:58:04.667+0000] {processor.py:154} INFO - Started process (PID=438) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:58:04.671+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:58:04.676+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:58:04.675+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:58:04.779+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:58:05.937+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T15:58:16.281+0000] {processor.py:154} INFO - Started process (PID=446) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:58:16.327+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:58:16.332+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:58:16.331+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:58:16.420+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:58:17.757+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:58:17.755+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:58:17.893+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:58:17.893+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:58:18.001+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.734 seconds
[2022-12-11T15:58:28.415+0000] {processor.py:154} INFO - Started process (PID=458) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:58:28.436+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:58:28.443+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:58:28.442+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:58:28.566+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:58:28.871+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T15:58:39.381+0000] {processor.py:154} INFO - Started process (PID=477) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:58:39.397+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:58:39.401+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:58:39.400+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:58:39.510+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:58:39.717+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:58:39.715+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:58:39.872+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:58:39.871+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:58:40.163+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.799 seconds
[2022-12-11T15:58:50.444+0000] {processor.py:154} INFO - Started process (PID=487) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:58:50.448+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:58:50.452+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:58:50.451+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:58:50.538+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:58:50.690+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:58:50.689+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:58:50.838+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:58:50.837+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:58:50.993+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.565 seconds
[2022-12-11T15:59:01.811+0000] {processor.py:154} INFO - Started process (PID=497) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:59:01.815+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:59:01.819+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:59:01.818+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:59:01.919+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:59:02.404+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T15:59:12.781+0000] {processor.py:154} INFO - Started process (PID=515) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:59:12.803+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:59:12.807+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:59:12.806+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:59:13.049+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:59:13.614+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:59:13.613+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:59:13.810+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:59:13.809+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:59:14.025+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.279 seconds
[2022-12-11T15:59:24.488+0000] {processor.py:154} INFO - Started process (PID=526) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:59:24.514+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:59:24.518+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:59:24.517+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:59:24.625+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:59:25.997+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:59:25.996+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T15:59:26.114+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:59:26.113+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T15:59:26.224+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.754 seconds
[2022-12-11T15:59:36.505+0000] {processor.py:154} INFO - Started process (PID=536) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:59:36.510+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:59:36.513+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:59:36.512+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:59:36.610+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:59:37.252+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T15:59:48.042+0000] {processor.py:154} INFO - Started process (PID=546) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:59:48.048+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:59:48.052+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:59:48.051+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:59:48.142+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:59:48.851+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T15:59:59.408+0000] {processor.py:154} INFO - Started process (PID=564) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:59:59.441+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T15:59:59.483+0000] {logging_mixin.py:137} INFO - [2022-12-11T15:59:59.477+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T15:59:59.789+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:00:00.108+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:00:00.099+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T16:00:00.421+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:00:00.420+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T16:00:00.629+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.266 seconds
[2022-12-11T16:00:11.183+0000] {processor.py:154} INFO - Started process (PID=574) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:00:11.188+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T16:00:11.193+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:00:11.192+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:00:11.286+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:00:11.443+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:00:11.442+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T16:00:11.562+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:00:11.562+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T16:00:11.868+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.701 seconds
[2022-12-11T16:00:22.139+0000] {processor.py:154} INFO - Started process (PID=584) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:00:22.169+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T16:00:22.173+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:00:22.172+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:00:22.258+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:00:22.954+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T16:00:33.238+0000] {processor.py:154} INFO - Started process (PID=594) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:00:33.261+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T16:00:33.266+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:00:33.265+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:00:33.351+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:00:33.488+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:00:33.486+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T16:00:33.644+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:00:33.643+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T16:00:33.755+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.534 seconds
[2022-12-11T16:00:44.173+0000] {processor.py:154} INFO - Started process (PID=613) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:00:44.212+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T16:00:44.221+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:00:44.215+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:00:44.447+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:00:44.594+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:00:44.594+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T16:00:44.718+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:00:44.717+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T16:00:44.845+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.720 seconds
[2022-12-11T16:00:55.322+0000] {processor.py:154} INFO - Started process (PID=623) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:00:55.337+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T16:00:55.350+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:00:55.349+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:00:55.669+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:00:55.832+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:00:55.831+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T16:00:55.950+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:00:55.949+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T16:00:56.085+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.789 seconds
[2022-12-11T16:01:06.622+0000] {processor.py:154} INFO - Started process (PID=633) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:01:06.633+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T16:01:06.638+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:01:06.637+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:01:06.769+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:01:08.298+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:01:08.297+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T16:01:08.438+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:01:08.437+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T16:01:08.566+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.975 seconds
[2022-12-11T16:01:19.102+0000] {processor.py:154} INFO - Started process (PID=651) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:01:19.113+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T16:01:19.118+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:01:19.117+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:01:19.402+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:01:19.922+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:01:19.921+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T16:01:20.114+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:01:20.113+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T16:01:20.265+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.212 seconds
[2022-12-11T16:01:30.710+0000] {processor.py:154} INFO - Started process (PID=661) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:01:30.740+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T16:01:30.745+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:01:30.744+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:01:30.831+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:01:30.967+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:01:30.966+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T16:01:31.080+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:01:31.079+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T16:01:31.208+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.514 seconds
[2022-12-11T16:01:41.694+0000] {processor.py:154} INFO - Started process (PID=671) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:01:41.709+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T16:01:41.718+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:01:41.717+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:01:41.891+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:01:42.037+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:01:42.036+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T16:01:42.151+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:01:42.150+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T16:01:42.275+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.644 seconds
[2022-12-11T16:01:52.594+0000] {processor.py:154} INFO - Started process (PID=681) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:01:52.608+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T16:01:52.613+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:01:52.612+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:01:52.984+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:01:53.202+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:01:53.201+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T16:01:53.352+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:01:53.351+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T16:01:53.552+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.988 seconds
[2022-12-11T16:02:04.022+0000] {processor.py:154} INFO - Started process (PID=699) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:02:04.027+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T16:02:04.031+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:02:04.030+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:02:04.183+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:02:04.711+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:02:04.710+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T16:02:05.269+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:02:05.268+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T16:02:05.413+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.406 seconds
[2022-12-11T16:02:15.739+0000] {processor.py:154} INFO - Started process (PID=709) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:02:15.760+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T16:02:15.769+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:02:15.768+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:02:15.892+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:02:16.065+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:02:16.064+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T16:02:16.223+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:02:16.222+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T16:02:16.380+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.658 seconds
[2022-12-11T16:02:26.677+0000] {processor.py:154} INFO - Started process (PID=719) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:02:26.681+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T16:02:26.684+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:02:26.684+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:02:26.766+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:02:28.022+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-11T16:02:38.258+0000] {processor.py:154} INFO - Started process (PID=736) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:02:38.264+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T16:02:38.270+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:02:38.269+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:02:38.484+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:02:39.986+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:02:39.985+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T16:02:40.282+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:02:40.271+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T16:02:40.725+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.486 seconds
[2022-12-11T16:02:51.140+0000] {processor.py:154} INFO - Started process (PID=747) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:02:51.144+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T16:02:51.154+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:02:51.153+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:02:51.282+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:02:52.756+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:02:52.755+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T16:02:53.077+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:02:53.076+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-11T16:02:53.455+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.356 seconds
[2022-12-11T16:03:03.919+0000] {processor.py:154} INFO - Started process (PID=757) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:03:03.958+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-11T16:03:03.966+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:03:03.965+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:03:04.330+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-11T16:03:05.735+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:03:05.734+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-11T16:03:06.296+0000] {logging_mixin.py:137} INFO - [2022-12-11T16:03:06.287+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
