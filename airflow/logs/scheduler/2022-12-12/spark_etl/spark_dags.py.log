[2022-12-12T12:05:01.428+0000] {processor.py:154} INFO - Started process (PID=186) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:05:01.472+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:05:01.537+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:05:01.536+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:05:02.980+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:05:21.176+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:05:21.173+0000] {process_utils.py:256} INFO - Waiting up to 5 seconds for processes to exit...
[2022-12-12T12:06:47.743+0000] {processor.py:154} INFO - Started process (PID=177) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:06:47.947+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:06:47.950+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:06:47.950+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:06:48.090+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:06:49.442+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:06:49.441+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:06:50.126+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:06:50.124+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:06:51.595+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.865 seconds
[2022-12-12T12:07:02.696+0000] {processor.py:154} INFO - Started process (PID=187) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:07:02.722+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:07:02.739+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:07:02.728+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:07:03.389+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:07:05.856+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:07:05.855+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:07:06.550+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:07:06.541+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:07:07.138+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 4.621 seconds
[2022-12-12T12:07:18.072+0000] {processor.py:154} INFO - Started process (PID=205) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:07:18.108+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:07:18.164+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:07:18.163+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:07:19.581+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:07:20.924+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:07:20.923+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:07:21.576+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:07:21.575+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:07:22.179+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 4.219 seconds
[2022-12-12T12:07:33.153+0000] {processor.py:154} INFO - Started process (PID=215) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:07:33.203+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:07:33.247+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:07:33.205+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:07:34.038+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:07:34.999+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:07:34.998+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:07:35.637+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:07:35.636+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:07:36.123+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.120 seconds
[2022-12-12T12:07:47.088+0000] {processor.py:154} INFO - Started process (PID=225) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:07:47.128+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:07:47.155+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:07:47.141+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:07:48.093+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:07:48.536+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:07:48.535+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:07:48.749+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:07:48.748+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:07:49.351+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.426 seconds
[2022-12-12T12:08:00.741+0000] {processor.py:154} INFO - Started process (PID=235) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:08:00.756+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:08:00.776+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:08:00.775+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:08:01.337+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:08:02.583+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:08:02.582+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:08:03.173+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:08:03.172+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:08:04.120+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.453 seconds
[2022-12-12T12:08:15.391+0000] {processor.py:154} INFO - Started process (PID=250) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:08:15.447+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:08:15.467+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:08:15.466+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:08:16.021+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:08:17.416+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:08:17.415+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:08:18.548+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:08:18.547+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:08:19.725+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 4.510 seconds
[2022-12-12T12:08:31.024+0000] {processor.py:154} INFO - Started process (PID=260) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:08:31.056+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:08:31.092+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:08:31.091+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:08:31.578+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:08:32.553+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:08:32.533+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:08:33.855+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:08:33.852+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:08:34.831+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.876 seconds
[2022-12-12T12:08:46.027+0000] {processor.py:154} INFO - Started process (PID=272) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:08:46.071+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:08:46.095+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:08:46.094+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:08:46.435+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:08:47.501+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:08:47.500+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:08:48.675+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:08:48.669+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:08:49.572+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.606 seconds
[2022-12-12T12:09:00.892+0000] {processor.py:154} INFO - Started process (PID=283) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:09:00.949+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:09:00.964+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:09:00.963+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:09:01.900+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:09:02.527+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:09:02.526+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:09:02.806+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:09:02.805+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:09:03.111+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.314 seconds
[2022-12-12T12:09:14.344+0000] {processor.py:154} INFO - Started process (PID=298) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:09:14.389+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:09:14.458+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:09:14.433+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:09:15.248+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:09:16.732+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:09:16.731+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:09:17.553+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:09:17.552+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:09:18.413+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 4.232 seconds
[2022-12-12T12:09:29.356+0000] {processor.py:154} INFO - Started process (PID=308) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:09:29.375+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:09:29.400+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:09:29.399+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:09:30.160+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:09:32.865+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:09:32.864+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:09:34.176+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:09:34.174+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:09:34.702+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 5.457 seconds
[2022-12-12T12:09:45.398+0000] {processor.py:154} INFO - Started process (PID=321) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:09:45.410+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:09:45.422+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:09:45.413+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:09:45.701+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:09:46.628+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:09:46.626+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:09:47.272+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:09:47.271+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:09:47.759+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.433 seconds
[2022-12-12T12:09:58.320+0000] {processor.py:154} INFO - Started process (PID=331) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:09:58.336+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:09:58.356+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:09:58.355+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:09:58.812+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:09:59.724+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:09:59.723+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:09:59.940+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:09:59.940+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:10:00.216+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.019 seconds
[2022-12-12T12:10:11.128+0000] {processor.py:154} INFO - Started process (PID=348) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:10:11.175+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:10:11.208+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:10:11.200+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:10:11.795+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:10:12.828+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:10:12.827+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:10:13.652+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:10:13.651+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:10:14.736+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.752 seconds
[2022-12-12T12:10:26.468+0000] {processor.py:154} INFO - Started process (PID=358) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:10:26.497+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:10:26.508+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:10:26.507+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:10:28.204+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:10:29.850+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:10:29.844+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:10:30.870+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:10:30.869+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:10:31.532+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 5.123 seconds
[2022-12-12T12:10:42.472+0000] {processor.py:154} INFO - Started process (PID=368) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:10:42.479+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:10:42.505+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:10:42.497+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:10:42.856+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:10:43.677+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:10:43.676+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:10:44.152+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:10:44.151+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:10:44.455+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.052 seconds
[2022-12-12T12:10:55.415+0000] {processor.py:154} INFO - Started process (PID=378) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:10:55.448+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:10:55.507+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:10:55.493+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:10:56.334+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:10:57.086+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:10:57.085+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:10:58.071+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:10:58.066+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:10:58.782+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.522 seconds
[2022-12-12T12:11:10.018+0000] {processor.py:154} INFO - Started process (PID=395) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:11:10.042+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:11:10.078+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:11:10.070+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:11:11.068+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:11:13.245+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:11:13.244+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:11:14.361+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:11:14.360+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:11:15.401+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 5.529 seconds
[2022-12-12T12:11:26.290+0000] {processor.py:154} INFO - Started process (PID=406) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:11:26.315+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:11:26.364+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:11:26.342+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:11:27.274+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:11:28.926+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:11:28.897+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:11:29.994+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:11:29.977+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:11:30.542+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 4.405 seconds
[2022-12-12T12:11:42.005+0000] {processor.py:154} INFO - Started process (PID=416) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:11:42.045+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:11:42.077+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:11:42.076+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:11:43.249+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:11:44.357+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:11:44.356+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:11:45.338+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:11:45.301+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:11:45.903+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 4.107 seconds
[2022-12-12T12:11:56.868+0000] {processor.py:154} INFO - Started process (PID=426) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:11:56.880+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:11:56.891+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:11:56.887+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:11:57.184+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:11:57.745+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:11:57.744+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:11:58.062+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:11:58.060+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:11:58.511+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.743 seconds
[2022-12-12T12:12:09.828+0000] {processor.py:154} INFO - Started process (PID=443) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:12:09.904+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:12:09.952+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:12:09.951+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:12:10.749+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:12:11.973+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:12:11.961+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:12:12.717+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:12:12.716+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:12:13.200+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.621 seconds
[2022-12-12T12:12:24.068+0000] {processor.py:154} INFO - Started process (PID=454) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:12:24.087+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:12:24.109+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:12:24.108+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:12:25.047+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:12:27.113+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:12:27.112+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:12:27.938+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:12:27.913+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:12:28.488+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 4.588 seconds
[2022-12-12T12:12:39.791+0000] {processor.py:154} INFO - Started process (PID=464) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:12:39.811+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:12:39.846+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:12:39.845+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:12:40.516+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:12:41.281+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:12:41.280+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:12:42.164+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:12:42.163+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:12:42.927+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.300 seconds
[2022-12-12T12:12:53.636+0000] {processor.py:154} INFO - Started process (PID=476) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:12:53.639+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:12:53.643+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:12:53.642+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:12:53.832+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:12:54.160+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:12:54.156+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:12:54.391+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:12:54.390+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:12:54.578+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.962 seconds
[2022-12-12T12:13:05.064+0000] {processor.py:154} INFO - Started process (PID=486) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:13:05.070+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:13:05.074+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:13:05.073+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:13:05.228+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:13:05.689+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:13:05.688+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:13:05.922+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:13:05.921+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:13:06.149+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.107 seconds
[2022-12-12T12:13:16.676+0000] {processor.py:154} INFO - Started process (PID=501) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:13:16.727+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:13:16.733+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:13:16.732+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:13:17.220+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:13:17.666+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:13:17.661+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:13:17.950+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:13:17.940+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:13:18.617+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.059 seconds
[2022-12-12T12:13:29.462+0000] {processor.py:154} INFO - Started process (PID=513) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:13:29.469+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:13:29.495+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:13:29.484+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:13:29.890+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:13:30.303+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:13:30.302+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:13:30.486+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:13:30.485+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:13:30.715+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.318 seconds
[2022-12-12T12:13:40.988+0000] {processor.py:154} INFO - Started process (PID=521) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:13:40.996+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:13:41.003+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:13:41.001+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:13:41.265+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:13:41.527+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:13:41.526+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:13:41.716+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:13:41.715+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:13:42.062+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.109 seconds
[2022-12-12T12:13:52.602+0000] {processor.py:154} INFO - Started process (PID=531) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:13:52.613+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:13:52.617+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:13:52.616+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:13:52.753+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:13:53.284+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:13:53.283+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:13:53.666+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:13:53.665+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:13:54.094+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.543 seconds
[2022-12-12T12:14:05.067+0000] {processor.py:154} INFO - Started process (PID=549) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:14:05.091+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:14:05.120+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:14:05.118+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:14:05.701+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:14:06.649+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:14:06.648+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:14:07.529+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:14:07.528+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:14:08.915+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.957 seconds
[2022-12-12T12:14:20.661+0000] {processor.py:154} INFO - Started process (PID=558) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:14:20.666+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:14:20.691+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:14:20.669+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:14:21.629+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:14:23.494+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:14:23.489+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:14:24.273+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:14:24.272+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:14:25.035+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 4.560 seconds
[2022-12-12T12:14:35.671+0000] {processor.py:154} INFO - Started process (PID=569) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:14:35.694+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:14:35.711+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:14:35.710+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:14:36.101+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:14:36.761+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:14:36.760+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:14:37.605+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:14:37.604+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:14:38.325+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.782 seconds
[2022-12-12T12:14:49.245+0000] {processor.py:154} INFO - Started process (PID=579) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:14:49.271+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:14:49.279+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:14:49.278+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:14:49.774+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:14:50.554+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:14:50.545+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:14:51.116+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:14:51.115+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:14:51.741+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.545 seconds
[2022-12-12T12:15:03.176+0000] {processor.py:154} INFO - Started process (PID=596) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:15:03.215+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:15:03.252+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:15:03.251+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:15:03.744+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:15:05.253+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:15:05.252+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:15:06.019+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:15:05.993+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:15:07.523+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 4.500 seconds
[2022-12-12T12:15:18.284+0000] {processor.py:154} INFO - Started process (PID=604) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:15:18.315+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:15:18.327+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:15:18.326+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:15:19.300+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:15:20.628+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:15:20.627+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:15:22.057+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:15:22.056+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:15:22.564+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 4.389 seconds
[2022-12-12T12:15:33.988+0000] {processor.py:154} INFO - Started process (PID=614) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:15:34.020+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:15:34.114+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:15:34.084+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:15:34.558+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:15:36.697+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:15:36.696+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:15:37.309+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:15:37.308+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:15:38.160+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 4.351 seconds
[2022-12-12T12:15:48.774+0000] {processor.py:154} INFO - Started process (PID=624) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:15:48.782+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:15:48.786+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:15:48.785+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:15:49.101+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:15:49.491+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:15:49.461+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:15:50.060+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:15:50.059+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:15:50.319+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.606 seconds
[2022-12-12T12:16:01.508+0000] {processor.py:154} INFO - Started process (PID=641) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:16:01.543+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:16:01.568+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:16:01.567+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:16:02.370+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:16:03.440+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T12:16:14.620+0000] {processor.py:154} INFO - Started process (PID=652) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:16:14.691+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:16:14.748+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:16:14.747+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:16:15.448+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:16:16.123+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:16:16.122+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:16:16.969+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:16:16.968+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:16:17.690+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.253 seconds
[2022-12-12T12:16:28.806+0000] {processor.py:154} INFO - Started process (PID=662) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:16:28.809+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:16:28.846+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:16:28.845+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:16:29.534+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:16:30.428+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:16:30.427+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:16:31.183+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:16:31.161+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:16:31.782+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.156 seconds
[2022-12-12T12:16:42.776+0000] {processor.py:154} INFO - Started process (PID=672) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:16:42.835+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:16:42.856+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:16:42.855+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:16:43.222+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:16:43.964+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:16:43.963+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:16:44.387+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:16:44.386+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:16:44.678+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.002 seconds
[2022-12-12T12:16:55.791+0000] {processor.py:154} INFO - Started process (PID=682) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:16:55.815+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:16:55.869+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:16:55.853+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:16:56.313+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:16:57.678+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:16:57.677+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:16:57.994+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:16:57.985+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:16:58.304+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.644 seconds
[2022-12-12T12:17:09.704+0000] {processor.py:154} INFO - Started process (PID=700) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:17:09.724+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:17:09.809+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:17:09.736+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:17:10.166+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:17:11.795+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:17:11.794+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:17:12.373+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:17:12.372+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:17:12.872+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.359 seconds
[2022-12-12T12:17:24.100+0000] {processor.py:154} INFO - Started process (PID=710) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:17:24.130+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:17:24.146+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:17:24.133+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:17:24.593+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:17:26.244+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T12:17:37.067+0000] {processor.py:154} INFO - Started process (PID=720) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:17:37.115+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:17:37.126+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:17:37.117+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:17:37.387+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:17:38.520+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:17:38.519+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:17:39.389+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:17:39.388+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:17:39.727+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.735 seconds
[2022-12-12T12:17:50.442+0000] {processor.py:154} INFO - Started process (PID=730) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:17:50.450+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:17:50.458+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:17:50.453+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:17:50.635+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:17:51.025+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:17:51.021+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:17:51.453+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:17:51.451+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:17:51.697+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.300 seconds
[2022-12-12T12:18:02.593+0000] {processor.py:154} INFO - Started process (PID=747) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:18:02.607+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:18:02.635+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:18:02.634+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:18:03.266+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:18:04.372+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T12:18:15.745+0000] {processor.py:154} INFO - Started process (PID=758) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:18:15.798+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:18:15.820+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:18:15.819+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:18:16.245+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:18:17.384+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:18:17.383+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:18:18.082+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:18:18.081+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:18:18.602+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.020 seconds
[2022-12-12T12:18:29.765+0000] {processor.py:154} INFO - Started process (PID=768) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:18:29.807+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:18:29.835+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:18:29.809+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:18:30.339+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:18:31.123+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:18:31.109+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:18:31.654+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:18:31.645+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:18:32.213+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.530 seconds
[2022-12-12T12:18:42.827+0000] {processor.py:154} INFO - Started process (PID=778) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:18:42.846+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:18:42.854+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:18:42.849+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:18:43.220+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:18:43.825+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:18:43.824+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:18:44.309+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:18:44.308+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:18:44.564+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.845 seconds
[2022-12-12T12:18:55.425+0000] {processor.py:154} INFO - Started process (PID=788) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:18:55.437+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:18:55.452+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:18:55.451+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:18:55.701+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:18:55.987+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:18:55.986+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:18:56.269+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:18:56.269+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:18:56.449+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.064 seconds
[2022-12-12T12:19:07.733+0000] {processor.py:154} INFO - Started process (PID=806) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:19:07.840+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:19:07.878+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:19:07.849+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:19:08.174+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:19:09.929+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T12:19:21.541+0000] {processor.py:154} INFO - Started process (PID=816) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:19:21.583+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:19:21.603+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:19:21.602+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:19:22.417+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:19:23.872+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:19:23.871+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:19:24.983+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:19:24.973+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:19:25.597+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 4.193 seconds
[2022-12-12T12:19:36.481+0000] {processor.py:154} INFO - Started process (PID=826) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:19:36.547+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:19:36.552+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:19:36.550+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:19:36.808+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:19:37.733+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T12:19:48.541+0000] {processor.py:154} INFO - Started process (PID=836) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:19:48.591+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:19:48.595+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:19:48.594+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:19:48.969+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:19:49.404+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:19:49.397+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:19:49.727+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:19:49.726+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:19:50.288+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.775 seconds
[2022-12-12T12:20:00.711+0000] {processor.py:154} INFO - Started process (PID=846) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:20:00.723+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:20:00.727+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:20:00.726+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:20:00.963+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:20:01.458+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:20:01.453+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:20:02.112+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:20:02.111+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:20:03.047+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.375 seconds
[2022-12-12T12:20:13.692+0000] {processor.py:154} INFO - Started process (PID=864) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:20:13.758+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:20:13.779+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:20:13.778+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:20:14.735+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:20:15.298+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:20:15.296+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:20:15.541+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:20:15.525+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:20:15.985+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.360 seconds
[2022-12-12T12:20:26.537+0000] {processor.py:154} INFO - Started process (PID=874) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:20:26.567+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:20:26.576+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:20:26.575+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:20:27.194+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:20:28.078+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:20:28.073+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:20:28.390+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:20:28.389+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:20:28.620+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.157 seconds
[2022-12-12T12:20:39.234+0000] {processor.py:154} INFO - Started process (PID=884) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:20:39.285+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:20:39.297+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:20:39.296+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:20:39.469+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:20:39.842+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:20:39.841+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:20:40.281+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:20:40.280+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:20:40.695+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.497 seconds
[2022-12-12T12:20:51.432+0000] {processor.py:154} INFO - Started process (PID=894) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:20:51.441+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:20:51.445+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:20:51.444+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:20:51.743+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:20:52.679+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:20:52.678+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:20:52.948+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:20:52.947+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:20:53.287+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.927 seconds
[2022-12-12T12:21:03.867+0000] {processor.py:154} INFO - Started process (PID=911) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:21:03.907+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:21:03.930+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:21:03.917+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:21:04.350+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:21:05.663+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:21:05.662+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:21:06.287+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:21:06.286+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:21:06.907+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.120 seconds
[2022-12-12T12:21:17.769+0000] {processor.py:154} INFO - Started process (PID=922) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:21:17.792+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:21:17.808+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:21:17.807+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:21:18.267+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:21:18.644+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:21:18.643+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:21:19.006+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:21:19.001+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:21:19.211+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.497 seconds
[2022-12-12T12:21:29.751+0000] {processor.py:154} INFO - Started process (PID=932) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:21:29.758+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:21:29.771+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:21:29.765+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:21:29.954+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:21:31.149+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:21:31.148+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:21:31.326+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:21:31.325+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:21:31.544+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.810 seconds
[2022-12-12T12:21:42.589+0000] {processor.py:154} INFO - Started process (PID=942) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:21:42.616+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:21:42.620+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:21:42.619+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:21:42.806+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:21:43.248+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:21:43.247+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:21:43.570+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:21:43.569+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:21:43.848+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.286 seconds
[2022-12-12T12:21:54.126+0000] {processor.py:154} INFO - Started process (PID=952) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:21:54.132+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:21:54.148+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:21:54.147+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:21:54.380+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:21:54.724+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:21:54.723+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:21:54.924+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:21:54.923+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:21:55.181+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.080 seconds
[2022-12-12T12:22:05.888+0000] {processor.py:154} INFO - Started process (PID=970) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:22:05.891+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:22:05.916+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:22:05.915+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:22:06.302+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:22:06.943+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T12:22:17.505+0000] {processor.py:154} INFO - Started process (PID=978) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:22:17.542+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:22:17.550+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:22:17.545+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:22:17.805+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:22:18.395+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:22:18.394+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:22:18.709+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:22:18.707+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:22:18.897+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.443 seconds
[2022-12-12T12:22:29.222+0000] {processor.py:154} INFO - Started process (PID=990) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:22:29.225+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:22:29.229+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:22:29.228+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:22:29.373+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:22:29.564+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:22:29.563+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:22:29.709+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:22:29.708+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:22:29.896+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.705 seconds
[2022-12-12T12:22:40.844+0000] {processor.py:154} INFO - Started process (PID=998) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:22:40.879+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:22:40.899+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:22:40.898+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:22:41.542+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:22:41.954+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:22:41.949+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:22:42.470+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:22:42.461+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:22:42.813+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.084 seconds
[2022-12-12T12:22:54.033+0000] {processor.py:154} INFO - Started process (PID=1015) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:22:54.073+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:22:54.113+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:22:54.112+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:22:55.156+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:22:56.979+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T12:23:08.148+0000] {processor.py:154} INFO - Started process (PID=1025) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:23:08.176+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:23:08.203+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:23:08.202+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:23:09.173+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:23:10.283+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:23:10.282+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:23:11.264+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:23:11.263+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:23:12.039+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.971 seconds
[2022-12-12T12:23:23.301+0000] {processor.py:154} INFO - Started process (PID=1036) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:23:23.343+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:23:23.379+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:23:23.378+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:23:24.298+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:23:24.969+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:23:24.968+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:23:25.969+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:23:25.968+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:23:26.567+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.407 seconds
[2022-12-12T12:23:36.977+0000] {processor.py:154} INFO - Started process (PID=1043) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:23:36.980+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:23:36.996+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:23:36.995+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:23:37.386+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:23:38.361+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:23:38.360+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:23:39.217+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:23:39.216+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:23:39.774+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.833 seconds
[2022-12-12T12:23:50.976+0000] {processor.py:154} INFO - Started process (PID=1061) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:23:51.001+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:23:51.036+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:23:51.035+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:23:51.737+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:23:54.797+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:23:54.796+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:23:55.290+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:23:55.289+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:23:55.813+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 4.930 seconds
[2022-12-12T12:24:06.511+0000] {processor.py:154} INFO - Started process (PID=1072) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:24:06.535+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:24:06.555+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:24:06.554+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:24:07.485+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:24:10.057+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:24:10.056+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:24:11.449+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:24:11.448+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:24:12.463+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 6.063 seconds
[2022-12-12T12:24:23.576+0000] {processor.py:154} INFO - Started process (PID=1085) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:24:23.631+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:24:23.675+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:24:23.674+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:24:24.506+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:24:25.459+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:24:25.458+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:24:26.545+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:24:26.544+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:24:27.538+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 4.017 seconds
[2022-12-12T12:24:38.593+0000] {processor.py:154} INFO - Started process (PID=1095) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:24:38.646+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:24:38.654+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:24:38.652+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:24:39.019+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:24:39.357+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:24:39.350+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:24:39.599+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:24:39.598+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:24:40.055+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.555 seconds
[2022-12-12T12:24:50.863+0000] {processor.py:154} INFO - Started process (PID=1112) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:24:50.927+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:24:50.954+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:24:50.937+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:24:51.484+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:24:52.013+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:24:52.012+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:24:52.541+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:24:52.540+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:24:53.231+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.446 seconds
[2022-12-12T12:25:04.440+0000] {processor.py:154} INFO - Started process (PID=1123) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:25:04.474+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:25:04.491+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:25:04.477+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:25:05.048+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:25:05.788+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:25:05.773+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:25:06.557+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:25:06.556+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:25:07.154+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.772 seconds
[2022-12-12T12:25:17.771+0000] {processor.py:154} INFO - Started process (PID=1134) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:25:17.822+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:25:17.829+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:25:17.825+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:25:18.285+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:25:19.303+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:25:19.302+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:25:19.572+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:25:19.571+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:25:19.859+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.152 seconds
[2022-12-12T12:25:30.325+0000] {processor.py:154} INFO - Started process (PID=1145) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:25:30.343+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:25:30.347+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:25:30.346+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:25:30.750+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:25:31.596+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:25:31.595+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:25:31.802+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:25:31.800+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:25:32.049+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.761 seconds
[2022-12-12T12:25:42.755+0000] {processor.py:154} INFO - Started process (PID=1162) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:25:42.768+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:25:42.782+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:25:42.777+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:25:43.579+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:25:44.909+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:25:44.896+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:25:45.812+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:25:45.765+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:25:46.525+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.821 seconds
[2022-12-12T12:25:57.684+0000] {processor.py:154} INFO - Started process (PID=1173) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:25:57.721+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:25:57.746+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:25:57.729+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:25:58.304+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:25:59.639+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:25:59.593+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:26:00.408+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:26:00.407+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:26:01.198+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.585 seconds
[2022-12-12T12:26:12.111+0000] {processor.py:154} INFO - Started process (PID=1183) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:26:12.136+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:26:12.148+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:26:12.147+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:26:12.814+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:26:14.570+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:26:14.544+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:26:15.096+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:26:15.090+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:26:15.511+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.523 seconds
[2022-12-12T12:26:26.120+0000] {processor.py:154} INFO - Started process (PID=1193) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:26:26.140+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:26:26.154+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:26:26.149+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:26:26.288+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:26:26.681+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:26:26.680+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:26:26.998+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:26:26.993+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:26:27.527+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.453 seconds
[2022-12-12T12:26:37.894+0000] {processor.py:154} INFO - Started process (PID=1203) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:26:37.929+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:26:37.933+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:26:37.932+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:26:38.136+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:26:38.531+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:26:38.529+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:26:38.779+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:26:38.778+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:26:39.167+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.312 seconds
[2022-12-12T12:26:49.777+0000] {processor.py:154} INFO - Started process (PID=1222) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:26:49.799+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:26:49.818+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:26:49.801+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:26:50.470+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:26:51.324+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:26:51.317+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:26:52.016+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:26:51.996+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:26:52.635+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.932 seconds
[2022-12-12T12:27:04.034+0000] {processor.py:154} INFO - Started process (PID=1232) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:27:04.047+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:27:04.086+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:27:04.073+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:27:04.518+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:27:05.261+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:27:05.260+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:27:06.102+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:27:06.085+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:27:06.634+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.834 seconds
[2022-12-12T12:27:17.603+0000] {processor.py:154} INFO - Started process (PID=1242) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:27:17.627+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:27:17.659+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:27:17.629+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:27:18.216+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:27:19.535+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:27:19.534+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:27:20.250+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:27:20.217+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:27:20.765+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.264 seconds
[2022-12-12T12:27:31.227+0000] {processor.py:154} INFO - Started process (PID=1252) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:27:31.239+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:27:31.253+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:27:31.252+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:27:31.546+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:27:32.109+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:27:32.108+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:27:32.519+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:27:32.518+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:27:32.879+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.676 seconds
[2022-12-12T12:27:44.023+0000] {processor.py:154} INFO - Started process (PID=1269) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:27:44.083+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:27:44.099+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:27:44.098+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:27:44.802+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:27:45.586+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:27:45.573+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:27:46.383+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:27:46.382+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:27:46.832+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.928 seconds
[2022-12-12T12:27:58.046+0000] {processor.py:154} INFO - Started process (PID=1280) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:27:58.049+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:27:58.094+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:27:58.077+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:27:58.596+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:27:59.484+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:27:59.483+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:27:59.995+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:27:59.973+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:28:00.531+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.633 seconds
[2022-12-12T12:28:11.924+0000] {processor.py:154} INFO - Started process (PID=1290) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:28:11.955+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:28:11.968+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:28:11.967+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:28:12.484+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:28:13.240+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:28:13.239+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:28:14.172+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:28:14.171+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:28:14.817+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.067 seconds
[2022-12-12T12:28:25.594+0000] {processor.py:154} INFO - Started process (PID=1300) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:28:25.607+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:28:25.621+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:28:25.620+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:28:26.209+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:28:26.925+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:28:26.923+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:28:27.300+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:28:27.299+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:28:27.941+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.400 seconds
[2022-12-12T12:28:39.139+0000] {processor.py:154} INFO - Started process (PID=1310) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:28:39.159+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:28:39.173+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:28:39.171+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:28:39.431+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:28:39.849+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:28:39.848+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:28:40.206+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:28:40.205+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:28:40.675+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.624 seconds
[2022-12-12T12:28:51.949+0000] {processor.py:154} INFO - Started process (PID=1328) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:28:52.016+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:28:52.020+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:28:52.019+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:28:52.625+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:28:53.737+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:28:53.736+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:28:54.689+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:28:54.688+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:28:55.399+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.543 seconds
[2022-12-12T12:29:06.749+0000] {processor.py:154} INFO - Started process (PID=1338) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:29:06.762+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:29:06.778+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:29:06.765+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:29:07.130+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:29:09.557+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:29:09.535+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:29:10.288+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:29:10.287+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:29:10.690+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 4.002 seconds
[2022-12-12T12:29:21.584+0000] {processor.py:154} INFO - Started process (PID=1348) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:29:21.649+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:29:21.653+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:29:21.652+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:29:22.011+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:29:23.140+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:29:23.133+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:29:23.410+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:29:23.409+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:29:23.638+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.094 seconds
[2022-12-12T12:29:34.318+0000] {processor.py:154} INFO - Started process (PID=1358) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:29:34.339+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:29:34.349+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:29:34.348+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:29:34.488+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:29:35.361+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:29:35.359+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:29:35.783+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:29:35.774+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:29:36.542+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.266 seconds
[2022-12-12T12:29:47.185+0000] {processor.py:154} INFO - Started process (PID=1375) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:29:47.188+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:29:47.209+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:29:47.209+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:29:47.689+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:29:48.969+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:29:48.957+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:29:50.020+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:29:50.005+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:29:51.019+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.911 seconds
[2022-12-12T12:30:01.918+0000] {processor.py:154} INFO - Started process (PID=1386) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:30:01.933+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:30:01.950+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:30:01.948+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:30:02.533+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:30:04.005+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:30:04.004+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:30:04.977+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:30:04.976+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:30:05.889+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 4.042 seconds
[2022-12-12T12:30:17.593+0000] {processor.py:154} INFO - Started process (PID=1399) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:30:17.618+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:30:17.629+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:30:17.621+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:30:18.062+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:30:19.648+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:30:19.647+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:30:20.163+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:30:20.162+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:30:20.713+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.194 seconds
[2022-12-12T12:30:31.381+0000] {processor.py:154} INFO - Started process (PID=1409) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:30:31.402+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:30:31.419+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:30:31.405+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:30:31.792+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:30:32.613+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:30:32.612+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:30:33.338+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:30:33.321+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:30:33.846+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.545 seconds
[2022-12-12T12:30:44.903+0000] {processor.py:154} INFO - Started process (PID=1426) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:30:44.925+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:30:44.965+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:30:44.941+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:30:45.700+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:30:47.027+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:30:47.026+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:30:48.212+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:30:48.188+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:30:49.609+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 4.823 seconds
[2022-12-12T12:31:00.405+0000] {processor.py:154} INFO - Started process (PID=1437) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:31:00.477+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:31:00.508+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:31:00.507+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:31:01.655+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:31:04.093+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:31:04.092+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:31:05.196+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:31:05.195+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:31:05.683+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 5.458 seconds
[2022-12-12T12:31:17.066+0000] {processor.py:154} INFO - Started process (PID=1447) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:31:17.069+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:31:17.109+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:31:17.108+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:31:17.868+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:31:19.079+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:31:19.078+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:31:20.085+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:31:20.084+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:31:20.947+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.989 seconds
[2022-12-12T12:31:31.925+0000] {processor.py:154} INFO - Started process (PID=1457) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:31:31.944+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:31:31.956+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:31:31.955+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:31:32.328+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:31:33.601+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:31:33.597+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:31:33.931+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:31:33.930+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:31:34.242+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.430 seconds
[2022-12-12T12:31:45.383+0000] {processor.py:154} INFO - Started process (PID=1475) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:31:45.414+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:31:45.442+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:31:45.417+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:31:46.244+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:31:48.368+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T12:32:00.583+0000] {processor.py:154} INFO - Started process (PID=1486) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:32:00.604+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:32:00.626+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:32:00.620+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:32:01.349+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:32:02.324+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:32:02.323+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:32:03.081+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:32:03.080+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:32:03.448+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.991 seconds
[2022-12-12T12:32:14.410+0000] {processor.py:154} INFO - Started process (PID=1496) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:32:14.444+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:32:14.494+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:32:14.489+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:32:15.794+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:32:17.063+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:32:17.062+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:32:17.509+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:32:17.508+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:32:17.765+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.506 seconds
[2022-12-12T12:32:29.022+0000] {processor.py:154} INFO - Started process (PID=1506) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:32:29.043+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:32:29.071+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:32:29.070+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:32:29.795+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:32:30.255+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:32:30.253+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:32:30.582+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:32:30.581+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:32:30.852+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.027 seconds
[2022-12-12T12:32:41.830+0000] {processor.py:154} INFO - Started process (PID=1518) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:32:41.835+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:32:41.863+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:32:41.862+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:32:42.264+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:32:42.987+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:32:42.981+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:32:43.416+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:32:43.401+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:32:44.095+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.371 seconds
[2022-12-12T12:32:54.909+0000] {processor.py:154} INFO - Started process (PID=1535) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:32:54.923+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:32:54.956+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:32:54.955+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:32:55.597+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:32:56.488+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:32:56.487+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:32:56.900+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:32:56.899+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:32:57.134+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.313 seconds
[2022-12-12T12:33:08.226+0000] {processor.py:154} INFO - Started process (PID=1545) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:33:08.229+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:33:08.295+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:33:08.294+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:33:09.181+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:33:09.848+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:33:09.847+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:33:10.693+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:33:10.692+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:33:11.239+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.248 seconds
[2022-12-12T12:33:22.123+0000] {processor.py:154} INFO - Started process (PID=1555) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:33:22.155+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:33:22.159+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:33:22.158+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:33:22.430+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:33:22.824+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:33:22.823+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:33:23.545+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:33:23.544+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:33:23.819+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.746 seconds
[2022-12-12T12:33:34.770+0000] {processor.py:154} INFO - Started process (PID=1565) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:33:34.777+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:33:34.781+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:33:34.780+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:33:35.159+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:33:36.097+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:33:36.087+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:33:36.701+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:33:36.700+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:33:37.083+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.363 seconds
[2022-12-12T12:33:48.172+0000] {processor.py:154} INFO - Started process (PID=1582) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:33:48.241+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:33:48.245+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:33:48.244+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:33:48.978+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:33:50.406+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:33:50.393+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:33:50.731+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:33:50.730+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:33:51.349+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.358 seconds
[2022-12-12T12:34:02.412+0000] {processor.py:154} INFO - Started process (PID=1593) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:34:02.442+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:34:02.455+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:34:02.445+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:34:02.905+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:34:03.615+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:34:03.614+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:34:04.489+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:34:04.488+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:34:05.039+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.726 seconds
[2022-12-12T12:34:16.279+0000] {processor.py:154} INFO - Started process (PID=1603) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:34:16.297+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:34:16.358+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:34:16.336+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:34:17.110+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:34:18.093+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:34:18.092+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:34:18.795+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:34:18.789+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:34:19.180+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.041 seconds
[2022-12-12T12:34:29.847+0000] {processor.py:154} INFO - Started process (PID=1613) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:34:29.900+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:34:29.909+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:34:29.908+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:34:30.200+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:34:31.868+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T12:34:42.773+0000] {processor.py:154} INFO - Started process (PID=1623) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:34:42.780+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:34:42.784+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:34:42.783+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:34:43.237+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:34:45.042+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:34:45.037+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:34:46.223+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:34:46.222+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:34:47.207+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 4.481 seconds
[2022-12-12T12:34:58.177+0000] {processor.py:154} INFO - Started process (PID=1642) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:34:58.203+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:34:58.232+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:34:58.230+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:34:59.228+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:35:00.113+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:35:00.112+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:35:00.596+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:35:00.595+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:35:01.304+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.220 seconds
[2022-12-12T12:35:12.297+0000] {processor.py:154} INFO - Started process (PID=1652) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:35:12.303+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:35:12.317+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:35:12.316+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:35:12.889+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:35:15.713+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:35:15.706+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:35:16.461+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:35:16.460+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:35:17.400+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 5.195 seconds
[2022-12-12T12:35:28.325+0000] {processor.py:154} INFO - Started process (PID=1662) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:35:28.341+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:35:28.355+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:35:28.354+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:35:28.607+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:35:29.145+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:35:29.144+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:35:29.685+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:35:29.684+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:35:30.301+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.025 seconds
[2022-12-12T12:35:40.955+0000] {processor.py:154} INFO - Started process (PID=1672) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:35:40.969+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:35:40.973+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:35:40.972+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:35:41.187+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:35:42.188+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T12:35:53.065+0000] {processor.py:154} INFO - Started process (PID=1690) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:35:53.122+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:35:53.154+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:35:53.133+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:35:53.531+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:35:55.712+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:35:55.711+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:35:56.642+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:35:56.620+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:35:57.312+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 4.381 seconds
[2022-12-12T12:36:08.437+0000] {processor.py:154} INFO - Started process (PID=1700) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:36:08.471+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:36:08.492+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:36:08.491+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:36:09.043+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:36:10.973+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:36:10.972+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:36:12.153+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:36:12.117+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:36:13.311+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 4.972 seconds
[2022-12-12T12:36:24.596+0000] {processor.py:154} INFO - Started process (PID=1710) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:36:24.619+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:36:24.651+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:36:24.649+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:36:25.634+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:36:27.126+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:36:27.125+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:36:28.181+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:36:28.180+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:36:28.903+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 4.446 seconds
[2022-12-12T12:36:39.725+0000] {processor.py:154} INFO - Started process (PID=1720) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:36:39.743+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:36:39.764+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:36:39.763+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:36:40.411+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:36:41.716+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:36:41.715+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:36:42.141+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:36:42.139+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:36:42.454+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.792 seconds
[2022-12-12T12:36:53.691+0000] {processor.py:154} INFO - Started process (PID=1738) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:36:53.714+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:36:53.785+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:36:53.717+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:36:54.667+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:36:56.547+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T12:37:08.600+0000] {processor.py:154} INFO - Started process (PID=1751) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:37:08.649+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:37:08.657+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:37:08.656+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:37:09.320+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:37:10.837+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:37:10.836+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:37:11.490+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:37:11.489+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:37:12.352+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.891 seconds
[2022-12-12T12:37:22.837+0000] {processor.py:154} INFO - Started process (PID=1761) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:37:22.907+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:37:22.912+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:37:22.909+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:37:23.558+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:37:24.241+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:37:24.240+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:37:24.644+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:37:24.643+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:37:25.175+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.423 seconds
[2022-12-12T12:37:36.103+0000] {processor.py:154} INFO - Started process (PID=1771) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:37:36.114+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:37:36.135+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:37:36.134+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:37:36.753+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:37:37.410+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:37:37.409+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:37:37.888+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:37:37.888+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:37:38.563+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.548 seconds
[2022-12-12T12:37:49.391+0000] {processor.py:154} INFO - Started process (PID=1787) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:37:49.411+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:37:49.450+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:37:49.442+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:37:50.528+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:37:52.935+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:37:52.934+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:37:53.700+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:37:53.684+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:37:54.711+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 5.442 seconds
[2022-12-12T12:38:05.663+0000] {processor.py:154} INFO - Started process (PID=1798) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:38:05.666+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:38:05.689+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:38:05.669+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:38:06.122+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:38:06.661+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:38:06.660+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:38:07.101+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:38:07.100+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:38:07.901+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.352 seconds
[2022-12-12T12:38:18.422+0000] {processor.py:154} INFO - Started process (PID=1810) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:38:18.436+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:38:18.447+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:38:18.446+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:38:18.669+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:38:19.093+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:38:19.092+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:38:19.654+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:38:19.644+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:38:19.954+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.602 seconds
[2022-12-12T12:38:30.680+0000] {processor.py:154} INFO - Started process (PID=1818) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:38:30.711+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:38:30.747+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:38:30.737+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:38:31.124+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:38:32.388+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T12:38:43.229+0000] {processor.py:154} INFO - Started process (PID=1830) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:38:43.296+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:38:43.300+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:38:43.299+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:38:43.519+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:38:44.733+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:38:44.721+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:38:45.366+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:38:45.365+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:38:46.183+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.036 seconds
[2022-12-12T12:38:57.472+0000] {processor.py:154} INFO - Started process (PID=1848) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:38:57.499+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:38:57.513+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:38:57.501+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:38:57.904+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:38:58.830+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:38:58.829+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:38:59.466+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:38:59.437+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:39:00.115+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.742 seconds
[2022-12-12T12:39:11.415+0000] {processor.py:154} INFO - Started process (PID=1858) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:39:11.436+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:39:11.504+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:39:11.467+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:39:12.177+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:39:13.537+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:39:13.536+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:39:14.611+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:39:14.610+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:39:15.255+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.987 seconds
[2022-12-12T12:39:26.187+0000] {processor.py:154} INFO - Started process (PID=1868) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:39:26.212+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:39:26.245+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:39:26.241+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:39:26.664+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:39:28.265+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T12:39:39.056+0000] {processor.py:154} INFO - Started process (PID=1878) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:39:39.063+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:39:39.108+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:39:39.107+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:39:39.473+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:39:40.316+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:39:40.288+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:39:40.662+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:39:40.661+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:39:40.896+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.872 seconds
[2022-12-12T12:39:52.120+0000] {processor.py:154} INFO - Started process (PID=1895) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:39:52.143+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:39:52.155+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:39:52.154+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:39:52.811+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:39:53.491+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:39:53.490+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:39:53.849+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:39:53.848+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:39:54.212+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.232 seconds
[2022-12-12T12:40:04.820+0000] {processor.py:154} INFO - Started process (PID=1905) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:40:04.878+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:40:04.890+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:40:04.881+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:40:05.154+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:40:05.980+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:40:05.979+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:40:06.273+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:40:06.272+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:40:06.420+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.628 seconds
[2022-12-12T12:40:16.900+0000] {processor.py:154} INFO - Started process (PID=1915) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:40:16.922+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:40:16.939+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:40:16.925+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:40:17.193+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:40:18.024+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:40:18.023+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:40:18.288+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:40:18.287+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:40:18.681+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.805 seconds
[2022-12-12T12:40:29.246+0000] {processor.py:154} INFO - Started process (PID=1925) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:40:29.287+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:40:29.293+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:40:29.291+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:40:29.439+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:40:29.673+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:40:29.672+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:40:30.038+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:40:30.025+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:40:30.467+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.261 seconds
[2022-12-12T12:40:41.034+0000] {processor.py:154} INFO - Started process (PID=1942) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:40:41.040+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:40:41.044+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:40:41.043+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:40:41.282+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:40:41.879+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:40:41.878+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:40:42.573+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:40:42.572+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:40:43.115+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.123 seconds
[2022-12-12T12:40:53.640+0000] {processor.py:154} INFO - Started process (PID=1952) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:40:53.643+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:40:53.650+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:40:53.649+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:40:53.777+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:40:53.946+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:40:53.945+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:40:54.124+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:40:54.123+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:40:54.325+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.732 seconds
[2022-12-12T12:41:04.723+0000] {processor.py:154} INFO - Started process (PID=1962) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:41:04.747+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:41:04.750+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:41:04.749+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:41:04.866+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:41:05.581+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:41:05.580+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:41:06.111+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:41:06.110+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:41:06.641+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.949 seconds
[2022-12-12T12:41:17.281+0000] {processor.py:154} INFO - Started process (PID=1972) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:41:17.293+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:41:17.301+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:41:17.300+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:41:17.771+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:41:19.333+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T12:41:30.801+0000] {processor.py:154} INFO - Started process (PID=1989) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:41:30.808+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:41:30.821+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:41:30.820+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:41:31.026+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:41:31.583+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:41:31.581+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:41:32.525+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:41:32.524+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:41:33.385+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.612 seconds
[2022-12-12T12:41:44.563+0000] {processor.py:154} INFO - Started process (PID=1999) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:41:44.596+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:41:44.604+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:41:44.603+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:41:45.093+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:41:46.860+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:41:46.859+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:41:47.599+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:41:47.598+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:41:48.271+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.800 seconds
[2022-12-12T12:41:58.778+0000] {processor.py:154} INFO - Started process (PID=2009) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:41:58.784+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:41:58.809+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:41:58.808+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:41:59.171+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:41:59.649+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:41:59.648+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:42:00.297+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:42:00.296+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:42:00.565+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.840 seconds
[2022-12-12T12:42:11.175+0000] {processor.py:154} INFO - Started process (PID=2019) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:42:11.200+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:42:11.204+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:42:11.203+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:42:11.843+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:42:12.460+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:42:12.459+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:42:12.840+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:42:12.838+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:42:13.102+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.963 seconds
[2022-12-12T12:42:24.084+0000] {processor.py:154} INFO - Started process (PID=2035) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:42:24.108+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:42:24.120+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:42:24.119+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:42:24.455+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:42:26.786+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:42:26.777+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:42:27.438+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:42:27.433+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:42:27.919+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.945 seconds
[2022-12-12T12:42:39.100+0000] {processor.py:154} INFO - Started process (PID=2046) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:42:39.140+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:42:39.168+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:42:39.167+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:42:39.808+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:42:40.568+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:42:40.567+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:42:41.169+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:42:41.168+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:42:41.508+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.553 seconds
[2022-12-12T12:42:52.361+0000] {processor.py:154} INFO - Started process (PID=2056) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:42:52.397+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:42:52.427+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:42:52.408+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:42:53.101+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:42:53.662+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:42:53.660+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:42:54.013+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:42:54.012+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:42:54.503+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.243 seconds
[2022-12-12T12:43:05.172+0000] {processor.py:154} INFO - Started process (PID=2066) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:43:05.198+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:43:05.204+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:43:05.203+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:43:05.482+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:43:05.973+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:43:05.972+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:43:06.436+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:43:06.435+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:43:06.754+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.627 seconds
[2022-12-12T12:43:17.653+0000] {processor.py:154} INFO - Started process (PID=2076) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:43:17.711+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:43:17.733+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:43:17.732+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:43:18.046+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:43:18.436+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:43:18.435+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:43:18.903+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:43:18.903+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:43:19.355+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.813 seconds
[2022-12-12T12:43:29.976+0000] {processor.py:154} INFO - Started process (PID=2093) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:43:30.018+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:43:30.037+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:43:30.034+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:43:30.741+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:43:31.781+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:43:31.780+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:43:32.257+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:43:32.256+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:43:32.774+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.865 seconds
[2022-12-12T12:43:43.390+0000] {processor.py:154} INFO - Started process (PID=2103) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:43:43.435+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:43:43.459+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:43:43.458+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:43:44.001+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:43:45.654+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:43:45.649+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:43:46.229+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:43:46.228+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:43:46.728+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.480 seconds
[2022-12-12T12:43:58.063+0000] {processor.py:154} INFO - Started process (PID=2113) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:43:58.088+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:43:58.147+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:43:58.132+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:43:58.485+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:43:59.612+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T12:44:10.678+0000] {processor.py:154} INFO - Started process (PID=2123) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:44:10.708+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:44:10.716+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:44:10.715+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:44:11.043+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:44:11.857+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:44:11.851+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:44:12.133+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:44:12.132+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:44:12.413+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.805 seconds
[2022-12-12T12:44:22.835+0000] {processor.py:154} INFO - Started process (PID=2140) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:44:22.879+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:44:22.882+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:44:22.882+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:44:23.285+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:44:24.490+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:44:24.489+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:44:24.777+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:44:24.776+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:44:25.056+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.253 seconds
[2022-12-12T12:44:35.312+0000] {processor.py:154} INFO - Started process (PID=2151) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:44:35.371+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:44:35.376+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:44:35.375+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:44:35.774+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:44:36.301+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:44:36.300+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:44:36.531+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:44:36.530+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:44:36.700+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.412 seconds
[2022-12-12T12:44:47.120+0000] {processor.py:154} INFO - Started process (PID=2161) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:44:47.194+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:44:47.197+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:44:47.196+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:44:47.304+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:44:47.481+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:44:47.480+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:44:47.699+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:44:47.698+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:44:47.983+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.886 seconds
[2022-12-12T12:44:58.186+0000] {processor.py:154} INFO - Started process (PID=2171) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:44:58.198+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:44:58.202+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:44:58.201+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:44:58.310+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:44:59.238+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:44:59.237+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:44:59.368+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:44:59.367+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:44:59.527+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.356 seconds
[2022-12-12T12:45:09.857+0000] {processor.py:154} INFO - Started process (PID=2190) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:45:09.898+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:45:09.908+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:45:09.901+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:45:10.127+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:45:10.568+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:45:10.567+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:45:10.858+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:45:10.857+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:45:11.132+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.301 seconds
[2022-12-12T12:45:21.383+0000] {processor.py:154} INFO - Started process (PID=2200) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:45:21.394+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:45:21.403+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:45:21.402+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:45:21.543+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:45:22.215+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:45:22.213+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:45:22.400+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:45:22.399+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:45:22.587+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.235 seconds
[2022-12-12T12:45:32.904+0000] {processor.py:154} INFO - Started process (PID=2210) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:45:32.948+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:45:32.952+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:45:32.951+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:45:33.087+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:45:33.264+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:45:33.263+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:45:33.439+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:45:33.438+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:45:33.693+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.808 seconds
[2022-12-12T12:45:44.057+0000] {processor.py:154} INFO - Started process (PID=2220) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:45:44.074+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:45:44.078+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:45:44.077+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:45:44.196+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:45:44.472+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:45:44.471+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:45:44.715+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:45:44.714+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:45:44.902+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.863 seconds
[2022-12-12T12:45:55.351+0000] {processor.py:154} INFO - Started process (PID=2238) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:45:55.377+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:45:55.381+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:45:55.380+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:45:55.591+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:45:55.970+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:45:55.961+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:45:56.271+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:45:56.270+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:45:56.550+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.230 seconds
[2022-12-12T12:46:07.140+0000] {processor.py:154} INFO - Started process (PID=2248) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:46:07.187+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:46:07.191+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:46:07.190+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:46:07.365+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:46:07.613+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:46:07.612+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:46:07.773+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:46:07.772+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:46:07.937+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.830 seconds
[2022-12-12T12:46:18.831+0000] {processor.py:154} INFO - Started process (PID=2258) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:46:18.888+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:46:18.893+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:46:18.892+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:46:19.126+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:46:19.877+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:46:19.876+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:46:20.052+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:46:20.052+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:46:20.291+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.478 seconds
[2022-12-12T12:46:30.717+0000] {processor.py:154} INFO - Started process (PID=2268) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:46:30.744+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:46:30.749+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:46:30.748+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:46:30.868+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:46:31.057+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:46:31.055+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:46:31.215+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:46:31.214+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:46:31.403+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.706 seconds
[2022-12-12T12:46:42.099+0000] {processor.py:154} INFO - Started process (PID=2286) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:46:42.139+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:46:42.144+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:46:42.143+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:46:42.399+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:46:43.364+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T12:46:54.171+0000] {processor.py:154} INFO - Started process (PID=2296) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:46:54.178+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:46:54.182+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:46:54.181+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:46:54.293+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:46:55.441+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:46:55.440+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:46:55.619+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:46:55.618+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:46:56.127+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.990 seconds
[2022-12-12T12:47:06.483+0000] {processor.py:154} INFO - Started process (PID=2306) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:47:06.492+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:47:06.497+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:47:06.495+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:47:06.639+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:47:07.806+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T12:47:18.214+0000] {processor.py:154} INFO - Started process (PID=2316) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:47:18.257+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:47:18.262+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:47:18.261+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:47:18.362+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:47:18.518+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:47:18.517+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:47:18.659+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:47:18.658+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:47:18.942+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.745 seconds
[2022-12-12T12:47:29.397+0000] {processor.py:154} INFO - Started process (PID=2334) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:47:29.431+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:47:29.437+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:47:29.436+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:47:29.828+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:47:30.149+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:47:30.148+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:47:30.502+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:47:30.501+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:47:30.736+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.411 seconds
[2022-12-12T12:47:41.150+0000] {processor.py:154} INFO - Started process (PID=2344) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:47:41.154+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:47:41.159+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:47:41.157+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:47:41.286+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:47:41.467+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:47:41.466+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:47:41.695+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:47:41.694+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:47:41.840+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.704 seconds
[2022-12-12T12:47:52.308+0000] {processor.py:154} INFO - Started process (PID=2354) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:47:52.331+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:47:52.338+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:47:52.333+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:47:52.537+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:47:54.005+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T12:48:04.821+0000] {processor.py:154} INFO - Started process (PID=2372) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:48:04.873+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:48:04.888+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:48:04.887+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:48:05.299+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:48:05.737+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:48:05.736+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:48:05.984+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:48:05.982+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:48:06.314+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.557 seconds
[2022-12-12T12:48:17.277+0000] {processor.py:154} INFO - Started process (PID=2383) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:48:17.308+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:48:17.360+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:48:17.359+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:48:17.935+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:48:18.605+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T12:48:29.240+0000] {processor.py:154} INFO - Started process (PID=2393) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:48:29.273+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:48:29.279+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:48:29.276+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:48:29.407+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:48:29.941+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:48:29.940+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:48:30.084+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:48:30.083+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:48:30.268+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.061 seconds
[2022-12-12T12:48:40.661+0000] {processor.py:154} INFO - Started process (PID=2403) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:48:40.685+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:48:40.689+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:48:40.688+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:48:40.841+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:48:41.059+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:48:41.054+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:48:41.230+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:48:41.229+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:48:41.391+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.747 seconds
[2022-12-12T12:48:51.983+0000] {processor.py:154} INFO - Started process (PID=2420) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:48:52.026+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:48:52.034+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:48:52.029+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:48:52.441+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:48:52.704+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:48:52.703+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:48:53.134+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:48:53.126+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:48:53.468+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.560 seconds
[2022-12-12T12:49:04.230+0000] {processor.py:154} INFO - Started process (PID=2431) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:49:04.253+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:49:04.257+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:49:04.256+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:49:04.477+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:49:05.189+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:49:05.187+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:49:05.440+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:49:05.439+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:49:05.648+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.478 seconds
[2022-12-12T12:49:16.014+0000] {processor.py:154} INFO - Started process (PID=2441) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:49:16.043+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:49:16.047+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:49:16.046+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:49:16.169+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:49:16.681+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:49:16.679+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:49:16.878+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:49:16.877+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:49:17.128+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.140 seconds
[2022-12-12T12:49:27.725+0000] {processor.py:154} INFO - Started process (PID=2451) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:49:27.767+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:49:27.774+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:49:27.773+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:49:27.962+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:49:28.159+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:49:28.158+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:49:28.298+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:49:28.297+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:49:28.459+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.821 seconds
[2022-12-12T12:49:39.048+0000] {processor.py:154} INFO - Started process (PID=2469) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:49:39.059+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:49:39.071+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:49:39.070+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:49:39.597+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:49:40.081+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:49:40.080+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:49:40.329+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:49:40.328+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:49:40.561+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.559 seconds
[2022-12-12T12:49:51.188+0000] {processor.py:154} INFO - Started process (PID=2479) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:49:51.205+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:49:51.215+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:49:51.214+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:49:51.479+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:49:51.815+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:49:51.812+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:49:51.959+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:49:51.957+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:49:52.210+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.066 seconds
[2022-12-12T12:50:02.583+0000] {processor.py:154} INFO - Started process (PID=2489) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:50:02.587+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:50:02.592+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:50:02.591+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:50:02.736+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:50:03.292+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:50:03.291+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:50:03.602+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:50:03.601+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:50:03.844+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.284 seconds
[2022-12-12T12:50:15.235+0000] {processor.py:154} INFO - Started process (PID=2499) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:50:15.278+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:50:15.293+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:50:15.291+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:50:15.704+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:50:15.992+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:50:15.991+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:50:16.160+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:50:16.159+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:50:16.298+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.241 seconds
[2022-12-12T12:50:26.885+0000] {processor.py:154} INFO - Started process (PID=2516) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:50:26.897+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:50:26.901+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:50:26.900+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:50:27.156+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:50:27.965+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:50:27.964+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:50:28.349+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:50:28.348+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:50:28.581+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.724 seconds
[2022-12-12T12:50:38.919+0000] {processor.py:154} INFO - Started process (PID=2526) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:50:38.969+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:50:38.973+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:50:38.972+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:50:39.250+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:50:39.876+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:50:39.875+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:50:40.217+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:50:40.216+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:50:40.416+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.512 seconds
[2022-12-12T12:50:51.270+0000] {processor.py:154} INFO - Started process (PID=2536) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:50:51.300+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:50:51.304+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:50:51.303+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:50:51.542+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:50:51.817+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:50:51.816+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:50:52.145+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:50:52.144+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:50:52.289+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.051 seconds
[2022-12-12T12:51:02.716+0000] {processor.py:154} INFO - Started process (PID=2546) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:51:02.727+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:51:02.736+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:51:02.735+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:51:02.905+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:51:03.347+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:51:03.346+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:51:04.018+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:51:03.977+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:51:04.444+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.776 seconds
[2022-12-12T12:51:15.054+0000] {processor.py:154} INFO - Started process (PID=2564) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:51:15.058+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:51:15.070+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:51:15.061+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:51:15.276+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:51:16.658+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:51:16.657+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:51:16.989+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:51:16.988+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:51:17.509+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.492 seconds
[2022-12-12T12:51:27.977+0000] {processor.py:154} INFO - Started process (PID=2574) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:51:28.030+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:51:28.037+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:51:28.033+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:51:28.264+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:51:29.124+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:51:29.123+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:51:29.350+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:51:29.348+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:51:29.867+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.918 seconds
[2022-12-12T12:51:40.308+0000] {processor.py:154} INFO - Started process (PID=2587) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:51:40.329+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:51:40.333+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:51:40.332+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:51:40.750+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:51:41.179+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:51:41.177+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:51:41.475+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:51:41.473+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:51:41.700+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.487 seconds
[2022-12-12T12:51:52.727+0000] {processor.py:154} INFO - Started process (PID=2605) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:51:52.747+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:51:52.779+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:51:52.749+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:51:53.056+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:51:54.285+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:51:54.284+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:51:54.613+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:51:54.612+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:51:55.046+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.436 seconds
[2022-12-12T12:52:05.628+0000] {processor.py:154} INFO - Started process (PID=2615) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:52:05.659+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:52:05.675+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:52:05.674+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:52:05.943+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:52:06.500+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:52:06.499+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:52:06.682+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:52:06.681+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:52:07.690+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.125 seconds
[2022-12-12T12:52:18.011+0000] {processor.py:154} INFO - Started process (PID=2627) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:52:18.058+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:52:18.066+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:52:18.061+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:52:18.178+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:52:18.373+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:52:18.372+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:52:18.527+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:52:18.525+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:52:18.749+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.757 seconds
[2022-12-12T12:52:29.124+0000] {processor.py:154} INFO - Started process (PID=2635) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:52:29.171+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:52:29.185+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:52:29.184+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:52:29.334+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:52:29.663+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:52:29.662+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:52:29.844+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:52:29.843+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:52:30.018+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.908 seconds
[2022-12-12T12:52:40.673+0000] {processor.py:154} INFO - Started process (PID=2653) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:52:40.686+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:52:40.690+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:52:40.689+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:52:40.945+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:52:42.453+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:52:42.452+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:52:42.807+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:52:42.806+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:52:43.143+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.524 seconds
[2022-12-12T12:52:53.668+0000] {processor.py:154} INFO - Started process (PID=2665) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:52:53.674+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:52:53.678+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:52:53.677+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:52:53.815+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:52:54.349+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:52:54.349+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:52:54.566+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:52:54.561+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:52:54.837+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.207 seconds
[2022-12-12T12:53:05.232+0000] {processor.py:154} INFO - Started process (PID=2675) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:53:05.257+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:53:05.260+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:53:05.260+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:53:05.378+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:53:05.723+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:53:05.722+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:53:05.876+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:53:05.876+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:53:06.119+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.903 seconds
[2022-12-12T12:53:16.733+0000] {processor.py:154} INFO - Started process (PID=2683) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:53:16.743+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:53:16.747+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:53:16.746+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:53:16.933+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:53:18.746+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:53:18.745+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:53:19.153+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:53:19.152+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:53:19.447+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.789 seconds
[2022-12-12T12:53:30.430+0000] {processor.py:154} INFO - Started process (PID=2703) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:53:30.437+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:53:30.443+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:53:30.441+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:53:30.671+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:53:31.125+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:53:31.124+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:53:31.533+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:53:31.532+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:53:31.733+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.347 seconds
[2022-12-12T12:53:42.847+0000] {processor.py:154} INFO - Started process (PID=2713) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:53:42.859+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:53:42.875+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:53:42.874+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:53:43.025+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:53:43.581+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:53:43.580+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:53:43.728+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:53:43.727+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:53:43.911+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.096 seconds
[2022-12-12T12:53:54.325+0000] {processor.py:154} INFO - Started process (PID=2723) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:53:54.352+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:53:54.358+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:53:54.357+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:53:54.481+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:53:54.747+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:53:54.746+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:53:54.924+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:53:54.923+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:53:55.095+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.789 seconds
[2022-12-12T12:54:05.659+0000] {processor.py:154} INFO - Started process (PID=2738) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:54:05.698+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:54:05.708+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:54:05.707+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:54:06.064+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:54:06.604+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:54:06.603+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:54:06.793+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:54:06.792+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:54:07.107+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.486 seconds
[2022-12-12T12:54:17.579+0000] {processor.py:154} INFO - Started process (PID=2750) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:54:17.586+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:54:17.595+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:54:17.594+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:54:17.719+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:54:17.886+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:54:17.885+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:54:18.030+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:54:18.029+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:54:18.215+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.660 seconds
[2022-12-12T12:54:28.715+0000] {processor.py:154} INFO - Started process (PID=2758) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:54:28.756+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:54:28.760+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:54:28.759+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:54:29.089+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:54:29.949+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:54:29.948+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:54:30.393+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:54:30.392+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:54:30.713+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.087 seconds
[2022-12-12T12:54:41.182+0000] {processor.py:154} INFO - Started process (PID=2768) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:54:41.299+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:54:41.323+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:54:41.322+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:54:41.547+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:54:42.215+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:54:42.213+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:54:42.396+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:54:42.396+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:54:42.584+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.434 seconds
[2022-12-12T12:54:53.174+0000] {processor.py:154} INFO - Started process (PID=2786) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:54:53.200+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:54:53.228+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:54:53.213+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:54:53.512+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:54:54.417+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:54:54.416+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:54:54.865+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:54:54.864+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:54:55.191+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.039 seconds
[2022-12-12T12:55:05.469+0000] {processor.py:154} INFO - Started process (PID=2795) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:55:05.522+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:55:05.532+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:55:05.526+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:55:05.718+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:55:05.960+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:55:05.959+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:55:06.209+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:55:06.208+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:55:06.382+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.931 seconds
[2022-12-12T12:55:16.671+0000] {processor.py:154} INFO - Started process (PID=2803) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:55:16.691+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:55:16.696+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:55:16.695+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:55:16.803+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:55:17.112+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:55:17.111+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:55:17.325+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:55:17.324+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:55:17.704+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.049 seconds
[2022-12-12T12:55:28.125+0000] {processor.py:154} INFO - Started process (PID=2813) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:55:28.149+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:55:28.153+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:55:28.152+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:55:28.254+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:55:29.063+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:55:29.062+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:55:29.239+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:55:29.238+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:55:29.552+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.445 seconds
[2022-12-12T12:55:40.016+0000] {processor.py:154} INFO - Started process (PID=2831) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:55:40.068+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:55:40.099+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:55:40.098+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:55:40.411+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:55:41.069+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:55:41.068+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:55:41.429+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:55:41.428+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:55:41.811+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.891 seconds
[2022-12-12T12:55:52.435+0000] {processor.py:154} INFO - Started process (PID=2841) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:55:52.501+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:55:52.505+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:55:52.504+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:55:52.644+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:55:52.842+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:55:52.841+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:55:53.002+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:55:53.001+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:55:53.188+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.783 seconds
[2022-12-12T12:56:03.449+0000] {processor.py:154} INFO - Started process (PID=2851) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:56:03.560+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:56:03.564+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:56:03.563+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:56:03.662+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:56:03.848+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:56:03.847+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:56:03.998+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:56:03.996+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:56:04.208+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.776 seconds
[2022-12-12T12:56:14.791+0000] {processor.py:154} INFO - Started process (PID=2861) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:56:14.809+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:56:14.839+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:56:14.838+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:56:15.246+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:56:16.346+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:56:16.343+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:56:16.732+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:56:16.727+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:56:17.084+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.399 seconds
[2022-12-12T12:56:27.799+0000] {processor.py:154} INFO - Started process (PID=2878) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:56:27.811+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:56:27.825+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:56:27.824+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:56:28.286+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:56:29.811+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:56:29.809+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:56:30.003+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:56:30.002+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:56:30.335+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.627 seconds
[2022-12-12T12:56:40.796+0000] {processor.py:154} INFO - Started process (PID=2893) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:56:40.879+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:56:40.926+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:56:40.897+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:56:41.266+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:56:42.567+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T12:56:53.416+0000] {processor.py:154} INFO - Started process (PID=2903) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:56:53.420+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:56:53.431+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:56:53.429+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:56:53.585+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:56:54.169+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:56:54.168+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:56:54.369+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:56:54.368+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:56:54.539+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.140 seconds
[2022-12-12T12:57:05.036+0000] {processor.py:154} INFO - Started process (PID=2921) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:57:05.075+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:57:05.089+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:57:05.088+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:57:05.359+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:57:06.170+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:57:06.169+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:57:06.461+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:57:06.453+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:57:06.753+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.757 seconds
[2022-12-12T12:57:17.569+0000] {processor.py:154} INFO - Started process (PID=2931) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:57:17.609+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:57:17.613+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:57:17.612+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:57:17.729+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:57:18.065+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:57:18.064+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:57:18.237+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:57:18.237+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:57:18.391+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.841 seconds
[2022-12-12T12:57:28.684+0000] {processor.py:154} INFO - Started process (PID=2941) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:57:28.712+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:57:28.719+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:57:28.718+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:57:28.862+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:57:29.438+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:57:29.437+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:57:29.696+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:57:29.695+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:57:29.871+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.207 seconds
[2022-12-12T12:57:40.127+0000] {processor.py:154} INFO - Started process (PID=2949) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:57:40.132+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:57:40.137+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:57:40.136+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:57:40.427+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:57:40.638+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:57:40.637+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:57:40.789+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:57:40.788+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:57:40.976+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.922 seconds
[2022-12-12T12:57:51.633+0000] {processor.py:154} INFO - Started process (PID=2967) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:57:51.663+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:57:51.674+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:57:51.665+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:57:52.112+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:57:53.661+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:57:53.660+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:57:53.954+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:57:53.953+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:57:54.158+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.589 seconds
[2022-12-12T12:58:04.629+0000] {processor.py:154} INFO - Started process (PID=2977) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:58:04.635+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:58:04.639+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:58:04.638+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:58:04.767+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:58:05.217+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:58:05.216+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:58:05.421+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:58:05.420+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:58:05.655+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.042 seconds
[2022-12-12T12:58:16.021+0000] {processor.py:154} INFO - Started process (PID=2987) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:58:16.067+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:58:16.076+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:58:16.073+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:58:16.443+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:58:16.659+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:58:16.658+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:58:16.831+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:58:16.831+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:58:17.047+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.101 seconds
[2022-12-12T12:58:27.756+0000] {processor.py:154} INFO - Started process (PID=3004) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:58:27.858+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:58:27.867+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:58:27.865+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:58:28.886+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:58:29.253+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:58:29.252+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:58:29.631+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:58:29.630+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:58:29.957+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.300 seconds
[2022-12-12T12:58:40.719+0000] {processor.py:154} INFO - Started process (PID=3015) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:58:40.731+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:58:40.752+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:58:40.733+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:58:41.039+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:58:41.406+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:58:41.405+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:58:41.965+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:58:41.945+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:58:43.423+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.767 seconds
[2022-12-12T12:58:53.785+0000] {processor.py:154} INFO - Started process (PID=3027) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:58:53.799+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:58:53.804+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:58:53.803+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:58:53.926+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:58:54.470+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:58:54.469+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:58:54.627+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:58:54.626+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:58:54.803+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.033 seconds
[2022-12-12T12:59:05.209+0000] {processor.py:154} INFO - Started process (PID=3037) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:59:05.234+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:59:05.237+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:59:05.236+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:59:05.346+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:59:05.778+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:59:05.776+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:59:05.957+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:59:05.956+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:59:06.118+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.929 seconds
[2022-12-12T12:59:16.749+0000] {processor.py:154} INFO - Started process (PID=3054) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:59:16.792+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:59:16.796+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:59:16.795+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:59:17.085+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:59:17.844+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T12:59:28.344+0000] {processor.py:154} INFO - Started process (PID=3064) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:59:28.359+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:59:28.377+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:59:28.376+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:59:28.791+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:59:29.575+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:59:29.573+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:59:29.845+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:59:29.844+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:59:30.052+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.772 seconds
[2022-12-12T12:59:40.484+0000] {processor.py:154} INFO - Started process (PID=3074) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:59:40.514+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:59:40.526+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:59:40.517+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:59:40.851+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:59:41.733+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:59:41.732+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:59:41.902+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:59:41.901+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:59:42.097+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.638 seconds
[2022-12-12T12:59:52.525+0000] {processor.py:154} INFO - Started process (PID=3084) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:59:52.547+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T12:59:52.555+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:59:52.553+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:59:52.817+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T12:59:53.273+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:59:53.272+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T12:59:53.463+0000] {logging_mixin.py:137} INFO - [2022-12-12T12:59:53.462+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T12:59:53.658+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.175 seconds
[2022-12-12T13:00:04.368+0000] {processor.py:154} INFO - Started process (PID=3101) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:00:04.444+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:00:04.467+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:00:04.466+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:00:04.974+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:00:05.721+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:00:05.720+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:00:06.089+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:00:06.088+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:00:06.436+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.152 seconds
[2022-12-12T13:00:16.888+0000] {processor.py:154} INFO - Started process (PID=3110) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:00:16.924+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:00:16.928+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:00:16.927+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:00:17.223+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:00:18.078+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:00:18.077+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:00:18.241+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:00:18.240+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:00:18.516+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.671 seconds
[2022-12-12T13:00:28.965+0000] {processor.py:154} INFO - Started process (PID=3121) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:00:29.012+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:00:29.020+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:00:29.019+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:00:29.306+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:00:29.564+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:00:29.564+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:00:29.731+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:00:29.730+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:00:29.891+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.950 seconds
[2022-12-12T13:00:40.384+0000] {processor.py:154} INFO - Started process (PID=3131) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:00:40.395+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:00:40.408+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:00:40.407+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:00:40.639+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:00:40.850+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:00:40.849+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:00:41.006+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:00:41.005+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:00:41.327+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.993 seconds
[2022-12-12T13:00:52.331+0000] {processor.py:154} INFO - Started process (PID=3149) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:00:52.374+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:00:52.409+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:00:52.406+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:00:52.953+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:00:53.806+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:00:53.805+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:00:54.378+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:00:54.353+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:00:54.741+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.565 seconds
[2022-12-12T13:01:05.059+0000] {processor.py:154} INFO - Started process (PID=3159) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:01:05.071+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:01:05.075+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:01:05.074+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:01:05.453+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:01:06.293+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:01:06.292+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:01:06.460+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:01:06.459+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:01:06.727+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.694 seconds
[2022-12-12T13:01:17.171+0000] {processor.py:154} INFO - Started process (PID=3169) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:01:17.244+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:01:17.248+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:01:17.247+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:01:17.432+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:01:18.104+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:01:28.470+0000] {processor.py:154} INFO - Started process (PID=3185) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:01:28.475+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:01:28.481+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:01:28.480+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:01:28.759+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:01:30.557+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:01:30.556+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:01:31.069+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:01:31.068+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:01:31.314+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.862 seconds
[2022-12-12T13:01:41.856+0000] {processor.py:154} INFO - Started process (PID=3196) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:01:41.868+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:01:41.875+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:01:41.874+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:01:42.148+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:01:43.125+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:01:43.124+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:01:43.368+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:01:43.367+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:01:43.516+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.691 seconds
[2022-12-12T13:01:54.189+0000] {processor.py:154} INFO - Started process (PID=3206) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:01:54.251+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:01:54.258+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:01:54.257+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:01:54.523+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:01:54.837+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:01:54.836+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:01:55.096+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:01:55.095+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:01:56.194+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.060 seconds
[2022-12-12T13:02:06.600+0000] {processor.py:154} INFO - Started process (PID=3216) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:02:06.604+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:02:06.608+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:02:06.607+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:02:06.729+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:02:07.758+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:02:07.757+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:02:07.926+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:02:07.925+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:02:08.112+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.555 seconds
[2022-12-12T13:02:18.695+0000] {processor.py:154} INFO - Started process (PID=3237) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:02:18.702+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:02:18.710+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:02:18.705+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:02:18.921+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:02:19.371+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:02:19.370+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:02:19.741+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:02:19.740+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:02:20.009+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.332 seconds
[2022-12-12T13:02:30.352+0000] {processor.py:154} INFO - Started process (PID=3247) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:02:30.357+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:02:30.362+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:02:30.360+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:02:30.488+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:02:31.268+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:02:41.532+0000] {processor.py:154} INFO - Started process (PID=3257) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:02:41.553+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:02:41.557+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:02:41.556+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:02:41.697+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:02:41.862+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:02:41.861+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:02:42.019+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:02:42.017+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:02:42.219+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.716 seconds
[2022-12-12T13:02:52.661+0000] {processor.py:154} INFO - Started process (PID=3265) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:02:52.730+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:02:52.739+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:02:52.733+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:02:53.013+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:02:54.753+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:02:54.752+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:02:55.054+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:02:55.053+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:02:55.511+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.894 seconds
[2022-12-12T13:03:06.280+0000] {processor.py:154} INFO - Started process (PID=3285) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:03:06.288+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:03:06.293+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:03:06.292+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:03:06.466+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:03:08.688+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:03:08.687+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:03:09.139+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:03:09.138+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:03:09.331+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.071 seconds
[2022-12-12T13:03:19.719+0000] {processor.py:154} INFO - Started process (PID=3295) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:03:19.725+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:03:19.733+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:03:19.730+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:03:19.914+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:03:20.315+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:03:20.314+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:03:20.499+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:03:20.493+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:03:20.859+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.166 seconds
[2022-12-12T13:03:31.231+0000] {processor.py:154} INFO - Started process (PID=3305) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:03:31.243+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:03:31.247+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:03:31.246+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:03:31.426+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:03:31.669+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:03:31.668+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:03:31.905+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:03:31.904+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:03:32.150+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.941 seconds
[2022-12-12T13:03:42.843+0000] {processor.py:154} INFO - Started process (PID=3323) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:03:42.899+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:03:42.908+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:03:42.907+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:03:43.104+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:03:43.368+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:03:43.367+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:03:43.905+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:03:43.904+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:03:44.395+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.576 seconds
[2022-12-12T13:03:54.677+0000] {processor.py:154} INFO - Started process (PID=3333) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:03:54.681+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:03:54.684+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:03:54.683+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:03:54.804+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:03:55.021+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:03:55.020+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:03:55.194+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:03:55.193+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:03:55.379+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.727 seconds
[2022-12-12T13:04:05.724+0000] {processor.py:154} INFO - Started process (PID=3343) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:04:05.770+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:04:05.777+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:04:05.773+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:04:05.885+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:04:06.610+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:04:06.609+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:04:06.799+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:04:06.798+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:04:06.992+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.286 seconds
[2022-12-12T13:04:17.350+0000] {processor.py:154} INFO - Started process (PID=3353) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:04:17.450+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:04:17.467+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:04:17.458+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:04:17.595+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:04:17.809+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:04:17.808+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:04:18.019+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:04:18.018+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:04:18.272+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.954 seconds
[2022-12-12T13:04:28.617+0000] {processor.py:154} INFO - Started process (PID=3371) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:04:28.663+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:04:28.672+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:04:28.671+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:04:28.930+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:04:29.838+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:04:29.837+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:04:30.093+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:04:30.092+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:04:30.336+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.768 seconds
[2022-12-12T13:04:40.641+0000] {processor.py:154} INFO - Started process (PID=3381) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:04:40.647+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:04:40.652+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:04:40.651+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:04:40.760+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:04:41.260+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:04:41.259+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:04:41.421+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:04:41.420+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:04:41.607+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.985 seconds
[2022-12-12T13:04:52.012+0000] {processor.py:154} INFO - Started process (PID=3391) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:04:52.016+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:04:52.022+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:04:52.021+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:04:52.212+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:04:52.480+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:04:52.479+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:04:52.669+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:04:52.668+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:04:52.847+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.865 seconds
[2022-12-12T13:05:03.155+0000] {processor.py:154} INFO - Started process (PID=3401) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:05:03.167+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:05:03.171+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:05:03.170+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:05:03.280+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:05:03.469+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:05:03.467+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:05:03.678+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:05:03.677+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:05:03.907+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.772 seconds
[2022-12-12T13:05:14.438+0000] {processor.py:154} INFO - Started process (PID=3416) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:05:14.460+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:05:14.468+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:05:14.467+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:05:14.777+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:05:15.212+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:05:15.211+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:05:15.950+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:05:15.945+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:05:18.034+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.643 seconds
[2022-12-12T13:05:28.579+0000] {processor.py:154} INFO - Started process (PID=3426) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:05:28.622+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:05:28.627+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:05:28.625+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:05:28.852+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:05:29.675+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:05:40.179+0000] {processor.py:154} INFO - Started process (PID=3436) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:05:40.197+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:05:40.204+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:05:40.203+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:05:40.336+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:05:41.480+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:05:52.020+0000] {processor.py:154} INFO - Started process (PID=3446) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:05:52.056+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:05:52.077+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:05:52.068+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:05:52.282+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:05:52.824+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:05:52.823+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:05:52.989+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:05:52.988+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:05:53.189+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.227 seconds
[2022-12-12T13:06:03.457+0000] {processor.py:154} INFO - Started process (PID=3465) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:06:03.466+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:06:03.477+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:06:03.476+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:06:03.755+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:06:03.956+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:06:03.955+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:06:04.348+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:06:04.347+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:06:04.604+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.187 seconds
[2022-12-12T13:06:15.078+0000] {processor.py:154} INFO - Started process (PID=3477) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:06:15.082+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:06:15.087+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:06:15.086+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:06:15.201+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:06:15.383+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:06:15.381+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:06:15.560+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:06:15.559+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:06:15.763+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.713 seconds
[2022-12-12T13:06:26.751+0000] {processor.py:154} INFO - Started process (PID=3487) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:06:26.780+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:06:26.784+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:06:26.783+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:06:26.904+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:06:27.343+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:06:27.342+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:06:27.485+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:06:27.484+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:06:27.767+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.031 seconds
[2022-12-12T13:06:38.197+0000] {processor.py:154} INFO - Started process (PID=3497) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:06:38.215+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:06:38.224+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:06:38.223+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:06:38.447+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:06:39.636+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:06:50.042+0000] {processor.py:154} INFO - Started process (PID=3514) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:06:50.052+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:06:50.062+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:06:50.061+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:06:50.262+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:06:50.617+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:06:50.616+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:06:50.881+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:06:50.880+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:06:51.380+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.381 seconds
[2022-12-12T13:07:01.757+0000] {processor.py:154} INFO - Started process (PID=3522) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:07:01.795+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:07:01.800+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:07:01.798+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:07:02.078+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:07:03.173+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:07:13.820+0000] {processor.py:154} INFO - Started process (PID=3532) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:07:13.849+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:07:13.863+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:07:13.862+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:07:14.144+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:07:14.807+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:07:14.805+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:07:15.017+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:07:15.016+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:07:15.162+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.371 seconds
[2022-12-12T13:07:25.749+0000] {processor.py:154} INFO - Started process (PID=3549) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:07:25.773+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:07:25.786+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:07:25.777+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:07:26.092+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:07:26.890+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:07:26.889+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:07:27.327+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:07:27.326+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:07:27.536+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.852 seconds
[2022-12-12T13:07:38.287+0000] {processor.py:154} INFO - Started process (PID=3560) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:07:38.336+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:07:38.340+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:07:38.339+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:07:38.529+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:07:38.748+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:07:38.748+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:07:38.915+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:07:38.914+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:07:39.174+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.954 seconds
[2022-12-12T13:07:49.599+0000] {processor.py:154} INFO - Started process (PID=3570) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:07:49.627+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:07:49.635+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:07:49.634+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:07:49.828+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:07:50.148+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:07:50.147+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:07:50.349+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:07:50.348+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:07:50.515+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.948 seconds
[2022-12-12T13:08:01.183+0000] {processor.py:154} INFO - Started process (PID=3580) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:08:01.222+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:08:01.228+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:08:01.227+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:08:01.578+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:08:03.071+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:08:13.675+0000] {processor.py:154} INFO - Started process (PID=3598) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:08:13.698+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:08:13.704+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:08:13.703+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:08:14.036+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:08:14.665+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:08:14.664+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:08:14.885+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:08:14.884+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:08:15.273+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.637 seconds
[2022-12-12T13:08:25.867+0000] {processor.py:154} INFO - Started process (PID=3608) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:08:25.888+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:08:25.894+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:08:25.893+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:08:26.091+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:08:26.637+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:08:26.636+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:08:26.950+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:08:26.949+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:08:28.177+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.352 seconds
[2022-12-12T13:08:38.639+0000] {processor.py:154} INFO - Started process (PID=3618) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:08:38.644+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:08:38.666+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:08:38.656+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:08:38.884+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:08:40.405+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:08:51.012+0000] {processor.py:154} INFO - Started process (PID=3628) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:08:51.066+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:08:51.073+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:08:51.069+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:08:51.892+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:08:52.348+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:08:52.347+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:08:52.569+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:08:52.568+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:08:52.786+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.850 seconds
[2022-12-12T13:09:03.568+0000] {processor.py:154} INFO - Started process (PID=3645) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:09:03.636+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:09:03.645+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:09:03.644+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:09:04.108+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:09:05.381+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:09:15.809+0000] {processor.py:154} INFO - Started process (PID=3657) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:09:15.814+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:09:15.819+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:09:15.818+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:09:15.924+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:09:16.382+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:09:26.908+0000] {processor.py:154} INFO - Started process (PID=3667) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:09:26.947+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:09:26.951+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:09:26.950+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:09:27.048+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:09:27.237+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:09:27.236+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:09:27.376+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:09:27.375+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:09:27.542+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.664 seconds
[2022-12-12T13:09:38.047+0000] {processor.py:154} INFO - Started process (PID=3684) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:09:38.051+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:09:38.056+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:09:38.055+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:09:38.183+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:09:38.627+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:09:38.626+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:09:38.939+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:09:38.938+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:09:39.275+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.250 seconds
[2022-12-12T13:09:49.851+0000] {processor.py:154} INFO - Started process (PID=3695) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:09:49.870+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:09:49.877+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:09:49.876+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:09:50.031+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:09:50.752+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:10:01.137+0000] {processor.py:154} INFO - Started process (PID=3705) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:10:01.183+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:10:01.191+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:10:01.185+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:10:01.402+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:10:01.615+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:10:01.613+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:10:01.770+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:10:01.769+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:10:01.950+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.829 seconds
[2022-12-12T13:10:12.192+0000] {processor.py:154} INFO - Started process (PID=3713) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:10:12.213+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:10:12.233+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:10:12.232+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:10:12.346+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:10:12.576+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:10:12.575+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:10:12.712+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:10:12.711+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:10:12.876+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.720 seconds
[2022-12-12T13:10:23.568+0000] {processor.py:154} INFO - Started process (PID=3731) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:10:23.623+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:10:23.651+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:10:23.650+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:10:24.016+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:10:24.680+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:10:24.679+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:10:24.821+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:10:24.820+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:10:25.015+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.513 seconds
[2022-12-12T13:10:35.562+0000] {processor.py:154} INFO - Started process (PID=3743) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:10:35.571+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:10:35.579+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:10:35.578+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:10:35.759+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:10:36.268+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:10:36.267+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:10:36.631+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:10:36.630+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:10:36.943+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.404 seconds
[2022-12-12T13:10:47.263+0000] {processor.py:154} INFO - Started process (PID=3753) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:10:47.316+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:10:47.320+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:10:47.319+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:10:47.427+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:10:47.792+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:10:47.791+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:10:48.022+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:10:48.021+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:10:48.439+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.194 seconds
[2022-12-12T13:10:58.836+0000] {processor.py:154} INFO - Started process (PID=3761) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:10:58.880+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:10:58.884+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:10:58.883+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:10:59.109+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:10:59.332+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:10:59.331+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:10:59.596+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:10:59.596+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:11:00.040+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.232 seconds
[2022-12-12T13:11:10.629+0000] {processor.py:154} INFO - Started process (PID=3779) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:11:10.692+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:11:10.700+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:11:10.699+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:11:11.308+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:11:11.720+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:11:11.719+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:11:12.335+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:11:12.334+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:11:13.088+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.545 seconds
[2022-12-12T13:11:23.512+0000] {processor.py:154} INFO - Started process (PID=3789) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:11:23.515+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:11:23.520+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:11:23.519+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:11:23.617+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:11:25.079+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:11:25.078+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:11:25.226+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:11:25.225+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:11:25.409+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.916 seconds
[2022-12-12T13:11:35.980+0000] {processor.py:154} INFO - Started process (PID=3801) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:11:36.011+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:11:36.035+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:11:36.025+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:11:36.249+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:11:36.529+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:11:36.528+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:11:36.696+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:11:36.694+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:11:36.864+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.995 seconds
[2022-12-12T13:11:47.333+0000] {processor.py:154} INFO - Started process (PID=3816) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:11:47.387+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:11:47.408+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:11:47.406+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:11:47.858+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:11:49.006+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:11:59.732+0000] {processor.py:154} INFO - Started process (PID=3826) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:11:59.758+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:11:59.763+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:11:59.761+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:11:59.886+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:12:01.798+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:12:01.797+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:12:02.837+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:12:02.836+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:12:03.128+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.423 seconds
[2022-12-12T13:12:13.399+0000] {processor.py:154} INFO - Started process (PID=3839) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:12:13.429+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:12:13.433+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:12:13.432+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:12:13.589+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:12:13.778+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:12:13.777+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:12:14.074+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:12:14.073+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:12:14.229+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.858 seconds
[2022-12-12T13:12:24.536+0000] {processor.py:154} INFO - Started process (PID=3849) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:12:24.565+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:12:24.569+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:12:24.568+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:12:24.756+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:12:24.964+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:12:24.963+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:12:25.288+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:12:25.281+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:12:25.543+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.033 seconds
[2022-12-12T13:12:36.528+0000] {processor.py:154} INFO - Started process (PID=3865) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:12:36.544+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:12:36.565+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:12:36.557+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:12:37.009+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:12:37.591+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:12:37.569+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:12:37.889+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:12:37.888+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:12:38.186+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.787 seconds
[2022-12-12T13:12:48.827+0000] {processor.py:154} INFO - Started process (PID=3875) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:12:48.875+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:12:48.878+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:12:48.878+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:12:49.027+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:12:49.228+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:12:49.227+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:12:49.482+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:12:49.473+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:12:49.948+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.178 seconds
[2022-12-12T13:13:00.608+0000] {processor.py:154} INFO - Started process (PID=3885) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:13:00.675+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:13:00.680+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:13:00.679+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:13:00.907+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:13:01.111+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:13:01.110+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:13:01.236+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:13:01.236+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:13:01.415+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.829 seconds
[2022-12-12T13:13:11.771+0000] {processor.py:154} INFO - Started process (PID=3892) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:13:11.791+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:13:11.803+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:13:11.793+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:13:12.062+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:13:12.887+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:13:12.886+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:13:13.171+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:13:13.170+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:13:13.658+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.918 seconds
[2022-12-12T13:13:24.337+0000] {processor.py:154} INFO - Started process (PID=3910) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:13:24.365+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:13:24.372+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:13:24.371+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:13:24.544+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:13:24.945+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:13:24.944+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:13:25.276+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:13:25.269+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:13:25.800+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.482 seconds
[2022-12-12T13:13:36.150+0000] {processor.py:154} INFO - Started process (PID=3920) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:13:36.153+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:13:36.157+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:13:36.156+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:13:36.254+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:13:37.557+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:13:37.556+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:13:37.741+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:13:37.740+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:13:37.954+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.818 seconds
[2022-12-12T13:13:48.397+0000] {processor.py:154} INFO - Started process (PID=3933) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:13:48.431+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:13:48.435+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:13:48.434+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:13:48.664+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:13:49.857+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:14:00.645+0000] {processor.py:154} INFO - Started process (PID=3951) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:14:00.697+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:14:00.709+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:14:00.707+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:14:01.149+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:14:01.796+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:14:01.795+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:14:02.060+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:14:02.059+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:14:02.349+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.871 seconds
[2022-12-12T13:14:12.780+0000] {processor.py:154} INFO - Started process (PID=3961) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:14:12.808+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:14:12.812+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:14:12.811+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:14:12.910+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:14:13.704+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:14:13.703+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:14:13.897+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:14:13.896+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:14:14.244+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.479 seconds
[2022-12-12T13:14:25.090+0000] {processor.py:154} INFO - Started process (PID=3970) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:14:25.151+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:14:25.155+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:14:25.154+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:14:25.382+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:14:25.559+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:14:25.558+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:14:25.695+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:14:25.695+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:14:25.843+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.768 seconds
[2022-12-12T13:14:36.187+0000] {processor.py:154} INFO - Started process (PID=3978) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:14:36.232+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:14:36.236+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:14:36.235+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:14:36.344+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:14:36.605+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:14:36.604+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:14:36.772+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:14:36.769+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:14:36.980+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.822 seconds
[2022-12-12T13:14:47.467+0000] {processor.py:154} INFO - Started process (PID=3995) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:14:47.512+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:14:47.527+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:14:47.526+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:14:47.773+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:14:48.029+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:14:48.028+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:14:48.253+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:14:48.252+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:14:48.491+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.055 seconds
[2022-12-12T13:14:58.723+0000] {processor.py:154} INFO - Started process (PID=4005) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:14:58.731+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:14:58.735+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:14:58.734+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:14:58.957+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:14:59.934+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:15:10.389+0000] {processor.py:154} INFO - Started process (PID=4015) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:15:10.416+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:15:10.421+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:15:10.419+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:15:10.627+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:15:12.230+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:15:22.734+0000] {processor.py:154} INFO - Started process (PID=4025) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:15:22.743+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:15:22.752+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:15:22.751+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:15:22.960+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:15:23.340+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:15:23.339+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:15:24.203+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:15:24.202+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:15:24.482+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.786 seconds
[2022-12-12T13:15:35.078+0000] {processor.py:154} INFO - Started process (PID=4043) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:15:35.110+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:15:35.118+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:15:35.113+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:15:35.494+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:15:35.742+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:15:35.740+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:15:35.911+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:15:35.910+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:15:36.103+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.095 seconds
[2022-12-12T13:15:46.604+0000] {processor.py:154} INFO - Started process (PID=4053) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:15:46.645+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:15:46.649+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:15:46.648+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:15:46.843+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:15:47.133+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:15:47.131+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:15:47.310+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:15:47.309+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:15:47.443+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.895 seconds
[2022-12-12T13:15:57.931+0000] {processor.py:154} INFO - Started process (PID=4063) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:15:57.934+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:15:57.938+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:15:57.937+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:15:58.033+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:15:58.266+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:15:58.265+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:15:58.706+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:15:58.681+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:15:59.256+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.339 seconds
[2022-12-12T13:16:09.978+0000] {processor.py:154} INFO - Started process (PID=4080) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:16:10.029+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:16:10.073+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:16:10.058+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:16:10.263+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:16:11.260+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:16:11.259+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:16:11.666+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:16:11.665+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:16:11.899+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.018 seconds
[2022-12-12T13:16:22.408+0000] {processor.py:154} INFO - Started process (PID=4091) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:16:22.508+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:16:22.525+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:16:22.524+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:16:22.771+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:16:23.036+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:16:23.036+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:16:23.273+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:16:23.272+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:16:23.502+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.111 seconds
[2022-12-12T13:16:33.656+0000] {processor.py:154} INFO - Started process (PID=4101) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:16:33.659+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:16:33.663+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:16:33.662+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:16:33.754+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:16:33.930+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:16:33.929+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:16:34.069+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:16:34.068+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:16:34.325+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.698 seconds
[2022-12-12T13:16:44.575+0000] {processor.py:154} INFO - Started process (PID=4111) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:16:44.622+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:16:44.626+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:16:44.625+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:16:44.737+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:16:45.617+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:16:45.616+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:16:45.898+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:16:45.893+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:16:46.198+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.638 seconds
[2022-12-12T13:16:57.038+0000] {processor.py:154} INFO - Started process (PID=4129) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:16:57.053+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:16:57.057+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:16:57.056+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:16:57.333+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:16:57.709+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:16:57.708+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:16:58.222+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:16:58.216+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:16:58.703+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.695 seconds
[2022-12-12T13:17:09.464+0000] {processor.py:154} INFO - Started process (PID=4139) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:17:09.518+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:17:09.524+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:17:09.521+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:17:09.720+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:17:09.929+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:17:09.928+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:17:10.093+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:17:10.092+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:17:10.295+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.876 seconds
[2022-12-12T13:17:20.768+0000] {processor.py:154} INFO - Started process (PID=4149) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:17:20.821+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:17:20.826+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:17:20.825+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:17:20.930+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:17:22.132+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:17:22.131+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:17:22.361+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:17:22.360+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:17:22.498+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.745 seconds
[2022-12-12T13:17:32.887+0000] {processor.py:154} INFO - Started process (PID=4159) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:17:32.932+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:17:32.936+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:17:32.935+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:17:33.053+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:17:33.360+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:17:33.359+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:17:33.642+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:17:33.641+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:17:34.215+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.355 seconds
[2022-12-12T13:17:44.604+0000] {processor.py:154} INFO - Started process (PID=4178) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:17:44.614+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:17:44.634+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:17:44.620+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:17:44.942+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:17:45.565+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:17:45.564+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:17:45.973+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:17:45.972+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:17:46.819+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.242 seconds
[2022-12-12T13:17:57.670+0000] {processor.py:154} INFO - Started process (PID=4188) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:17:57.728+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:17:57.751+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:17:57.750+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:17:58.232+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:17:59.141+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:17:59.133+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:17:59.319+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:17:59.318+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:17:59.493+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.893 seconds
[2022-12-12T13:18:10.026+0000] {processor.py:154} INFO - Started process (PID=4198) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:18:10.050+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:18:10.060+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:18:10.053+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:18:10.237+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:18:11.804+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:18:11.803+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:18:12.152+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:18:12.151+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:18:13.326+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.385 seconds
[2022-12-12T13:18:23.772+0000] {processor.py:154} INFO - Started process (PID=4215) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:18:23.799+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:18:23.802+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:18:23.801+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:18:24.001+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:18:24.965+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:18:24.964+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:18:25.418+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:18:25.417+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:18:27.347+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.606 seconds
[2022-12-12T13:18:37.896+0000] {processor.py:154} INFO - Started process (PID=4228) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:18:37.947+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:18:37.964+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:18:37.962+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:18:38.181+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:18:38.396+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:18:38.395+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:18:39.529+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:18:39.528+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:18:39.819+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.945 seconds
[2022-12-12T13:18:50.208+0000] {processor.py:154} INFO - Started process (PID=4238) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:18:50.223+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:18:50.238+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:18:50.235+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:18:50.395+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:18:50.769+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:18:50.768+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:18:50.935+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:18:50.934+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:18:51.088+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.896 seconds
[2022-12-12T13:19:01.414+0000] {processor.py:154} INFO - Started process (PID=4248) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:19:01.433+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:19:01.438+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:19:01.436+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:19:01.622+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:19:01.866+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:19:01.865+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:19:02.133+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:19:02.132+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:19:02.644+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.245 seconds
[2022-12-12T13:19:13.465+0000] {processor.py:154} INFO - Started process (PID=4267) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:19:13.516+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:19:13.563+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:19:13.544+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:19:14.058+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:19:15.308+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:19:26.109+0000] {processor.py:154} INFO - Started process (PID=4279) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:19:26.136+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:19:26.142+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:19:26.141+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:19:26.244+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:19:26.426+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:19:26.425+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:19:26.561+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:19:26.558+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:19:26.707+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.626 seconds
[2022-12-12T13:19:37.444+0000] {processor.py:154} INFO - Started process (PID=4289) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:19:37.459+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:19:37.476+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:19:37.472+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:19:37.673+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:19:38.170+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:19:38.168+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:19:38.353+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:19:38.353+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:19:38.663+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.239 seconds
[2022-12-12T13:19:49.175+0000] {processor.py:154} INFO - Started process (PID=4306) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:19:49.227+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:19:49.231+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:19:49.230+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:19:49.443+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:19:49.684+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:19:49.683+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:19:49.936+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:19:49.935+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:19:50.472+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.335 seconds
[2022-12-12T13:20:00.798+0000] {processor.py:154} INFO - Started process (PID=4317) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:20:00.851+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:20:00.857+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:20:00.856+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:20:01.323+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:20:01.793+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:20:01.792+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:20:01.958+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:20:01.957+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:20:02.169+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.429 seconds
[2022-12-12T13:20:12.531+0000] {processor.py:154} INFO - Started process (PID=4327) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:20:12.535+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:20:12.541+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:20:12.540+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:20:12.666+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:20:13.404+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:20:13.403+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:20:13.557+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:20:13.556+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:20:13.715+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.201 seconds
[2022-12-12T13:20:24.063+0000] {processor.py:154} INFO - Started process (PID=4337) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:20:24.070+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:20:24.075+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:20:24.074+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:20:24.185+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:20:24.960+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:20:24.959+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:20:25.126+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:20:25.125+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:20:25.292+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.249 seconds
[2022-12-12T13:20:35.670+0000] {processor.py:154} INFO - Started process (PID=4354) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:20:35.695+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:20:35.710+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:20:35.701+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:20:35.866+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:20:36.802+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:20:47.923+0000] {processor.py:154} INFO - Started process (PID=4364) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:20:47.971+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:20:47.976+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:20:47.975+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:20:48.072+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:20:48.245+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:20:48.244+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:20:48.378+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:20:48.376+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:20:48.547+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.648 seconds
[2022-12-12T13:20:58.844+0000] {processor.py:154} INFO - Started process (PID=4374) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:20:58.864+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:20:58.871+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:20:58.870+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:20:58.998+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:20:59.152+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:20:59.151+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:20:59.348+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:20:59.347+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:20:59.525+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.705 seconds
[2022-12-12T13:21:09.761+0000] {processor.py:154} INFO - Started process (PID=4384) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:21:09.812+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:21:09.817+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:21:09.815+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:21:09.955+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:21:10.180+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:21:10.179+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:21:10.347+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:21:10.347+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:21:10.600+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.858 seconds
[2022-12-12T13:21:20.912+0000] {processor.py:154} INFO - Started process (PID=4402) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:21:20.934+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:21:20.941+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:21:20.937+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:21:21.220+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:21:21.698+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:21:21.697+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:21:22.227+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:21:22.216+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:21:22.790+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.939 seconds
[2022-12-12T13:21:33.326+0000] {processor.py:154} INFO - Started process (PID=4412) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:21:33.358+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:21:33.366+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:21:33.361+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:21:33.471+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:21:33.968+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:21:44.424+0000] {processor.py:154} INFO - Started process (PID=4420) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:21:44.440+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:21:44.445+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:21:44.444+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:21:44.607+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:21:44.863+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:21:44.861+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:21:45.071+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:21:45.070+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:21:46.019+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.613 seconds
[2022-12-12T13:21:56.342+0000] {processor.py:154} INFO - Started process (PID=4430) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:21:56.394+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:21:56.401+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:21:56.400+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:21:56.560+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:21:57.734+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:21:57.733+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:21:58.682+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:21:58.681+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:21:58.852+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.541 seconds
[2022-12-12T13:22:09.555+0000] {processor.py:154} INFO - Started process (PID=4450) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:22:09.575+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:22:09.588+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:22:09.587+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:22:09.835+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:22:10.858+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:22:21.460+0000] {processor.py:154} INFO - Started process (PID=4460) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:22:21.483+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:22:21.495+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:22:21.494+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:22:21.666+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:22:21.835+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:22:21.834+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:22:21.971+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:22:21.970+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:22:22.114+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.677 seconds
[2022-12-12T13:22:32.419+0000] {processor.py:154} INFO - Started process (PID=4470) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:22:32.433+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:22:32.439+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:22:32.438+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:22:32.568+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:22:32.735+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:22:32.734+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:22:32.871+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:22:32.870+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:22:33.060+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.664 seconds
[2022-12-12T13:22:43.391+0000] {processor.py:154} INFO - Started process (PID=4487) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:22:43.453+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:22:43.458+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:22:43.457+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:22:43.585+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:22:43.894+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:22:43.889+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:22:44.195+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:22:44.185+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:22:44.439+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.082 seconds
[2022-12-12T13:22:55.007+0000] {processor.py:154} INFO - Started process (PID=4498) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:22:55.015+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:22:55.019+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:22:55.018+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:22:55.248+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:22:55.897+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:22:55.896+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:22:56.223+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:22:56.222+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:22:56.478+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.486 seconds
[2022-12-12T13:23:06.812+0000] {processor.py:154} INFO - Started process (PID=4508) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:23:06.818+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:23:06.829+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:23:06.828+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:23:06.933+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:23:08.119+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:23:18.616+0000] {processor.py:154} INFO - Started process (PID=4518) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:23:18.671+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:23:18.675+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:23:18.674+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:23:18.835+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:23:19.512+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:23:19.511+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:23:19.705+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:23:19.705+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:23:19.869+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.277 seconds
[2022-12-12T13:23:30.356+0000] {processor.py:154} INFO - Started process (PID=4536) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:23:30.461+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:23:30.466+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:23:30.464+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:23:30.884+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:23:31.540+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:23:31.539+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:23:31.823+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:23:31.822+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:23:32.095+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.787 seconds
[2022-12-12T13:23:42.288+0000] {processor.py:154} INFO - Started process (PID=4546) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:23:42.316+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:23:42.320+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:23:42.319+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:23:42.578+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:23:42.961+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:23:42.959+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:23:43.104+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:23:43.103+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:23:43.295+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.048 seconds
[2022-12-12T13:23:53.691+0000] {processor.py:154} INFO - Started process (PID=4556) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:23:53.740+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:23:53.744+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:23:53.743+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:23:53.865+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:23:54.264+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:23:54.264+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:23:54.396+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:23:54.395+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:23:54.589+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.945 seconds
[2022-12-12T13:24:05.008+0000] {processor.py:154} INFO - Started process (PID=4566) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:24:05.019+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:24:05.027+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:24:05.026+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:24:05.133+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:24:05.312+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:24:05.310+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:24:05.596+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:24:05.583+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:24:05.736+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.754 seconds
[2022-12-12T13:24:16.438+0000] {processor.py:154} INFO - Started process (PID=4583) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:24:16.476+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:24:16.489+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:24:16.488+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:24:16.943+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:24:18.476+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:24:18.475+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:24:18.849+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:24:18.848+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:24:19.214+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.888 seconds
[2022-12-12T13:24:29.700+0000] {processor.py:154} INFO - Started process (PID=4593) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:24:29.729+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:24:29.734+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:24:29.733+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:24:29.923+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:24:30.949+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:24:41.476+0000] {processor.py:154} INFO - Started process (PID=4603) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:24:41.547+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:24:41.552+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:24:41.551+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:24:41.654+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:24:41.868+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:24:41.866+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:24:42.020+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:24:42.019+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:24:43.008+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.547 seconds
[2022-12-12T13:24:53.527+0000] {processor.py:154} INFO - Started process (PID=4619) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:24:53.548+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:24:53.553+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:24:53.552+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:24:53.774+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:24:55.302+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:24:55.299+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:24:55.692+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:24:55.691+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:24:58.104+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 4.608 seconds
[2022-12-12T13:25:08.561+0000] {processor.py:154} INFO - Started process (PID=4630) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:25:08.607+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:25:08.611+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:25:08.610+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:25:08.743+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:25:08.945+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:25:08.944+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:25:09.969+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:25:09.967+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:25:10.183+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.658 seconds
[2022-12-12T13:25:20.501+0000] {processor.py:154} INFO - Started process (PID=4640) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:25:20.552+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:25:20.559+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:25:20.557+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:25:20.675+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:25:20.867+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:25:20.867+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:25:21.736+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:25:21.735+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:25:21.927+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.447 seconds
[2022-12-12T13:25:32.234+0000] {processor.py:154} INFO - Started process (PID=4650) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:25:32.286+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:25:32.294+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:25:32.293+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:25:32.405+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:25:32.568+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:25:32.566+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:25:32.744+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:25:32.743+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:25:32.972+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.756 seconds
[2022-12-12T13:25:43.566+0000] {processor.py:154} INFO - Started process (PID=4668) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:25:43.599+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:25:43.608+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:25:43.607+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:25:43.904+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:25:44.865+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:25:44.864+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:25:45.112+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:25:45.111+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:25:45.321+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.804 seconds
[2022-12-12T13:25:55.453+0000] {processor.py:154} INFO - Started process (PID=4678) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:25:55.458+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:25:55.468+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:25:55.468+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:25:55.618+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:25:56.353+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:25:56.352+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:25:56.487+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:25:56.486+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:25:56.651+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.214 seconds
[2022-12-12T13:26:06.991+0000] {processor.py:154} INFO - Started process (PID=4688) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:26:06.996+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:26:07.000+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:26:06.999+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:26:07.105+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:26:07.333+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:26:07.332+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:26:07.510+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:26:07.509+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:26:07.729+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.757 seconds
[2022-12-12T13:26:18.091+0000] {processor.py:154} INFO - Started process (PID=4698) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:26:18.105+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:26:18.114+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:26:18.113+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:26:18.370+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:26:18.887+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:26:18.886+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:26:19.079+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:26:19.078+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:26:19.295+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.233 seconds
[2022-12-12T13:26:30.115+0000] {processor.py:154} INFO - Started process (PID=4715) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:26:30.151+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:26:30.164+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:26:30.163+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:26:30.790+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:26:31.153+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:26:31.152+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:26:31.449+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:26:31.448+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:26:31.723+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.695 seconds
[2022-12-12T13:26:42.354+0000] {processor.py:154} INFO - Started process (PID=4725) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:26:42.381+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:26:42.384+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:26:42.383+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:26:42.748+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:26:43.233+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:26:43.232+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:26:43.434+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:26:43.429+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:26:43.602+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.295 seconds
[2022-12-12T13:26:54.101+0000] {processor.py:154} INFO - Started process (PID=4735) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:26:54.122+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:26:54.144+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:26:54.143+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:26:54.384+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:26:55.281+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:26:55.280+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:26:55.608+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:26:55.607+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:26:55.811+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.774 seconds
[2022-12-12T13:27:06.275+0000] {processor.py:154} INFO - Started process (PID=4745) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:27:06.305+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:27:06.309+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:27:06.308+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:27:06.550+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:27:06.841+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:27:06.840+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:27:07.005+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:27:07.004+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:27:07.157+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.947 seconds
[2022-12-12T13:27:17.723+0000] {processor.py:154} INFO - Started process (PID=4763) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:27:17.731+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:27:17.735+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:27:17.734+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:27:17.974+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:27:19.319+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:27:19.318+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:27:19.544+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:27:19.543+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:27:19.878+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.184 seconds
[2022-12-12T13:27:30.461+0000] {processor.py:154} INFO - Started process (PID=4773) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:27:30.507+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:27:30.513+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:27:30.512+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:27:30.862+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:27:31.765+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:27:42.431+0000] {processor.py:154} INFO - Started process (PID=4783) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:27:42.471+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:27:42.479+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:27:42.474+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:27:42.662+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:27:43.861+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:27:54.450+0000] {processor.py:154} INFO - Started process (PID=4798) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:27:54.501+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:27:54.516+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:27:54.509+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:27:54.894+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:27:55.245+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:27:55.244+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:27:55.525+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:27:55.524+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:27:57.065+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.689 seconds
[2022-12-12T13:28:07.821+0000] {processor.py:154} INFO - Started process (PID=4809) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:28:07.832+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:28:07.845+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:28:07.844+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:28:08.096+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:28:09.092+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:28:19.620+0000] {processor.py:154} INFO - Started process (PID=4821) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:28:19.647+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:28:19.651+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:28:19.650+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:28:19.795+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:28:19.983+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:28:19.982+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:28:21.099+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:28:21.098+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:28:21.251+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.668 seconds
[2022-12-12T13:28:31.911+0000] {processor.py:154} INFO - Started process (PID=4831) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:28:31.965+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:28:31.970+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:28:31.969+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:28:32.079+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:28:32.340+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:28:32.340+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:28:33.376+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:28:33.375+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:28:33.543+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.648 seconds
[2022-12-12T13:28:44.147+0000] {processor.py:154} INFO - Started process (PID=4847) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:28:44.159+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:28:44.179+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:28:44.178+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:28:44.483+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:28:45.306+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:28:45.297+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:28:45.566+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:28:45.565+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:28:45.853+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.776 seconds
[2022-12-12T13:28:56.459+0000] {processor.py:154} INFO - Started process (PID=4857) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:28:56.474+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:28:56.484+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:28:56.477+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:28:56.682+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:28:56.928+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:28:56.927+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:28:57.264+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:28:57.263+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:28:57.442+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.018 seconds
[2022-12-12T13:29:07.873+0000] {processor.py:154} INFO - Started process (PID=4867) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:29:07.877+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:29:07.881+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:29:07.880+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:29:08.085+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:29:08.559+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:29:08.558+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:29:08.790+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:29:08.781+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:29:09.009+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.175 seconds
[2022-12-12T13:29:19.322+0000] {processor.py:154} INFO - Started process (PID=4877) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:29:19.372+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:29:19.376+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:29:19.375+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:29:19.550+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:29:19.822+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:29:19.821+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:29:20.248+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:29:20.247+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:29:20.520+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.213 seconds
[2022-12-12T13:29:31.107+0000] {processor.py:154} INFO - Started process (PID=4895) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:29:31.121+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:29:31.135+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:29:31.134+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:29:31.413+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:29:32.709+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:29:32.708+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:29:33.009+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:29:33.008+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:29:33.445+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.382 seconds
[2022-12-12T13:29:43.861+0000] {processor.py:154} INFO - Started process (PID=4907) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:29:43.873+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:29:43.877+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:29:43.876+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:29:44.099+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:29:44.335+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:29:44.333+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:29:44.481+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:29:44.480+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:29:44.681+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.842 seconds
[2022-12-12T13:29:55.181+0000] {processor.py:154} INFO - Started process (PID=4915) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:29:55.205+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:29:55.209+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:29:55.208+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:29:55.433+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:29:55.600+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:29:55.598+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:29:55.741+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:29:55.740+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:29:56.011+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.871 seconds
[2022-12-12T13:30:06.420+0000] {processor.py:154} INFO - Started process (PID=4925) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:30:06.458+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:30:06.475+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:30:06.468+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:30:06.700+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:30:07.410+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:30:18.145+0000] {processor.py:154} INFO - Started process (PID=4943) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:30:18.204+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:30:18.208+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:30:18.207+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:30:18.410+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:30:19.763+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:30:19.761+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:30:20.064+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:30:20.063+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:30:20.468+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.365 seconds
[2022-12-12T13:30:31.153+0000] {processor.py:154} INFO - Started process (PID=4953) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:30:31.239+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:30:31.244+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:30:31.243+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:30:31.509+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:30:31.776+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:30:31.775+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:30:32.090+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:30:32.089+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:30:32.360+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.271 seconds
[2022-12-12T13:30:42.904+0000] {processor.py:154} INFO - Started process (PID=4963) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:30:42.933+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:30:42.969+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:30:42.953+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:30:43.319+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:30:43.685+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:30:43.684+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:30:43.859+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:30:43.858+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:30:44.028+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.181 seconds
[2022-12-12T13:30:54.588+0000] {processor.py:154} INFO - Started process (PID=4980) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:30:54.645+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:30:54.649+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:30:54.648+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:30:54.980+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:30:55.757+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:30:55.756+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:30:56.014+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:30:56.013+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:30:56.297+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.784 seconds
[2022-12-12T13:31:06.489+0000] {processor.py:154} INFO - Started process (PID=4991) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:31:06.493+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:31:06.497+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:31:06.496+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:31:06.606+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:31:06.811+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:31:06.809+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:31:07.000+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:31:06.997+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:31:07.234+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.761 seconds
[2022-12-12T13:31:17.832+0000] {processor.py:154} INFO - Started process (PID=5001) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:31:17.842+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:31:17.850+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:31:17.845+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:31:18.091+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:31:18.304+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:31:18.302+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:31:18.451+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:31:18.450+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:31:19.764+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.977 seconds
[2022-12-12T13:31:30.244+0000] {processor.py:154} INFO - Started process (PID=5011) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:31:30.300+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:31:30.324+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:31:30.323+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:31:30.624+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:31:31.388+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:31:31.387+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:31:31.544+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:31:31.543+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:31:32.466+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.260 seconds
[2022-12-12T13:31:42.944+0000] {processor.py:154} INFO - Started process (PID=5028) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:31:42.977+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:31:42.992+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:31:42.991+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:31:43.256+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:31:44.434+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:31:44.423+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:31:47.171+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:31:47.170+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:31:47.447+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 4.574 seconds
[2022-12-12T13:31:58.171+0000] {processor.py:154} INFO - Started process (PID=5038) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:31:58.183+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:31:58.195+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:31:58.194+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:31:58.561+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:31:59.072+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:31:59.071+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:32:00.533+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:32:00.529+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:32:00.795+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.722 seconds
[2022-12-12T13:32:11.106+0000] {processor.py:154} INFO - Started process (PID=5050) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:32:11.110+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:32:11.114+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:32:11.113+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:32:11.324+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:32:12.726+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:32:12.725+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:32:12.869+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:32:12.868+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:32:13.013+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.925 seconds
[2022-12-12T13:32:23.469+0000] {processor.py:154} INFO - Started process (PID=5060) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:32:23.523+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:32:23.528+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:32:23.527+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:32:23.838+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:32:24.615+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:32:24.609+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:32:25.029+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:32:25.028+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:32:25.419+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.979 seconds
[2022-12-12T13:32:36.083+0000] {processor.py:154} INFO - Started process (PID=5078) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:32:36.094+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:32:36.104+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:32:36.100+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:32:36.312+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:32:37.653+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:32:37.652+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:32:38.286+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:32:38.285+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:32:38.570+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.537 seconds
[2022-12-12T13:32:49.029+0000] {processor.py:154} INFO - Started process (PID=5088) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:32:49.059+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:32:49.072+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:32:49.070+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:32:49.200+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:32:49.392+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:32:49.391+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:32:49.561+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:32:49.560+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:32:49.748+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.755 seconds
[2022-12-12T13:33:00.671+0000] {processor.py:154} INFO - Started process (PID=5098) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:33:00.676+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:33:00.681+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:33:00.680+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:33:00.808+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:33:01.458+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:33:01.457+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:33:01.608+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:33:01.607+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:33:01.763+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.111 seconds
[2022-12-12T13:33:12.128+0000] {processor.py:154} INFO - Started process (PID=5117) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:33:12.185+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:33:12.189+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:33:12.188+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:33:12.389+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:33:13.259+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:33:13.258+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:33:13.486+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:33:13.485+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:33:13.711+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.624 seconds
[2022-12-12T13:33:24.320+0000] {processor.py:154} INFO - Started process (PID=5127) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:33:24.323+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:33:24.327+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:33:24.326+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:33:24.431+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:33:24.685+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:33:24.684+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:33:24.820+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:33:24.819+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:33:24.966+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.667 seconds
[2022-12-12T13:33:35.274+0000] {processor.py:154} INFO - Started process (PID=5137) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:33:35.292+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:33:35.296+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:33:35.295+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:33:35.398+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:33:35.975+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:33:35.974+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:33:36.193+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:33:36.192+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:33:36.485+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.233 seconds
[2022-12-12T13:33:46.842+0000] {processor.py:154} INFO - Started process (PID=5147) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:33:46.846+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:33:46.851+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:33:46.850+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:33:46.969+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:33:47.371+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:33:47.370+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:33:47.597+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:33:47.596+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:33:47.758+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.939 seconds
[2022-12-12T13:33:58.453+0000] {processor.py:154} INFO - Started process (PID=5165) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:33:58.487+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:33:58.507+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:33:58.500+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:33:58.718+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:33:59.446+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:33:59.445+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:33:59.625+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:33:59.624+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:33:59.885+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.478 seconds
[2022-12-12T13:34:10.450+0000] {processor.py:154} INFO - Started process (PID=5175) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:34:10.478+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:34:10.487+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:34:10.481+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:34:10.769+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:34:11.113+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:34:11.112+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:34:11.314+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:34:11.313+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:34:11.656+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.257 seconds
[2022-12-12T13:34:22.782+0000] {processor.py:154} INFO - Started process (PID=5185) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:34:22.834+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:34:22.842+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:34:22.837+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:34:23.010+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:34:23.260+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:34:23.259+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:34:23.439+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:34:23.438+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:34:23.640+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.917 seconds
[2022-12-12T13:34:34.084+0000] {processor.py:154} INFO - Started process (PID=5195) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:34:34.088+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:34:34.096+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:34:34.095+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:34:34.213+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:34:34.458+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:34:34.457+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:34:34.762+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:34:34.761+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:34:36.363+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.300 seconds
[2022-12-12T13:34:47.241+0000] {processor.py:154} INFO - Started process (PID=5213) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:34:47.426+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:34:47.431+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:34:47.429+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:34:47.749+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:34:49.027+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:34:59.634+0000] {processor.py:154} INFO - Started process (PID=5223) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:34:59.681+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:34:59.685+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:34:59.684+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:34:59.826+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:35:00.221+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:35:00.220+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:35:01.300+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:35:01.299+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:35:01.477+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.893 seconds
[2022-12-12T13:35:11.877+0000] {processor.py:154} INFO - Started process (PID=5233) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:35:11.890+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:35:11.894+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:35:11.893+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:35:12.049+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:35:12.392+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:35:12.391+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:35:13.187+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:35:13.186+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:35:13.352+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.498 seconds
[2022-12-12T13:35:24.475+0000] {processor.py:154} INFO - Started process (PID=5250) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:35:24.503+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:35:24.514+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:35:24.512+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:35:24.874+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:35:25.209+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:35:25.208+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:35:25.505+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:35:25.504+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:35:25.855+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.459 seconds
[2022-12-12T13:35:36.229+0000] {processor.py:154} INFO - Started process (PID=5261) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:35:36.308+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:35:36.324+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:35:36.323+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:35:36.591+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:35:36.813+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:35:36.812+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:35:36.965+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:35:36.964+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:35:37.127+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.935 seconds
[2022-12-12T13:35:47.545+0000] {processor.py:154} INFO - Started process (PID=5271) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:35:47.565+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:35:47.569+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:35:47.568+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:35:47.681+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:35:48.240+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:35:48.239+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:35:48.403+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:35:48.402+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:35:48.541+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.012 seconds
[2022-12-12T13:35:58.932+0000] {processor.py:154} INFO - Started process (PID=5281) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:35:58.992+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:35:58.996+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:35:58.995+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:35:59.226+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:35:59.392+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:35:59.391+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:35:59.528+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:35:59.527+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:35:59.703+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.796 seconds
[2022-12-12T13:36:10.204+0000] {processor.py:154} INFO - Started process (PID=5298) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:36:10.240+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:36:10.251+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:36:10.250+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:36:10.561+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:36:10.969+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:36:10.968+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:36:11.314+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:36:11.296+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:36:11.696+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.566 seconds
[2022-12-12T13:36:22.121+0000] {processor.py:154} INFO - Started process (PID=5309) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:36:22.133+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:36:22.137+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:36:22.136+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:36:22.296+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:36:22.493+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:36:22.492+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:36:22.672+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:36:22.669+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:36:22.873+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.790 seconds
[2022-12-12T13:36:33.229+0000] {processor.py:154} INFO - Started process (PID=5319) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:36:33.275+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:36:33.284+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:36:33.283+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:36:33.575+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:36:34.544+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:36:45.353+0000] {processor.py:154} INFO - Started process (PID=5329) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:36:45.427+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:36:45.439+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:36:45.438+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:36:45.942+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:36:46.286+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:36:46.281+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:36:46.470+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:36:46.469+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:36:46.685+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.443 seconds
[2022-12-12T13:36:49.584+0000] {processor.py:154} INFO - Started process (PID=5341) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:36:49.609+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:36:49.613+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:36:49.612+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:36:50.114+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:36:52.148+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:36:52.147+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:36:53.163+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:36:53.130+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:36:53.839+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 4.312 seconds
[2022-12-12T13:37:05.026+0000] {processor.py:154} INFO - Started process (PID=5360) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:37:05.068+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:37:05.072+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:37:05.071+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:37:05.472+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:37:06.920+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:37:17.698+0000] {processor.py:154} INFO - Started process (PID=5370) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:37:17.719+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:37:17.728+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:37:17.727+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:37:17.865+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:37:18.681+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:37:18.680+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:37:19.147+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:37:19.145+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:37:19.300+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.632 seconds
[2022-12-12T13:37:29.909+0000] {processor.py:154} INFO - Started process (PID=5380) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:37:29.941+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:37:29.958+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:37:29.951+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:37:30.227+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:37:31.356+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:37:41.797+0000] {processor.py:154} INFO - Started process (PID=5397) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:37:41.862+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:37:41.872+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:37:41.871+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:37:42.146+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:37:42.842+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:37:42.841+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:37:43.147+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:37:43.146+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:37:43.463+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.689 seconds
[2022-12-12T13:37:54.114+0000] {processor.py:154} INFO - Started process (PID=5408) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:37:54.118+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:37:54.129+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:37:54.128+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:37:54.417+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:37:56.207+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:37:56.198+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:37:56.424+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:37:56.423+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:37:56.584+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.502 seconds
[2022-12-12T13:38:07.005+0000] {processor.py:154} INFO - Started process (PID=5420) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:38:07.031+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:38:07.035+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:38:07.034+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:38:07.172+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:38:07.359+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:38:07.357+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:38:07.497+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:38:07.496+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:38:07.684+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.701 seconds
[2022-12-12T13:38:18.204+0000] {processor.py:154} INFO - Started process (PID=5430) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:38:18.211+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:38:18.215+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:38:18.214+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:38:18.326+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:38:19.047+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:38:19.046+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:38:19.353+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:38:19.352+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:38:19.493+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.334 seconds
[2022-12-12T13:38:29.835+0000] {processor.py:154} INFO - Started process (PID=5447) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:38:29.870+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:38:29.877+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:38:29.876+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:38:30.228+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:38:31.232+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:38:31.230+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:38:31.547+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:38:31.546+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:38:31.911+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.103 seconds
[2022-12-12T13:38:42.367+0000] {processor.py:154} INFO - Started process (PID=5459) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:38:42.372+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:38:42.377+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:38:42.376+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:38:42.484+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:38:42.669+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:38:42.668+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:38:42.817+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:38:42.816+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:38:43.042+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.691 seconds
[2022-12-12T13:38:53.376+0000] {processor.py:154} INFO - Started process (PID=5467) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:38:53.430+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:38:53.434+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:38:53.433+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:38:53.717+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:38:54.193+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:38:54.192+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:38:54.499+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:38:54.498+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:38:54.763+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.410 seconds
[2022-12-12T13:39:05.074+0000] {processor.py:154} INFO - Started process (PID=5477) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:39:05.098+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:39:05.103+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:39:05.102+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:39:05.295+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:39:06.388+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:39:17.362+0000] {processor.py:154} INFO - Started process (PID=5495) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:39:17.387+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:39:17.396+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:39:17.395+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:39:17.732+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:39:18.016+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:39:18.015+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:39:18.461+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:39:18.460+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:39:18.944+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.626 seconds
[2022-12-12T13:39:29.547+0000] {processor.py:154} INFO - Started process (PID=5505) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:39:29.599+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:39:29.623+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:39:29.622+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:39:29.800+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:39:30.850+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:39:30.849+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:39:30.985+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:39:30.984+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:39:31.153+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.636 seconds
[2022-12-12T13:39:41.989+0000] {processor.py:154} INFO - Started process (PID=5517) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:39:42.010+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:39:42.020+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:39:42.019+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:39:42.117+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:39:42.588+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:39:42.587+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:39:42.800+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:39:42.799+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:39:43.107+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.134 seconds
[2022-12-12T13:39:53.451+0000] {processor.py:154} INFO - Started process (PID=5527) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:39:53.491+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:39:53.496+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:39:53.495+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:39:53.611+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:39:54.819+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:39:54.817+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:39:54.984+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:39:54.983+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:39:55.177+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.742 seconds
[2022-12-12T13:40:05.640+0000] {processor.py:154} INFO - Started process (PID=5545) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:40:05.668+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:40:05.673+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:40:05.672+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:40:05.878+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:40:06.453+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:40:17.201+0000] {processor.py:154} INFO - Started process (PID=5555) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:40:17.208+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:40:17.212+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:40:17.211+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:40:17.386+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:40:17.734+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:40:17.733+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:40:17.917+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:40:17.916+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:40:18.108+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.959 seconds
[2022-12-12T13:40:28.583+0000] {processor.py:154} INFO - Started process (PID=5565) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:40:28.597+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:40:28.601+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:40:28.600+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:40:28.720+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:40:30.016+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:40:40.534+0000] {processor.py:154} INFO - Started process (PID=5575) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:40:40.540+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:40:40.546+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:40:40.545+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:40:40.759+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:40:40.955+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:40:40.954+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:40:41.095+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:40:41.095+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:40:41.407+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.895 seconds
[2022-12-12T13:40:52.255+0000] {processor.py:154} INFO - Started process (PID=5594) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:40:52.311+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:40:52.316+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:40:52.315+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:40:52.573+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:40:52.908+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:40:52.908+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:40:53.318+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:40:53.311+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:40:53.801+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.588 seconds
[2022-12-12T13:41:04.326+0000] {processor.py:154} INFO - Started process (PID=5604) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:41:04.346+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:41:04.360+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:41:04.359+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:41:04.677+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:41:06.257+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:41:16.599+0000] {processor.py:154} INFO - Started process (PID=5612) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:41:16.639+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:41:16.652+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:41:16.651+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:41:16.872+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:41:17.078+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:41:17.077+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:41:17.222+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:41:17.221+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:41:17.390+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.827 seconds
[2022-12-12T13:41:28.017+0000] {processor.py:154} INFO - Started process (PID=5629) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:41:28.082+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:41:28.089+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:41:28.085+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:41:28.310+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:41:28.749+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:41:28.748+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:41:29.138+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:41:29.133+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:41:29.578+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.641 seconds
[2022-12-12T13:41:40.121+0000] {processor.py:154} INFO - Started process (PID=5639) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:41:40.132+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:41:40.136+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:41:40.136+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:41:40.514+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:41:41.775+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:41:41.774+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:41:41.922+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:41:41.921+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:41:42.092+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.019 seconds
[2022-12-12T13:41:52.307+0000] {processor.py:154} INFO - Started process (PID=5652) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:41:52.327+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:41:52.331+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:41:52.330+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:41:52.477+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:41:53.467+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:41:53.466+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:41:53.660+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:41:53.659+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:41:53.798+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.512 seconds
[2022-12-12T13:42:04.102+0000] {processor.py:154} INFO - Started process (PID=5662) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:42:04.129+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:42:04.138+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:42:04.132+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:42:04.357+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:42:04.581+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:42:04.580+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:42:04.733+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:42:04.731+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:42:04.879+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.828 seconds
[2022-12-12T13:42:15.380+0000] {processor.py:154} INFO - Started process (PID=5677) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:42:15.410+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:42:15.424+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:42:15.413+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:42:15.611+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:42:16.065+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:42:16.053+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:42:16.363+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:42:16.357+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:42:16.756+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.452 seconds
[2022-12-12T13:42:27.036+0000] {processor.py:154} INFO - Started process (PID=5689) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:42:27.042+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:42:27.046+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:42:27.045+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:42:27.164+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:42:27.357+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:42:27.356+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:42:27.608+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:42:27.597+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:42:27.770+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.751 seconds
[2022-12-12T13:42:37.971+0000] {processor.py:154} INFO - Started process (PID=5697) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:42:37.996+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:42:38.008+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:42:38.005+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:42:38.223+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:42:38.396+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:42:38.395+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:42:38.526+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:42:38.525+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:42:38.681+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.734 seconds
[2022-12-12T13:42:49.094+0000] {processor.py:154} INFO - Started process (PID=5707) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:42:49.143+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:42:49.149+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:42:49.148+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:42:49.404+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:42:50.136+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:42:50.135+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:42:50.324+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:42:50.323+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:42:50.487+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.434 seconds
[2022-12-12T13:43:01.281+0000] {processor.py:154} INFO - Started process (PID=5725) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:43:01.310+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:43:01.315+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:43:01.314+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:43:01.641+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:43:02.061+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:43:02.060+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:43:02.389+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:43:02.373+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:43:02.769+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.588 seconds
[2022-12-12T13:43:13.373+0000] {processor.py:154} INFO - Started process (PID=5736) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:43:13.379+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:43:13.384+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:43:13.383+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:43:13.515+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:43:13.796+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:43:13.795+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:43:14.388+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:43:14.387+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:43:14.649+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.325 seconds
[2022-12-12T13:43:25.208+0000] {processor.py:154} INFO - Started process (PID=5747) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:43:25.246+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:43:25.250+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:43:25.249+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:43:25.440+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:43:25.924+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:43:25.923+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:43:26.093+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:43:26.092+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:43:26.256+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.101 seconds
[2022-12-12T13:43:36.462+0000] {processor.py:154} INFO - Started process (PID=5755) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:43:36.662+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:43:36.670+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:43:36.669+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:43:36.946+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:43:37.335+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:43:37.334+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:43:37.594+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:43:37.593+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:43:37.791+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.348 seconds
[2022-12-12T13:43:48.479+0000] {processor.py:154} INFO - Started process (PID=5772) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:43:48.531+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:43:48.561+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:43:48.560+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:43:49.261+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:43:50.500+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:43:50.499+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:43:50.786+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:43:50.785+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:43:51.106+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.787 seconds
[2022-12-12T13:44:01.731+0000] {processor.py:154} INFO - Started process (PID=5783) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:44:01.796+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:44:01.805+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:44:01.804+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:44:02.020+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:44:02.713+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:44:02.712+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:44:02.945+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:44:02.944+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:44:03.262+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.584 seconds
[2022-12-12T13:44:13.583+0000] {processor.py:154} INFO - Started process (PID=5793) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:44:13.642+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:44:13.711+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:44:13.710+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:44:14.191+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:44:14.957+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:44:14.956+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:44:15.149+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:44:15.148+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:44:15.371+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.824 seconds
[2022-12-12T13:44:25.961+0000] {processor.py:154} INFO - Started process (PID=5809) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:44:25.983+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:44:25.992+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:44:25.991+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:44:26.310+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:44:26.721+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:44:26.720+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:44:27.053+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:44:27.052+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:44:27.371+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.497 seconds
[2022-12-12T13:44:38.074+0000] {processor.py:154} INFO - Started process (PID=5820) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:44:38.087+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:44:38.104+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:44:38.103+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:44:38.436+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:44:39.109+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:44:49.606+0000] {processor.py:154} INFO - Started process (PID=5830) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:44:49.656+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:44:49.660+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:44:49.659+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:44:50.019+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:44:50.424+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:44:50.423+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:44:50.614+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:44:50.613+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:44:50.808+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.252 seconds
[2022-12-12T13:45:01.278+0000] {processor.py:154} INFO - Started process (PID=5840) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:45:01.282+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:45:01.287+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:45:01.286+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:45:01.408+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:45:01.959+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:45:01.958+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:45:02.380+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:45:02.379+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:45:02.679+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.428 seconds
[2022-12-12T13:45:13.322+0000] {processor.py:154} INFO - Started process (PID=5858) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:45:13.351+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:45:13.372+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:45:13.361+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:45:13.619+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:45:13.952+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:45:13.951+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:45:14.184+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:45:14.183+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:45:14.455+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.176 seconds
[2022-12-12T13:45:25.052+0000] {processor.py:154} INFO - Started process (PID=5865) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:45:25.140+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:45:25.149+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:45:25.148+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:45:25.294+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:45:25.615+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:45:25.614+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:45:25.921+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:45:25.920+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:45:26.285+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.270 seconds
[2022-12-12T13:45:36.588+0000] {processor.py:154} INFO - Started process (PID=5875) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:45:36.594+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:45:36.599+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:45:36.598+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:45:36.731+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:45:36.961+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:45:36.960+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:45:37.216+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:45:37.215+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:45:37.492+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.925 seconds
[2022-12-12T13:45:47.911+0000] {processor.py:154} INFO - Started process (PID=5885) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:45:47.934+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:45:47.939+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:45:47.938+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:45:48.122+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:45:48.331+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:45:48.329+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:45:48.635+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:45:48.634+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:45:48.792+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.899 seconds
[2022-12-12T13:45:58.972+0000] {processor.py:154} INFO - Started process (PID=5903) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:45:59.028+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:45:59.036+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:45:59.034+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:45:59.462+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:46:00.620+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:46:00.606+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:46:01.241+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:46:01.240+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:46:01.553+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.607 seconds
[2022-12-12T13:46:11.956+0000] {processor.py:154} INFO - Started process (PID=5913) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:46:11.961+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:46:11.966+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:46:11.965+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:46:12.084+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:46:12.540+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:46:12.539+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:46:12.753+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:46:12.752+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:46:13.066+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.141 seconds
[2022-12-12T13:46:23.535+0000] {processor.py:154} INFO - Started process (PID=5923) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:46:23.555+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:46:23.588+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:46:23.574+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:46:23.928+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:46:25.353+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:46:25.352+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:46:25.652+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:46:25.637+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:46:25.901+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.415 seconds
[2022-12-12T13:46:36.319+0000] {processor.py:154} INFO - Started process (PID=5945) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:46:36.346+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:46:36.361+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:46:36.359+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:46:36.580+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:46:37.353+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:46:47.976+0000] {processor.py:154} INFO - Started process (PID=5953) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:46:48.002+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:46:48.015+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:46:48.014+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:46:48.259+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:46:50.018+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:47:00.691+0000] {processor.py:154} INFO - Started process (PID=5963) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:47:00.745+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:47:00.751+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:47:00.749+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:47:00.895+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:47:01.089+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:47:01.088+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:47:01.229+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:47:01.228+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:47:01.409+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.765 seconds
[2022-12-12T13:47:12.025+0000] {processor.py:154} INFO - Started process (PID=5973) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:47:12.071+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:47:12.077+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:47:12.075+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:47:12.213+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:47:12.999+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:47:23.324+0000] {processor.py:154} INFO - Started process (PID=5990) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:47:23.354+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:47:23.359+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:47:23.358+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:47:23.605+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:47:23.909+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:47:23.908+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:47:24.111+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:47:24.110+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:47:24.335+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.027 seconds
[2022-12-12T13:47:35.022+0000] {processor.py:154} INFO - Started process (PID=6001) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:47:35.065+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:47:35.069+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:47:35.068+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:47:35.413+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:47:35.586+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:47:35.585+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:47:35.719+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:47:35.718+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:47:35.852+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.873 seconds
[2022-12-12T13:47:46.089+0000] {processor.py:154} INFO - Started process (PID=6011) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:47:46.171+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:47:46.176+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:47:46.175+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:47:46.351+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:47:46.846+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:47:46.845+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:47:47.062+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:47:47.060+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:47:47.362+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.295 seconds
[2022-12-12T13:47:58.704+0000] {processor.py:154} INFO - Started process (PID=6024) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:47:58.781+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:47:58.814+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:47:58.806+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:47:59.129+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:47:59.337+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:47:59.336+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:47:59.471+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:47:59.470+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:47:59.747+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.114 seconds
[2022-12-12T13:48:10.316+0000] {processor.py:154} INFO - Started process (PID=6039) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:48:10.341+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:48:10.348+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:48:10.344+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:48:10.756+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:48:11.525+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:48:11.524+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:48:11.899+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:48:11.888+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:48:12.183+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.917 seconds
[2022-12-12T13:48:22.551+0000] {processor.py:154} INFO - Started process (PID=6049) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:48:22.697+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:48:22.714+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:48:22.700+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:48:23.083+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:48:23.814+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:48:23.813+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:48:23.960+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:48:23.959+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:48:24.234+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.707 seconds
[2022-12-12T13:48:34.736+0000] {processor.py:154} INFO - Started process (PID=6062) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:48:34.772+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:48:34.776+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:48:34.775+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:48:35.062+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:48:35.735+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:48:35.734+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:48:35.879+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:48:35.878+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:48:36.028+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.396 seconds
[2022-12-12T13:48:46.507+0000] {processor.py:154} INFO - Started process (PID=6072) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:48:46.535+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:48:46.551+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:48:46.550+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:48:47.420+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:48:48.738+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:48:48.726+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:48:49.070+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:48:49.069+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:48:49.359+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.934 seconds
[2022-12-12T13:48:59.962+0000] {processor.py:154} INFO - Started process (PID=6090) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:48:59.992+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:48:59.997+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:48:59.996+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:49:00.146+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:49:00.454+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:49:00.453+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:49:00.705+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:49:00.704+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:49:00.914+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.971 seconds
[2022-12-12T13:49:11.399+0000] {processor.py:154} INFO - Started process (PID=6100) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:49:11.404+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:49:11.408+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:49:11.407+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:49:11.592+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:49:11.769+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:49:11.768+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:49:11.907+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:49:11.906+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:49:12.201+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.852 seconds
[2022-12-12T13:49:22.752+0000] {processor.py:154} INFO - Started process (PID=6107) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:49:22.757+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:49:22.763+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:49:22.762+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:49:22.902+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:49:23.619+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:49:23.618+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:49:23.760+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:49:23.759+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:49:23.971+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.254 seconds
[2022-12-12T13:49:34.691+0000] {processor.py:154} INFO - Started process (PID=6125) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:49:34.732+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:49:34.739+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:49:34.738+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:49:34.961+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:49:36.467+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:49:36.460+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:49:36.936+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:49:36.935+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:49:37.209+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.578 seconds
[2022-12-12T13:49:47.495+0000] {processor.py:154} INFO - Started process (PID=6140) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:49:47.500+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:49:47.506+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:49:47.505+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:49:47.617+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:49:48.651+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:49:59.071+0000] {processor.py:154} INFO - Started process (PID=6148) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:49:59.084+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:49:59.090+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:49:59.088+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:49:59.279+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:50:00.119+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:50:00.118+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:50:00.268+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:50:00.267+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:50:00.411+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.367 seconds
[2022-12-12T13:50:10.723+0000] {processor.py:154} INFO - Started process (PID=6158) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:50:10.745+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:50:10.761+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:50:10.759+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:50:10.917+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:50:11.743+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:50:11.740+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:50:11.900+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:50:11.899+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:50:12.034+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.348 seconds
[2022-12-12T13:50:22.609+0000] {processor.py:154} INFO - Started process (PID=6176) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:50:22.631+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:50:22.640+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:50:22.639+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:50:22.893+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:50:23.217+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:50:23.216+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:50:23.550+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:50:23.549+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:50:23.906+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.349 seconds
[2022-12-12T13:50:34.462+0000] {processor.py:154} INFO - Started process (PID=6185) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:50:34.476+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:50:34.484+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:50:34.480+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:50:34.673+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:50:35.738+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:50:35.737+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:50:35.945+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:50:35.944+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:50:36.133+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.689 seconds
[2022-12-12T13:50:46.464+0000] {processor.py:154} INFO - Started process (PID=6196) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:50:46.479+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:50:46.495+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:50:46.494+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:50:46.660+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:50:47.187+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:50:47.179+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:50:47.370+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:50:47.369+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:50:47.570+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.126 seconds
[2022-12-12T13:50:57.994+0000] {processor.py:154} INFO - Started process (PID=6213) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:50:58.055+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:50:58.059+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:50:58.059+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:50:58.346+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:50:58.598+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:50:58.597+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:50:58.818+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:50:58.817+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:50:59.083+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.128 seconds
[2022-12-12T13:51:09.404+0000] {processor.py:154} INFO - Started process (PID=6221) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:51:09.431+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:51:09.449+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:51:09.445+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:51:09.641+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:51:10.017+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:51:10.016+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:51:10.191+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:51:10.190+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:51:10.364+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.017 seconds
[2022-12-12T13:51:20.756+0000] {processor.py:154} INFO - Started process (PID=6231) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:51:20.760+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:51:20.765+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:51:20.764+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:51:21.007+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:51:21.760+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:51:21.759+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:51:21.960+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:51:21.959+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:51:22.119+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.380 seconds
[2022-12-12T13:51:32.266+0000] {processor.py:154} INFO - Started process (PID=6241) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:51:32.315+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:51:32.320+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:51:32.319+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:51:32.431+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:51:32.761+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:51:32.759+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:51:33.153+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:51:33.152+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:51:33.575+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.326 seconds
[2022-12-12T13:51:44.048+0000] {processor.py:154} INFO - Started process (PID=6259) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:51:44.098+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:51:44.113+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:51:44.101+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:51:44.271+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:51:45.309+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:51:45.308+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:51:45.789+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:51:45.788+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:51:46.009+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.035 seconds
[2022-12-12T13:51:57.083+0000] {processor.py:154} INFO - Started process (PID=6269) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:51:57.109+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:51:57.115+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:51:57.113+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:51:57.344+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:51:57.566+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:51:57.565+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:51:57.706+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:51:57.706+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:51:57.842+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.781 seconds
[2022-12-12T13:52:08.148+0000] {processor.py:154} INFO - Started process (PID=6279) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:52:08.152+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:52:08.156+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:52:08.155+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:52:08.315+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:52:08.702+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:52:08.701+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:52:08.965+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:52:08.964+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:52:09.218+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.089 seconds
[2022-12-12T13:52:19.593+0000] {processor.py:154} INFO - Started process (PID=6289) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:52:19.639+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:52:19.643+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:52:19.642+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:52:19.752+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:52:20.012+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:52:20.010+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:52:20.365+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:52:20.363+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:52:20.747+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.174 seconds
[2022-12-12T13:52:31.342+0000] {processor.py:154} INFO - Started process (PID=6307) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:52:31.394+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:52:31.399+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:52:31.398+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:52:31.727+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:52:32.132+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:52:32.131+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:52:32.560+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:52:32.541+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:52:32.974+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.672 seconds
[2022-12-12T13:52:43.573+0000] {processor.py:154} INFO - Started process (PID=6317) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:52:43.633+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:52:43.664+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:52:43.657+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:52:43.937+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:52:44.964+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:52:44.960+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:52:45.167+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:52:45.166+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:52:45.403+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.858 seconds
[2022-12-12T13:52:55.859+0000] {processor.py:154} INFO - Started process (PID=6327) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:52:55.939+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:52:55.948+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:52:55.947+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:52:56.357+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:52:56.724+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:52:56.723+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:52:56.941+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:52:56.940+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:52:57.104+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.275 seconds
[2022-12-12T13:53:07.639+0000] {processor.py:154} INFO - Started process (PID=6344) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:53:07.707+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:53:07.719+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:53:07.718+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:53:08.385+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:53:09.152+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:53:09.151+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:53:09.513+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:53:09.512+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:53:09.894+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.321 seconds
[2022-12-12T13:53:20.456+0000] {processor.py:154} INFO - Started process (PID=6358) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:53:20.506+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:53:20.511+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:53:20.509+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:53:20.646+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:53:20.999+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:53:20.998+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:53:21.197+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:53:21.196+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:53:21.429+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.005 seconds
[2022-12-12T13:53:31.795+0000] {processor.py:154} INFO - Started process (PID=6366) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:53:31.828+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:53:31.842+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:53:31.841+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:53:32.025+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:53:32.717+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:53:32.714+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:53:33.004+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:53:33.003+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:53:33.178+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.401 seconds
[2022-12-12T13:53:43.424+0000] {processor.py:154} INFO - Started process (PID=6377) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:53:43.440+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:53:43.445+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:53:43.444+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:53:43.591+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:53:44.166+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:53:44.165+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:53:44.397+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:53:44.396+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:53:44.546+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.140 seconds
[2022-12-12T13:53:55.104+0000] {processor.py:154} INFO - Started process (PID=6394) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:53:55.134+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:53:55.141+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:53:55.140+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:53:55.443+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:53:56.239+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:53:56.238+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:53:56.499+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:53:56.498+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:53:56.865+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.832 seconds
[2022-12-12T13:54:07.464+0000] {processor.py:154} INFO - Started process (PID=6404) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:54:07.508+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:54:07.513+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:54:07.512+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:54:07.641+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:54:08.408+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:54:08.407+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:54:08.864+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:54:08.863+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:54:09.183+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.737 seconds
[2022-12-12T13:54:19.561+0000] {processor.py:154} INFO - Started process (PID=6416) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:54:19.617+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:54:19.621+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:54:19.620+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:54:19.746+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:54:20.035+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:54:20.033+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:54:20.274+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:54:20.273+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:54:20.843+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.306 seconds
[2022-12-12T13:54:31.589+0000] {processor.py:154} INFO - Started process (PID=6430) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:54:31.615+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:54:31.621+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:54:31.619+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:54:32.195+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:54:33.115+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:54:33.114+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:54:33.440+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:54:33.439+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:54:33.920+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.360 seconds
[2022-12-12T13:54:44.687+0000] {processor.py:154} INFO - Started process (PID=6442) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:54:44.743+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:54:44.747+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:54:44.746+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:54:44.996+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:54:45.353+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:54:45.352+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:54:45.756+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:54:45.755+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:54:46.275+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.658 seconds
[2022-12-12T13:54:56.677+0000] {processor.py:154} INFO - Started process (PID=6452) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:54:56.706+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:54:56.711+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:54:56.710+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:54:56.812+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:54:56.979+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:54:56.978+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:54:57.122+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:54:57.121+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:54:57.283+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.625 seconds
[2022-12-12T13:55:07.560+0000] {processor.py:154} INFO - Started process (PID=6462) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:55:07.573+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:55:07.577+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:55:07.576+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:55:07.745+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:55:08.436+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:55:18.967+0000] {processor.py:154} INFO - Started process (PID=6479) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:55:19.007+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:55:19.017+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:55:19.016+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:55:19.268+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:55:20.499+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:55:20.498+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:55:20.877+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:55:20.876+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:55:21.405+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.460 seconds
[2022-12-12T13:55:31.794+0000] {processor.py:154} INFO - Started process (PID=6490) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:55:31.807+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:55:31.816+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:55:31.815+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:55:32.035+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:55:33.293+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:55:33.292+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:55:33.517+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:55:33.516+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:55:33.835+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.082 seconds
[2022-12-12T13:55:44.189+0000] {processor.py:154} INFO - Started process (PID=6503) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:55:44.258+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:55:44.263+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:55:44.262+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:55:44.465+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:55:44.953+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:55:44.952+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:55:45.230+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:55:45.229+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:55:45.572+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.423 seconds
[2022-12-12T13:55:55.858+0000] {processor.py:154} INFO - Started process (PID=6513) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:55:55.906+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:55:55.911+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:55:55.910+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:55:56.035+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:55:56.770+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:55:56.769+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:55:56.944+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:55:56.942+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:55:57.238+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.398 seconds
[2022-12-12T13:56:08.256+0000] {processor.py:154} INFO - Started process (PID=6531) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:56:08.300+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:56:08.315+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:56:08.314+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:56:08.731+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:56:10.704+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:56:10.697+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:56:10.993+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:56:10.992+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:56:11.387+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.300 seconds
[2022-12-12T13:56:21.875+0000] {processor.py:154} INFO - Started process (PID=6543) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:56:21.879+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:56:21.883+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:56:21.882+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:56:22.078+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:56:22.621+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:56:22.620+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:56:23.049+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:56:23.048+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:56:23.365+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.513 seconds
[2022-12-12T13:56:33.696+0000] {processor.py:154} INFO - Started process (PID=6553) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:56:33.700+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:56:33.704+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:56:33.703+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:56:33.812+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:56:34.092+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:56:34.091+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:56:34.309+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:56:34.308+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:56:34.521+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.848 seconds
[2022-12-12T13:56:44.882+0000] {processor.py:154} INFO - Started process (PID=6563) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:56:44.886+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:56:44.890+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:56:44.889+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:56:44.995+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:56:45.171+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:56:45.171+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:56:45.307+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:56:45.306+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:56:45.452+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.591 seconds
[2022-12-12T13:56:55.677+0000] {processor.py:154} INFO - Started process (PID=6582) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:56:55.734+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:56:55.740+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:56:55.737+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:56:55.995+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:56:56.626+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:56:56.625+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:56:56.903+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:56:56.902+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:56:57.217+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.572 seconds
[2022-12-12T13:57:07.794+0000] {processor.py:154} INFO - Started process (PID=6592) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:57:07.798+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:57:07.802+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:57:07.801+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:57:07.914+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:57:08.616+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:57:19.037+0000] {processor.py:154} INFO - Started process (PID=6602) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:57:19.068+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:57:19.072+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:57:19.071+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:57:19.212+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:57:19.892+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:57:30.276+0000] {processor.py:154} INFO - Started process (PID=6612) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:57:30.281+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:57:30.285+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:57:30.284+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:57:30.388+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:57:31.229+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:57:41.982+0000] {processor.py:154} INFO - Started process (PID=6628) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:57:41.989+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:57:41.993+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:57:41.992+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:57:42.139+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:57:42.569+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:57:42.568+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:57:42.854+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:57:42.853+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:57:43.129+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.217 seconds
[2022-12-12T13:57:53.403+0000] {processor.py:154} INFO - Started process (PID=6638) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:57:53.580+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:57:53.589+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:57:53.588+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:57:53.747+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:57:54.835+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:58:05.612+0000] {processor.py:154} INFO - Started process (PID=6648) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:58:05.662+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:58:05.676+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:58:05.675+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:58:06.095+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:58:06.790+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:58:06.789+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:58:06.945+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:58:06.944+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:58:07.209+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.678 seconds
[2022-12-12T13:58:17.954+0000] {processor.py:154} INFO - Started process (PID=6664) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:58:17.964+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:58:17.986+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:58:17.982+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:58:18.428+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:58:18.917+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:58:18.905+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:58:19.291+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:58:19.290+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:58:19.543+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.653 seconds
[2022-12-12T13:58:30.108+0000] {processor.py:154} INFO - Started process (PID=6674) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:58:30.132+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:58:30.144+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:58:30.143+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:58:30.441+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:58:31.765+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:58:31.764+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:58:31.899+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:58:31.898+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:58:32.056+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.023 seconds
[2022-12-12T13:58:42.336+0000] {processor.py:154} INFO - Started process (PID=6687) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:58:42.370+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:58:42.379+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:58:42.378+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:58:42.513+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:58:43.064+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:58:43.064+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:58:43.220+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:58:43.219+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:58:43.492+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.188 seconds
[2022-12-12T13:58:53.945+0000] {processor.py:154} INFO - Started process (PID=6697) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:58:53.951+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:58:53.956+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:58:53.955+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:58:54.091+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:58:55.488+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:58:55.487+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:58:55.654+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:58:55.653+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:58:55.822+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.900 seconds
[2022-12-12T13:59:06.325+0000] {processor.py:154} INFO - Started process (PID=6715) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:59:06.337+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:59:06.341+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:59:06.340+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:59:06.565+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:59:06.931+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:59:06.930+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:59:07.226+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:59:07.221+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:59:07.622+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.314 seconds
[2022-12-12T13:59:18.172+0000] {processor.py:154} INFO - Started process (PID=6725) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:59:18.181+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:59:18.185+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:59:18.184+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:59:18.348+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:59:19.256+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T13:59:29.725+0000] {processor.py:154} INFO - Started process (PID=6735) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:59:29.730+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:59:29.739+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:59:29.738+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:59:29.901+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:59:30.102+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:59:30.101+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:59:30.256+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:59:30.255+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:59:30.433+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.737 seconds
[2022-12-12T13:59:40.682+0000] {processor.py:154} INFO - Started process (PID=6745) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:59:40.686+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:59:40.690+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:59:40.689+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:59:40.817+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:59:41.306+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:59:41.304+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:59:41.585+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:59:41.584+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:59:41.767+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.115 seconds
[2022-12-12T13:59:52.429+0000] {processor.py:154} INFO - Started process (PID=6761) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:59:52.471+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T13:59:52.476+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:59:52.475+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:59:52.835+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T13:59:53.603+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:59:53.602+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T13:59:53.946+0000] {logging_mixin.py:137} INFO - [2022-12-12T13:59:53.945+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T13:59:54.204+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.849 seconds
[2022-12-12T14:00:04.943+0000] {processor.py:154} INFO - Started process (PID=6771) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:00:04.951+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:00:04.957+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:00:04.956+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:00:05.210+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:00:05.685+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:00:05.684+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:00:05.840+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:00:05.839+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:00:06.154+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.254 seconds
[2022-12-12T14:00:16.528+0000] {processor.py:154} INFO - Started process (PID=6781) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:00:16.535+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:00:16.545+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:00:16.543+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:00:16.782+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:00:17.711+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:00:17.710+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:00:17.942+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:00:17.929+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:00:18.340+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.836 seconds
[2022-12-12T14:00:28.799+0000] {processor.py:154} INFO - Started process (PID=6788) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:00:28.830+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:00:28.840+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:00:28.839+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:00:28.987+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:00:29.242+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:00:29.240+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:00:29.517+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:00:29.516+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:00:29.982+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.198 seconds
[2022-12-12T14:00:40.642+0000] {processor.py:154} INFO - Started process (PID=6806) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:00:40.651+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:00:40.655+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:00:40.654+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:00:40.840+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:00:41.452+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:00:41.451+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:00:41.784+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:00:41.782+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:00:42.107+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.500 seconds
[2022-12-12T14:00:52.970+0000] {processor.py:154} INFO - Started process (PID=6818) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:00:52.995+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:00:53.004+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:00:53.003+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:00:53.299+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:00:53.559+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:00:53.556+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:00:53.705+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:00:53.704+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:00:54.044+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.091 seconds
[2022-12-12T14:01:04.485+0000] {processor.py:154} INFO - Started process (PID=6826) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:01:04.508+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:01:04.515+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:01:04.512+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:01:04.625+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:01:04.970+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:01:04.969+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:01:05.207+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:01:05.206+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:01:05.511+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.044 seconds
[2022-12-12T14:01:16.053+0000] {processor.py:154} INFO - Started process (PID=6845) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:01:16.101+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:01:16.119+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:01:16.118+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:01:16.481+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:01:17.870+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T14:01:28.442+0000] {processor.py:154} INFO - Started process (PID=6855) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:01:28.495+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:01:28.504+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:01:28.503+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:01:28.667+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:01:28.893+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:01:28.892+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:01:29.341+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:01:29.340+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:01:29.700+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.289 seconds
[2022-12-12T14:01:40.185+0000] {processor.py:154} INFO - Started process (PID=6865) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:01:40.196+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:01:40.222+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:01:40.221+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:01:40.452+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:01:40.656+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:01:40.653+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:01:40.948+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:01:40.947+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:01:41.383+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.248 seconds
[2022-12-12T14:01:51.967+0000] {processor.py:154} INFO - Started process (PID=6875) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:01:51.987+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:01:51.994+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:01:51.993+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:01:52.139+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:01:53.086+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T14:02:03.821+0000] {processor.py:154} INFO - Started process (PID=6895) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:02:03.865+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:02:03.878+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:02:03.872+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:02:04.276+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:02:06.029+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:02:06.028+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:02:06.230+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:02:06.229+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:02:06.478+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.684 seconds
[2022-12-12T14:02:16.899+0000] {processor.py:154} INFO - Started process (PID=6908) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:02:16.939+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:02:16.943+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:02:16.942+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:02:17.052+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:02:17.470+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:02:17.469+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:02:17.700+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:02:17.699+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:02:17.871+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.992 seconds
[2022-12-12T14:02:28.306+0000] {processor.py:154} INFO - Started process (PID=6918) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:02:28.313+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:02:28.317+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:02:28.316+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:02:28.431+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:02:28.841+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:02:28.840+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:02:28.990+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:02:28.989+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:02:29.137+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.853 seconds
[2022-12-12T14:02:39.488+0000] {processor.py:154} INFO - Started process (PID=6926) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:02:39.513+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:02:39.522+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:02:39.516+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:02:39.653+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:02:40.517+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:02:40.516+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:02:40.861+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:02:40.860+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:02:41.148+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.682 seconds
[2022-12-12T14:02:51.749+0000] {processor.py:154} INFO - Started process (PID=6944) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:02:51.824+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:02:51.834+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:02:51.829+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:02:52.100+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:02:53.007+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:02:53.005+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:02:53.167+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:02:53.166+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:02:53.329+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.623 seconds
[2022-12-12T14:03:03.658+0000] {processor.py:154} INFO - Started process (PID=6956) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:03:03.780+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:03:03.783+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:03:03.782+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:03:03.895+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:03:04.070+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:03:04.069+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:03:04.201+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:03:04.200+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:03:04.382+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.746 seconds
[2022-12-12T14:03:14.606+0000] {processor.py:154} INFO - Started process (PID=6964) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:03:14.629+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:03:14.640+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:03:14.640+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:03:14.857+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:03:16.777+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:03:16.776+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:03:16.957+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:03:16.956+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:03:17.111+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.531 seconds
[2022-12-12T14:03:27.592+0000] {processor.py:154} INFO - Started process (PID=6984) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:03:27.614+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:03:27.627+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:03:27.617+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:03:27.846+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:03:28.824+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:03:28.823+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:03:29.156+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:03:29.155+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:03:29.375+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.850 seconds
[2022-12-12T14:03:39.846+0000] {processor.py:154} INFO - Started process (PID=6994) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:03:39.914+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:03:39.918+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:03:39.917+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:03:40.052+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:03:40.531+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T14:03:51.027+0000] {processor.py:154} INFO - Started process (PID=7004) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:03:51.061+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:03:51.065+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:03:51.064+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:03:51.189+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:03:51.395+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:03:51.394+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:03:51.532+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:03:51.531+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:03:51.705+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.726 seconds
[2022-12-12T14:04:02.029+0000] {processor.py:154} INFO - Started process (PID=7014) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:04:02.066+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:04:02.073+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:04:02.069+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:04:02.205+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:04:03.674+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:04:03.673+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:04:03.804+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:04:03.803+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:04:04.007+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.999 seconds
[2022-12-12T14:04:14.666+0000] {processor.py:154} INFO - Started process (PID=7031) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:04:14.675+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:04:14.680+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:04:14.679+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:04:14.900+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:04:15.429+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:04:15.428+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:04:15.681+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:04:15.680+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:04:15.951+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.323 seconds
[2022-12-12T14:04:26.207+0000] {processor.py:154} INFO - Started process (PID=7041) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:04:26.213+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:04:26.217+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:04:26.216+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:04:26.340+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:04:26.860+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:04:26.860+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:04:27.062+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:04:27.061+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:04:27.354+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.190 seconds
[2022-12-12T14:04:37.695+0000] {processor.py:154} INFO - Started process (PID=7051) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:04:37.724+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:04:37.730+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:04:37.728+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:04:37.830+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:04:38.037+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:04:38.036+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:04:38.217+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:04:38.216+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:04:38.407+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.731 seconds
[2022-12-12T14:04:48.756+0000] {processor.py:154} INFO - Started process (PID=7061) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:04:48.772+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:04:48.780+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:04:48.779+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:04:49.050+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:04:50.042+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:04:50.037+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:04:50.330+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:04:50.329+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:04:50.768+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.031 seconds
[2022-12-12T14:05:01.392+0000] {processor.py:154} INFO - Started process (PID=7079) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:05:01.401+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:05:01.409+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:05:01.405+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:05:01.612+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:05:03.095+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:05:03.094+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:05:03.380+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:05:03.379+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:05:03.719+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.370 seconds
[2022-12-12T14:05:14.351+0000] {processor.py:154} INFO - Started process (PID=7089) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:05:14.380+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:05:14.385+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:05:14.384+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:05:14.561+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:05:14.776+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:05:14.775+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:05:14.923+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:05:14.922+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:05:15.056+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.740 seconds
[2022-12-12T14:05:25.768+0000] {processor.py:154} INFO - Started process (PID=7099) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:05:25.776+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:05:25.780+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:05:25.779+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:05:25.881+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:05:27.214+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T14:05:37.740+0000] {processor.py:154} INFO - Started process (PID=7117) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:05:37.755+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:05:37.759+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:05:37.758+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:05:37.976+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:05:38.328+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:05:38.327+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:05:38.764+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:05:38.763+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:05:39.031+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.332 seconds
[2022-12-12T14:05:49.358+0000] {processor.py:154} INFO - Started process (PID=7127) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:05:49.366+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:05:49.371+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:05:49.370+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:05:49.476+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:05:50.001+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:05:50.000+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:05:50.136+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:05:50.135+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:05:50.339+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.000 seconds
[2022-12-12T14:06:00.622+0000] {processor.py:154} INFO - Started process (PID=7137) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:06:00.628+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:06:00.632+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:06:00.631+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:06:00.745+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:06:00.941+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:06:00.940+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:06:01.089+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:06:01.087+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:06:01.246+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.644 seconds
[2022-12-12T14:06:11.881+0000] {processor.py:154} INFO - Started process (PID=7147) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:06:11.933+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:06:11.937+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:06:11.936+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:06:12.076+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:06:12.452+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:06:12.451+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:06:12.601+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:06:12.600+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:06:12.788+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.947 seconds
[2022-12-12T14:06:23.453+0000] {processor.py:154} INFO - Started process (PID=7166) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:06:23.463+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:06:23.468+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:06:23.467+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:06:23.692+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:06:25.116+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T14:06:35.905+0000] {processor.py:154} INFO - Started process (PID=7176) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:06:35.955+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:06:35.959+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:06:35.958+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:06:36.116+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:06:36.310+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:06:36.309+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:06:36.460+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:06:36.459+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:06:36.626+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.764 seconds
[2022-12-12T14:06:47.173+0000] {processor.py:154} INFO - Started process (PID=7225) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:06:47.191+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:06:47.204+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:06:47.203+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:06:47.331+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:06:47.499+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:06:47.498+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:06:47.629+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:06:47.629+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:06:47.790+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.635 seconds
[2022-12-12T14:07:36.634+0000] {processor.py:154} INFO - Started process (PID=170) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:07:36.638+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:07:36.657+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:07:36.656+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:07:36.883+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:07:38.052+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:07:38.051+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:07:38.299+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:07:38.298+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:07:38.554+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.938 seconds
[2022-12-12T14:07:48.952+0000] {processor.py:154} INFO - Started process (PID=180) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:07:49.002+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:07:49.011+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:07:49.005+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:07:49.169+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:07:49.596+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:07:49.595+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:07:49.818+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:07:49.817+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:07:50.037+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.142 seconds
[2022-12-12T14:08:00.473+0000] {processor.py:154} INFO - Started process (PID=191) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:08:00.526+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:08:00.533+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:08:00.532+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:08:00.674+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:08:01.071+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:08:01.070+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:08:01.421+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:08:01.420+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:08:01.695+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.243 seconds
[2022-12-12T14:08:12.055+0000] {processor.py:154} INFO - Started process (PID=201) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:08:12.075+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:08:12.079+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:08:12.078+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:08:12.226+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:08:12.572+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:08:12.571+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:08:12.895+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:08:12.894+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:08:13.156+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.120 seconds
[2022-12-12T14:08:23.672+0000] {processor.py:154} INFO - Started process (PID=217) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:08:23.712+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:08:23.721+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:08:23.718+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:08:23.937+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:08:24.685+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:08:24.684+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:08:25.083+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:08:25.082+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:08:25.341+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.718 seconds
[2022-12-12T14:08:35.696+0000] {processor.py:154} INFO - Started process (PID=227) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:08:35.753+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:08:35.757+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:08:35.756+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:08:35.892+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:08:36.468+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:08:36.467+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:08:36.671+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:08:36.670+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:08:36.819+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.142 seconds
[2022-12-12T14:08:47.176+0000] {processor.py:154} INFO - Started process (PID=235) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:08:47.199+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:08:47.205+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:08:47.204+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:08:47.316+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:08:48.319+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T14:08:58.944+0000] {processor.py:154} INFO - Started process (PID=253) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:08:59.006+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:08:59.020+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:08:59.019+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:08:59.292+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:09:00.262+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:09:00.249+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:09:00.509+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:09:00.508+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:09:00.792+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.949 seconds
[2022-12-12T14:09:11.340+0000] {processor.py:154} INFO - Started process (PID=263) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:09:11.408+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:09:11.412+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:09:11.411+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:09:11.525+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:09:13.240+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T14:09:23.860+0000] {processor.py:154} INFO - Started process (PID=276) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:09:23.881+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:09:23.890+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:09:23.884+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:09:24.061+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:09:24.414+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:09:24.413+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:09:24.596+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:09:24.595+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:09:24.831+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.988 seconds
[2022-12-12T14:09:35.016+0000] {processor.py:154} INFO - Started process (PID=288) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:09:35.041+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:09:35.045+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:09:35.044+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:09:35.153+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:09:35.548+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:09:35.547+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:09:35.711+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:09:35.710+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:09:35.918+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.922 seconds
[2022-12-12T14:09:46.524+0000] {processor.py:154} INFO - Started process (PID=304) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:09:46.575+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:09:46.587+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:09:46.586+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:09:46.814+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:09:47.318+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:09:47.302+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:09:47.643+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:09:47.642+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:09:47.943+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.506 seconds
[2022-12-12T14:09:58.349+0000] {processor.py:154} INFO - Started process (PID=314) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:09:58.419+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:09:58.427+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:09:58.426+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:09:58.578+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:09:59.175+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:09:59.174+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:09:59.395+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:09:59.394+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:09:59.563+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.231 seconds
[2022-12-12T14:10:09.953+0000] {processor.py:154} INFO - Started process (PID=324) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:10:10.009+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:10:10.013+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:10:10.012+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:10:10.436+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:10:10.949+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:10:10.948+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:10:11.233+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:10:11.232+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:10:11.416+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.491 seconds
[2022-12-12T14:10:22.006+0000] {processor.py:154} INFO - Started process (PID=340) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:10:22.012+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:10:22.032+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:10:22.031+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:10:22.320+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:10:23.221+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:10:23.220+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:10:24.253+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:10:24.225+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:10:24.956+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.016 seconds
[2022-12-12T14:10:35.560+0000] {processor.py:154} INFO - Started process (PID=353) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:10:35.585+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:10:35.589+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:10:35.588+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:10:35.775+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:10:36.727+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:10:36.726+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:10:37.018+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:10:37.017+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:10:37.174+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.646 seconds
[2022-12-12T14:10:47.958+0000] {processor.py:154} INFO - Started process (PID=363) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:10:48.044+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:10:48.055+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:10:48.054+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:10:48.254+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:10:48.582+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:10:48.581+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:10:48.743+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:10:48.742+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:10:48.908+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.980 seconds
[2022-12-12T14:10:59.105+0000] {processor.py:154} INFO - Started process (PID=373) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:10:59.157+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:10:59.161+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:10:59.160+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:10:59.426+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:10:59.937+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:10:59.936+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:11:00.240+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:11:00.239+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:11:00.377+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.297 seconds
[2022-12-12T14:11:10.956+0000] {processor.py:154} INFO - Started process (PID=392) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:11:10.979+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:11:10.988+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:11:10.987+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:11:11.218+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:11:12.029+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:11:12.028+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:11:12.524+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:11:12.523+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:11:12.674+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.804 seconds
[2022-12-12T14:11:23.511+0000] {processor.py:154} INFO - Started process (PID=402) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:11:23.567+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:11:23.577+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:11:23.576+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:11:23.933+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:11:24.746+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:11:24.745+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:11:24.918+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:11:24.917+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:11:25.068+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.652 seconds
[2022-12-12T14:11:36.087+0000] {processor.py:154} INFO - Started process (PID=412) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:11:36.132+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:11:36.137+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:11:36.136+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:11:36.235+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:11:37.203+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T14:11:47.736+0000] {processor.py:154} INFO - Started process (PID=428) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:11:47.763+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:11:47.772+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:11:47.771+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:11:48.016+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:11:48.702+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:11:48.701+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:11:49.142+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:11:49.142+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:11:49.408+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.702 seconds
[2022-12-12T14:11:59.942+0000] {processor.py:154} INFO - Started process (PID=439) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:11:59.996+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:12:00.005+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:12:00.001+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:12:00.110+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:12:01.633+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T14:12:11.993+0000] {processor.py:154} INFO - Started process (PID=449) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:12:12.018+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:12:12.022+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:12:12.021+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:12:12.125+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:12:12.433+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:12:12.432+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:12:12.613+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:12:12.612+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:12:12.778+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.808 seconds
[2022-12-12T14:12:22.948+0000] {processor.py:154} INFO - Started process (PID=459) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:12:22.976+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:12:22.981+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:12:22.980+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:12:23.083+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:12:23.324+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:12:23.323+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:12:23.498+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:12:23.497+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:12:23.674+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.742 seconds
[2022-12-12T14:12:33.907+0000] {processor.py:154} INFO - Started process (PID=478) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:12:34.001+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:12:34.013+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:12:34.012+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:12:34.491+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:12:35.320+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:12:35.319+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:12:35.574+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:12:35.566+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:12:35.889+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.010 seconds
[2022-12-12T14:12:46.181+0000] {processor.py:154} INFO - Started process (PID=488) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:12:46.211+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:12:46.220+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:12:46.219+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:12:46.438+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:12:47.073+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:12:47.072+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:12:47.245+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:12:47.244+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:12:47.411+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.267 seconds
[2022-12-12T14:12:57.687+0000] {processor.py:154} INFO - Started process (PID=498) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:12:57.713+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:12:57.717+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:12:57.716+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:12:57.827+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:12:59.485+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:12:59.484+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:12:59.817+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:12:59.816+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:12:59.939+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.270 seconds
[2022-12-12T14:13:10.319+0000] {processor.py:154} INFO - Started process (PID=508) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:13:10.324+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:13:10.333+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:13:10.332+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:13:10.496+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:13:10.874+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:13:10.873+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:13:11.192+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:13:11.191+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:13:11.632+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.333 seconds
[2022-12-12T14:13:22.409+0000] {processor.py:154} INFO - Started process (PID=526) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:13:22.423+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:13:22.438+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:13:22.437+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:13:22.586+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:13:23.090+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:13:23.089+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:13:23.480+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:13:23.479+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:13:24.099+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.736 seconds
[2022-12-12T14:13:34.544+0000] {processor.py:154} INFO - Started process (PID=536) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:13:34.550+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:13:34.554+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:13:34.553+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:13:34.655+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:13:35.025+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:13:35.021+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:13:35.198+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:13:35.193+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:13:35.764+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.234 seconds
[2022-12-12T14:13:46.089+0000] {processor.py:154} INFO - Started process (PID=546) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:13:46.094+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:13:46.099+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:13:46.098+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:13:46.208+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:13:46.533+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:13:46.532+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:13:46.778+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:13:46.773+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:13:47.082+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.011 seconds
[2022-12-12T14:13:57.781+0000] {processor.py:154} INFO - Started process (PID=563) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:13:57.831+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:13:57.856+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:13:57.855+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:13:58.279+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:13:59.572+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:13:59.557+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:14:00.475+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:14:00.469+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:14:00.688+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.928 seconds
[2022-12-12T14:14:11.266+0000] {processor.py:154} INFO - Started process (PID=574) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:14:11.308+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:14:11.312+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:14:11.311+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:14:11.505+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:14:12.785+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:14:12.784+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:14:12.998+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:14:12.997+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:14:13.202+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.974 seconds
[2022-12-12T14:14:23.932+0000] {processor.py:154} INFO - Started process (PID=584) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:14:23.937+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:14:23.944+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:14:23.943+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:14:24.108+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:14:25.388+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T14:14:36.114+0000] {processor.py:154} INFO - Started process (PID=594) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:14:36.175+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:14:36.191+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:14:36.190+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:14:36.564+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:14:37.134+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:14:37.133+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:14:37.367+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:14:37.366+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:14:37.687+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.631 seconds
[2022-12-12T14:14:48.438+0000] {processor.py:154} INFO - Started process (PID=612) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:14:48.488+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:14:48.496+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:14:48.495+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:14:48.880+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:14:51.285+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:14:51.284+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:14:51.828+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:14:51.827+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:14:52.117+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.740 seconds
[2022-12-12T14:15:02.816+0000] {processor.py:154} INFO - Started process (PID=622) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:15:02.824+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:15:02.835+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:15:02.834+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:15:03.017+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:15:04.389+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T14:15:14.970+0000] {processor.py:154} INFO - Started process (PID=632) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:15:14.975+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:15:14.980+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:15:14.979+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:15:15.172+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:15:15.921+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:15:15.919+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:15:16.192+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:15:16.191+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:15:16.403+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.451 seconds
[2022-12-12T14:15:27.283+0000] {processor.py:154} INFO - Started process (PID=642) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:15:27.335+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:15:27.340+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:15:27.339+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:15:27.525+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:15:28.100+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:15:28.099+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:15:28.396+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:15:28.393+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:15:28.714+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.447 seconds
[2022-12-12T14:15:39.451+0000] {processor.py:154} INFO - Started process (PID=660) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:15:39.479+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:15:39.496+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:15:39.495+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:15:39.941+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:15:40.816+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:15:40.815+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:15:41.681+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:15:41.680+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:15:42.303+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.934 seconds
[2022-12-12T14:15:52.700+0000] {processor.py:154} INFO - Started process (PID=670) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:15:52.710+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:15:52.719+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:15:52.718+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:15:52.894+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:15:53.257+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:15:53.256+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:15:53.592+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:15:53.589+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:15:53.978+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.333 seconds
[2022-12-12T14:16:04.493+0000] {processor.py:154} INFO - Started process (PID=680) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:16:04.518+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:16:04.523+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:16:04.521+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:16:04.646+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:16:05.584+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:16:05.582+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:16:05.804+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:16:05.803+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:16:06.068+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.601 seconds
[2022-12-12T14:16:16.386+0000] {processor.py:154} INFO - Started process (PID=690) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:16:16.402+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:16:16.415+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:16:16.414+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:16:16.621+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:16:16.982+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:16:16.977+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:16:17.198+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:16:17.197+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:16:17.386+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.035 seconds
[2022-12-12T14:16:27.916+0000] {processor.py:154} INFO - Started process (PID=709) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:16:27.933+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:16:27.942+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:16:27.936+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:16:28.192+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:16:28.787+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:16:28.779+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:16:29.314+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:16:29.309+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:16:29.916+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.044 seconds
[2022-12-12T14:16:40.843+0000] {processor.py:154} INFO - Started process (PID=719) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:16:40.877+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:16:40.881+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:16:40.881+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:16:41.051+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:16:41.369+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:16:41.368+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:16:41.539+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:16:41.538+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:16:41.683+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.875 seconds
[2022-12-12T14:16:52.270+0000] {processor.py:154} INFO - Started process (PID=729) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:16:52.324+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:16:52.329+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:16:52.328+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:16:52.457+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:16:53.121+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:16:53.121+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:16:53.282+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:16:53.281+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:16:53.436+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.182 seconds
[2022-12-12T14:17:03.760+0000] {processor.py:154} INFO - Started process (PID=739) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:17:03.786+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:17:03.799+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:17:03.789+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:17:04.045+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:17:04.391+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:17:04.390+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:17:04.684+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:17:04.683+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:17:04.941+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.236 seconds
[2022-12-12T14:17:15.491+0000] {processor.py:154} INFO - Started process (PID=756) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:17:15.499+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:17:15.503+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:17:15.502+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:17:15.693+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:17:16.222+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:17:16.221+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:17:16.708+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:17:16.707+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:17:16.952+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.529 seconds
[2022-12-12T14:17:27.638+0000] {processor.py:154} INFO - Started process (PID=766) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:17:27.647+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:17:27.655+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:17:27.654+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:17:27.785+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:17:28.335+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:17:28.333+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:17:28.573+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:17:28.565+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:17:28.759+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.168 seconds
[2022-12-12T14:17:39.219+0000] {processor.py:154} INFO - Started process (PID=776) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:17:39.243+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:17:39.248+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:17:39.246+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:17:39.347+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:17:40.307+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:17:40.305+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:17:40.509+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:17:40.508+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:17:40.657+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.460 seconds
[2022-12-12T14:17:51.045+0000] {processor.py:154} INFO - Started process (PID=786) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:17:51.093+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:17:51.119+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:17:51.117+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:17:51.246+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:17:51.518+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:17:51.517+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:17:51.684+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:17:51.683+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:17:51.870+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.843 seconds
[2022-12-12T14:18:02.684+0000] {processor.py:154} INFO - Started process (PID=803) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:18:02.710+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:18:02.716+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:18:02.715+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:18:03.269+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:18:03.901+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:18:03.900+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:18:04.206+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:18:04.200+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:18:04.495+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.893 seconds
[2022-12-12T14:18:14.931+0000] {processor.py:154} INFO - Started process (PID=813) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:18:14.952+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:18:14.956+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:18:14.955+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:18:15.112+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:18:15.350+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:18:15.349+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:18:15.499+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:18:15.498+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:18:15.660+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.786 seconds
[2022-12-12T14:18:25.876+0000] {processor.py:154} INFO - Started process (PID=823) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:18:25.908+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:18:25.912+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:18:25.911+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:18:26.342+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:18:26.765+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:18:26.764+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:18:26.935+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:18:26.934+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:18:27.096+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.241 seconds
[2022-12-12T14:18:37.414+0000] {processor.py:154} INFO - Started process (PID=833) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:18:37.430+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:18:37.434+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:18:37.433+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:18:37.555+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:18:38.402+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:18:38.396+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:18:38.929+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:18:38.928+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:18:39.588+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.190 seconds
[2022-12-12T14:18:50.047+0000] {processor.py:154} INFO - Started process (PID=851) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:18:50.104+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:18:50.114+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:18:50.111+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:18:50.343+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:18:51.433+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:18:51.432+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:18:51.769+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:18:51.768+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:18:51.950+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.983 seconds
[2022-12-12T14:19:02.244+0000] {processor.py:154} INFO - Started process (PID=861) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:19:02.292+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:19:02.297+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:19:02.296+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:19:02.392+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:19:02.715+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:19:02.710+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:19:03.048+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:19:03.047+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:19:03.318+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.091 seconds
[2022-12-12T14:19:13.671+0000] {processor.py:154} INFO - Started process (PID=871) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:19:13.717+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:19:13.722+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:19:13.721+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:19:13.815+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:19:14.049+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:19:14.048+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:19:14.272+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:19:14.271+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:19:14.719+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.062 seconds
[2022-12-12T14:19:25.060+0000] {processor.py:154} INFO - Started process (PID=889) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:19:25.110+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:19:25.114+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:19:25.113+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:19:25.339+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:19:26.137+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:19:26.136+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:19:26.671+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:19:26.668+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:19:26.934+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.910 seconds
[2022-12-12T14:19:37.584+0000] {processor.py:154} INFO - Started process (PID=899) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:19:37.630+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:19:37.644+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:19:37.643+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:19:37.772+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:19:39.270+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T14:19:50.096+0000] {processor.py:154} INFO - Started process (PID=909) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:19:50.134+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:19:50.137+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:19:50.137+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:19:50.366+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:19:50.936+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:19:50.934+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:19:51.122+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:19:51.121+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:19:51.269+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.192 seconds
[2022-12-12T14:20:01.588+0000] {processor.py:154} INFO - Started process (PID=919) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:20:01.639+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:20:01.643+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:20:01.642+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:20:01.769+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:20:02.867+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:20:02.865+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:20:03.011+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:20:03.010+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:20:03.169+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.595 seconds
[2022-12-12T14:20:13.824+0000] {processor.py:154} INFO - Started process (PID=937) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:20:13.882+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:20:13.886+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:20:13.885+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:20:14.285+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:20:15.533+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:20:15.532+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:20:15.841+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:20:15.840+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:20:16.091+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.315 seconds
[2022-12-12T14:20:27.054+0000] {processor.py:154} INFO - Started process (PID=947) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:20:27.088+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:20:27.092+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:20:27.091+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:20:27.220+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:20:27.488+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:20:27.487+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:20:27.647+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:20:27.646+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:20:27.813+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.774 seconds
[2022-12-12T14:20:38.458+0000] {processor.py:154} INFO - Started process (PID=957) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:20:38.479+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:20:38.484+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:20:38.482+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:20:38.639+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:20:38.957+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:20:38.956+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:20:39.145+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:20:39.143+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:20:39.295+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.855 seconds
[2022-12-12T14:20:50.176+0000] {processor.py:154} INFO - Started process (PID=975) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:20:50.235+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:20:50.244+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:20:50.243+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:20:50.531+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:20:51.240+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:20:51.239+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:20:51.471+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:20:51.470+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:20:51.780+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.650 seconds
[2022-12-12T14:21:02.411+0000] {processor.py:154} INFO - Started process (PID=985) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:21:02.423+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:21:02.428+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:21:02.427+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:21:02.572+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:21:02.935+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:21:02.933+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:21:03.104+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:21:03.100+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:21:03.267+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.895 seconds
[2022-12-12T14:21:14.359+0000] {processor.py:154} INFO - Started process (PID=995) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:21:14.379+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:21:14.394+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:21:14.381+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:21:14.688+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:21:15.144+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:21:15.143+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:21:15.303+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:21:15.302+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:21:15.453+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.158 seconds
[2022-12-12T14:21:25.609+0000] {processor.py:154} INFO - Started process (PID=1005) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:21:25.659+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:21:25.664+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:21:25.663+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:21:25.759+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:21:26.065+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:21:26.064+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:21:26.481+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:21:26.480+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:21:26.791+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.199 seconds
[2022-12-12T14:21:37.313+0000] {processor.py:154} INFO - Started process (PID=1023) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:21:37.345+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:21:37.349+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:21:37.348+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:21:37.560+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:21:39.360+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:21:39.355+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:21:39.571+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:21:39.570+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:21:39.873+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.588 seconds
[2022-12-12T14:21:50.811+0000] {processor.py:154} INFO - Started process (PID=1033) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:21:50.839+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:21:50.847+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:21:50.846+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:21:51.175+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:21:52.273+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:21:52.272+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:21:52.464+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:21:52.463+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:21:52.669+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.904 seconds
[2022-12-12T14:22:03.085+0000] {processor.py:154} INFO - Started process (PID=1043) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:22:03.116+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:22:03.121+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:22:03.120+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:22:03.213+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:22:04.045+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:22:04.044+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:22:04.213+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:22:04.212+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:22:04.375+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.308 seconds
[2022-12-12T14:22:15.392+0000] {processor.py:154} INFO - Started process (PID=1053) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:22:15.448+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:22:15.452+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:22:15.451+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:22:15.551+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:22:16.601+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:22:16.600+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:22:16.824+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:22:16.823+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:22:17.124+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.749 seconds
[2022-12-12T14:22:27.712+0000] {processor.py:154} INFO - Started process (PID=1071) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:22:27.716+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:22:27.722+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:22:27.721+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:22:27.920+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:22:28.526+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:22:28.520+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:22:28.863+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:22:28.861+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:22:29.327+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.636 seconds
[2022-12-12T14:22:39.629+0000] {processor.py:154} INFO - Started process (PID=1081) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:22:39.675+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:22:39.680+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:22:39.678+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:22:39.814+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:22:40.197+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:22:40.196+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:22:40.395+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:22:40.394+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:22:40.569+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.958 seconds
[2022-12-12T14:22:50.936+0000] {processor.py:154} INFO - Started process (PID=1091) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:22:50.988+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:22:50.991+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:22:50.991+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:22:51.107+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:22:51.438+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:22:51.436+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:22:51.627+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:22:51.626+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:22:51.797+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.896 seconds
[2022-12-12T14:23:02.409+0000] {processor.py:154} INFO - Started process (PID=1108) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:23:02.434+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:23:02.445+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:23:02.437+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:23:02.591+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:23:03.433+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:23:03.432+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:23:03.638+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:23:03.637+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:23:03.845+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.470 seconds
[2022-12-12T14:23:14.516+0000] {processor.py:154} INFO - Started process (PID=1119) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:23:14.534+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:23:14.543+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:23:14.537+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:23:14.737+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:23:15.149+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:23:15.148+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:23:15.311+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:23:15.310+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:23:15.459+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.999 seconds
[2022-12-12T14:23:25.888+0000] {processor.py:154} INFO - Started process (PID=1129) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:23:25.915+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:23:25.919+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:23:25.918+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:23:26.024+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:23:26.272+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:23:26.271+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:23:26.430+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:23:26.429+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:23:26.588+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.714 seconds
[2022-12-12T14:23:37.437+0000] {processor.py:154} INFO - Started process (PID=1139) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:23:37.460+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:23:37.465+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:23:37.464+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:23:37.573+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:23:38.149+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:23:38.148+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:23:38.355+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:23:38.354+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:23:38.742+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.319 seconds
[2022-12-12T14:23:49.679+0000] {processor.py:154} INFO - Started process (PID=1157) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:23:49.747+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:23:49.764+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:23:49.763+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:23:50.348+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:23:51.678+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T14:24:02.320+0000] {processor.py:154} INFO - Started process (PID=1167) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:24:02.368+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:24:02.372+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:24:02.371+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:24:02.471+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:24:02.728+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:24:02.727+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:24:02.883+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:24:02.882+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:24:03.042+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.737 seconds
[2022-12-12T14:24:13.375+0000] {processor.py:154} INFO - Started process (PID=1177) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:24:13.415+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:24:13.419+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:24:13.419+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:24:13.522+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:24:13.861+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:24:13.860+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:24:14.013+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:24:14.012+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:24:14.196+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.835 seconds
[2022-12-12T14:24:24.547+0000] {processor.py:154} INFO - Started process (PID=1187) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:24:24.564+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:24:24.570+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:24:24.569+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:24:24.660+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:24:25.309+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:24:25.308+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:24:25.469+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:24:25.468+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:24:25.599+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.066 seconds
[2022-12-12T14:24:35.982+0000] {processor.py:154} INFO - Started process (PID=1205) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:24:36.029+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:24:36.033+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:24:36.032+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:24:36.271+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:24:36.678+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:24:36.677+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:24:36.879+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:24:36.878+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:24:37.188+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.253 seconds
[2022-12-12T14:24:47.576+0000] {processor.py:154} INFO - Started process (PID=1215) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:24:47.606+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:24:47.612+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:24:47.611+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:24:47.716+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:24:48.028+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:24:48.027+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:24:48.226+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:24:48.214+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:24:48.443+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.886 seconds
[2022-12-12T14:24:59.198+0000] {processor.py:154} INFO - Started process (PID=1225) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:24:59.219+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:24:59.223+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:24:59.222+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:24:59.326+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:24:59.807+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:24:59.806+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:24:59.963+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:24:59.962+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:25:00.243+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.063 seconds
[2022-12-12T14:25:10.617+0000] {processor.py:154} INFO - Started process (PID=1235) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:25:10.688+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:25:10.694+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:25:10.693+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:25:10.849+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:25:12.340+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T14:25:23.106+0000] {processor.py:154} INFO - Started process (PID=1253) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:25:23.128+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:25:23.135+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:25:23.134+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:25:23.403+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:25:24.267+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:25:24.249+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:25:24.745+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:25:24.744+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:25:25.087+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.038 seconds
[2022-12-12T14:25:35.543+0000] {processor.py:154} INFO - Started process (PID=1266) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:25:35.615+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:25:35.624+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:25:35.623+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:25:35.826+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:25:36.191+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:25:36.190+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:25:36.389+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:25:36.388+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:25:36.671+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.144 seconds
[2022-12-12T14:25:46.969+0000] {processor.py:154} INFO - Started process (PID=1273) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:25:46.987+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:25:46.991+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:25:46.990+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:25:47.084+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:25:48.099+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T14:25:58.560+0000] {processor.py:154} INFO - Started process (PID=1290) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:25:58.593+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:25:58.597+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:25:58.596+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:25:58.858+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:25:59.276+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:25:59.275+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:25:59.461+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:25:59.460+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:25:59.791+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.269 seconds
[2022-12-12T14:26:10.540+0000] {processor.py:154} INFO - Started process (PID=1301) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:26:10.571+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:26:10.575+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:26:10.574+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:26:10.679+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:26:11.376+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:26:11.375+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:26:11.558+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:26:11.557+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:26:11.715+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.200 seconds
[2022-12-12T14:26:22.091+0000] {processor.py:154} INFO - Started process (PID=1311) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:26:22.128+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:26:22.136+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:26:22.135+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:26:22.243+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:26:23.180+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T14:26:33.636+0000] {processor.py:154} INFO - Started process (PID=1321) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:26:33.661+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:26:33.666+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:26:33.665+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:26:33.762+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:26:34.955+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:26:34.954+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:26:35.169+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:26:35.168+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:26:35.384+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.765 seconds
[2022-12-12T14:26:45.885+0000] {processor.py:154} INFO - Started process (PID=1341) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:26:45.908+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:26:45.920+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:26:45.919+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:26:46.233+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:26:47.341+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:26:47.340+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:26:47.677+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:26:47.676+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:26:47.978+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.126 seconds
[2022-12-12T14:26:58.510+0000] {processor.py:154} INFO - Started process (PID=1351) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:26:58.645+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:26:58.649+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:26:58.648+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:26:58.753+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:26:59.149+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:26:59.148+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:26:59.307+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:26:59.306+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:26:59.464+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.988 seconds
[2022-12-12T14:27:09.814+0000] {processor.py:154} INFO - Started process (PID=1359) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:27:09.836+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:27:09.841+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:27:09.840+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:27:09.961+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:27:10.196+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:27:10.195+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:27:10.349+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:27:10.348+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:27:10.506+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.706 seconds
[2022-12-12T14:27:20.872+0000] {processor.py:154} INFO - Started process (PID=1369) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:27:20.882+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:27:20.886+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:27:20.885+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:27:21.019+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:27:21.504+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:27:21.503+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:27:21.669+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:27:21.668+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:27:21.812+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.961 seconds
[2022-12-12T14:27:32.419+0000] {processor.py:154} INFO - Started process (PID=1387) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:27:32.427+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:27:32.433+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:27:32.432+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:27:32.566+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:27:33.387+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:27:33.385+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:27:33.952+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:27:33.951+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:27:34.596+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.213 seconds
[2022-12-12T14:27:45.318+0000] {processor.py:154} INFO - Started process (PID=1397) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:27:45.369+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:27:45.373+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:27:45.372+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:27:45.468+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:27:45.732+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:27:45.731+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:27:45.923+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:27:45.922+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:27:46.077+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.777 seconds
[2022-12-12T14:27:56.437+0000] {processor.py:154} INFO - Started process (PID=1407) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:27:56.462+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:27:56.466+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:27:56.465+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:27:56.561+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:27:56.797+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:27:56.796+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:27:56.958+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:27:56.958+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:27:57.114+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.693 seconds
[2022-12-12T14:28:07.551+0000] {processor.py:154} INFO - Started process (PID=1417) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:28:07.579+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:28:07.583+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:28:07.582+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:28:07.729+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:28:08.920+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:28:08.913+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:28:09.719+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:28:09.705+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:28:10.153+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.684 seconds
[2022-12-12T14:28:20.895+0000] {processor.py:154} INFO - Started process (PID=1434) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:28:20.910+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:28:20.919+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:28:20.913+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:28:21.129+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:28:21.623+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:28:21.622+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:28:22.030+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:28:22.021+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:28:22.284+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.469 seconds
[2022-12-12T14:28:32.760+0000] {processor.py:154} INFO - Started process (PID=1444) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:28:32.787+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:28:32.791+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:28:32.790+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:28:32.912+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:28:34.122+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T14:28:44.605+0000] {processor.py:154} INFO - Started process (PID=1454) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:28:44.652+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:28:44.656+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:28:44.655+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:28:44.802+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:28:45.195+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:28:45.194+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:28:45.409+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:28:45.408+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:28:45.614+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.042 seconds
[2022-12-12T14:28:55.999+0000] {processor.py:154} INFO - Started process (PID=1471) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:28:56.064+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:28:56.067+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:28:56.067+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:28:56.280+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:28:56.859+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:28:56.854+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:28:57.388+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:28:57.387+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:28:57.771+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.818 seconds
[2022-12-12T14:29:08.490+0000] {processor.py:154} INFO - Started process (PID=1482) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:29:08.518+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:29:08.522+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:29:08.521+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:29:08.739+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:29:09.202+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:29:09.201+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:29:09.661+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:29:09.660+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:29:10.052+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.595 seconds
[2022-12-12T14:29:20.462+0000] {processor.py:154} INFO - Started process (PID=1492) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:29:20.525+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:29:20.534+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:29:20.532+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:29:20.735+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:29:21.027+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:29:21.026+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:29:21.192+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:29:21.191+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:29:21.408+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.989 seconds
[2022-12-12T14:29:31.868+0000] {processor.py:154} INFO - Started process (PID=1502) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:29:31.923+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:29:31.927+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:29:31.926+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:29:32.066+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:29:33.013+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T14:29:43.679+0000] {processor.py:154} INFO - Started process (PID=1518) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:29:43.722+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:29:43.731+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:29:43.725+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:29:44.015+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:29:44.472+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:29:44.470+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:29:44.747+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:29:44.746+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:29:45.042+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.437 seconds
[2022-12-12T14:29:55.467+0000] {processor.py:154} INFO - Started process (PID=1530) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:29:55.481+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:29:55.485+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:29:55.484+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:29:55.720+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:29:56.389+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:29:56.388+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:29:56.745+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:29:56.730+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:29:57.216+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.788 seconds
[2022-12-12T14:30:07.760+0000] {processor.py:154} INFO - Started process (PID=1540) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:30:07.810+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:30:07.815+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:30:07.813+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:30:07.922+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:30:09.193+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:30:09.192+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:30:09.348+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:30:09.347+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:30:09.516+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.771 seconds
[2022-12-12T14:30:20.202+0000] {processor.py:154} INFO - Started process (PID=1550) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:30:20.231+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:30:20.235+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:30:20.234+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:30:20.368+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:30:20.715+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:30:20.714+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:30:20.917+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:30:20.916+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:30:21.121+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.942 seconds
[2022-12-12T14:30:31.921+0000] {processor.py:154} INFO - Started process (PID=1568) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:30:31.943+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:30:31.947+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:30:31.946+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:30:32.333+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:30:34.313+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T14:30:45.079+0000] {processor.py:154} INFO - Started process (PID=1578) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:30:45.084+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:30:45.088+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:30:45.087+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:30:45.392+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:30:46.153+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:30:46.152+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:30:46.550+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:30:46.549+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:30:46.685+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.702 seconds
[2022-12-12T14:30:57.164+0000] {processor.py:154} INFO - Started process (PID=1588) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:30:57.185+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:30:57.199+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:30:57.198+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:30:57.332+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:30:58.056+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:30:58.053+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:30:58.255+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:30:58.253+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:30:58.426+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.291 seconds
[2022-12-12T14:31:08.836+0000] {processor.py:154} INFO - Started process (PID=1598) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:31:08.864+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:31:08.868+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:31:08.867+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:31:08.987+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:31:10.292+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:31:10.291+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:31:10.509+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:31:10.508+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:31:10.678+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.868 seconds
[2022-12-12T14:31:21.480+0000] {processor.py:154} INFO - Started process (PID=1615) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:31:21.506+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:31:21.511+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:31:21.509+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:31:21.739+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:31:22.896+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:31:22.895+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:31:23.293+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:31:23.292+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:31:23.885+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.437 seconds
[2022-12-12T14:31:34.588+0000] {processor.py:154} INFO - Started process (PID=1625) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:31:34.614+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:31:34.618+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:31:34.617+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:31:34.761+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:31:35.210+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:31:35.205+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:31:35.375+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:31:35.374+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:31:35.535+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.979 seconds
[2022-12-12T14:31:45.991+0000] {processor.py:154} INFO - Started process (PID=1635) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:31:46.017+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:31:46.021+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:31:46.020+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:31:46.142+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:31:46.976+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T14:31:57.464+0000] {processor.py:154} INFO - Started process (PID=1645) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:31:57.469+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:31:57.475+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:31:57.472+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:31:57.606+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:31:57.995+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:31:57.994+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:31:58.188+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:31:58.188+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:31:58.363+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.918 seconds
[2022-12-12T14:32:08.954+0000] {processor.py:154} INFO - Started process (PID=1665) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:32:08.976+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:32:08.980+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:32:08.979+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:32:09.210+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:32:10.051+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:32:10.050+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:32:10.428+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:32:10.427+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:32:10.736+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.812 seconds
[2022-12-12T14:32:21.569+0000] {processor.py:154} INFO - Started process (PID=1675) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:32:21.607+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:32:21.629+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:32:21.620+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:32:21.787+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:32:22.567+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:32:22.566+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:32:22.804+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:32:22.803+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:32:23.096+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.571 seconds
[2022-12-12T14:32:33.463+0000] {processor.py:154} INFO - Started process (PID=1685) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:32:33.474+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:32:33.479+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:32:33.477+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:32:33.603+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:32:34.490+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:32:34.489+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:32:34.709+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:32:34.708+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:32:34.980+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.538 seconds
[2022-12-12T14:32:45.528+0000] {processor.py:154} INFO - Started process (PID=1695) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:32:45.552+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:32:45.556+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:32:45.555+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:32:45.700+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:32:46.246+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:32:46.241+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:32:46.576+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:32:46.575+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:32:46.737+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.245 seconds
[2022-12-12T14:32:57.276+0000] {processor.py:154} INFO - Started process (PID=1713) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:32:57.307+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:32:57.348+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:32:57.347+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:32:57.696+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:32:58.369+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:32:58.368+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:32:58.754+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:32:58.748+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:32:58.981+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.808 seconds
[2022-12-12T14:33:09.392+0000] {processor.py:154} INFO - Started process (PID=1723) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:33:09.395+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:33:09.400+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:33:09.399+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:33:09.531+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:33:09.823+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:33:09.820+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:33:09.986+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:33:09.985+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:33:10.159+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.800 seconds
[2022-12-12T14:33:20.632+0000] {processor.py:154} INFO - Started process (PID=1733) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:33:20.667+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:33:20.671+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:33:20.670+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:33:20.875+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:33:21.898+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:33:21.897+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:33:22.125+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:33:22.124+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:33:22.377+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.776 seconds
[2022-12-12T14:33:32.953+0000] {processor.py:154} INFO - Started process (PID=1743) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:33:32.958+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:33:32.968+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:33:32.967+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:33:33.102+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:33:33.572+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:33:33.571+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:33:33.872+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:33:33.872+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:33:34.148+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.219 seconds
[2022-12-12T14:33:44.607+0000] {processor.py:154} INFO - Started process (PID=1759) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:33:44.616+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:33:44.631+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:33:44.630+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:33:45.056+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:33:45.821+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:33:45.820+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:33:46.342+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:33:46.333+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:33:46.665+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.100 seconds
[2022-12-12T14:33:57.309+0000] {processor.py:154} INFO - Started process (PID=1769) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:33:57.327+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:33:57.341+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:33:57.340+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:33:57.596+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:33:58.145+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:33:58.144+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:33:58.543+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:33:58.539+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:33:58.713+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.455 seconds
[2022-12-12T14:34:09.236+0000] {processor.py:154} INFO - Started process (PID=1779) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:34:09.240+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:34:09.252+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:34:09.251+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:34:09.501+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:34:10.006+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:34:10.004+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:34:10.287+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:34:10.286+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:34:10.443+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.252 seconds
[2022-12-12T14:34:20.961+0000] {processor.py:154} INFO - Started process (PID=1789) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:34:20.973+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:34:20.999+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:34:20.989+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:34:21.201+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:34:21.570+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:34:21.569+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:34:21.756+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:34:21.755+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:34:22.030+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.109 seconds
[2022-12-12T14:34:32.444+0000] {processor.py:154} INFO - Started process (PID=1807) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:34:32.502+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:34:32.511+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:34:32.505+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:34:32.916+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:34:34.202+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:34:34.191+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:34:34.721+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:34:34.720+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:34:35.028+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.647 seconds
[2022-12-12T14:34:45.564+0000] {processor.py:154} INFO - Started process (PID=1817) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:34:45.575+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:34:45.584+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:34:45.583+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:34:45.785+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:34:46.829+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:34:46.828+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:34:47.025+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:34:47.025+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:34:47.191+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.688 seconds
[2022-12-12T14:34:57.433+0000] {processor.py:154} INFO - Started process (PID=1829) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:34:57.436+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:34:57.443+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:34:57.440+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:34:57.564+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:34:58.323+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T14:35:08.688+0000] {processor.py:154} INFO - Started process (PID=1837) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:35:08.748+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:35:08.756+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:35:08.755+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:35:08.957+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:35:10.688+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T14:35:21.270+0000] {processor.py:154} INFO - Started process (PID=1857) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:35:21.274+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:35:21.279+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:35:21.277+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:35:21.402+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:35:21.781+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:35:21.780+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:35:21.928+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:35:21.927+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:35:22.074+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.835 seconds
[2022-12-12T14:35:32.645+0000] {processor.py:154} INFO - Started process (PID=1867) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:35:32.696+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:35:32.700+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:35:32.699+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:35:32.818+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:35:33.374+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:35:33.373+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:35:33.622+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:35:33.620+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:35:33.862+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.269 seconds
[2022-12-12T14:35:44.135+0000] {processor.py:154} INFO - Started process (PID=1877) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:35:44.138+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:35:44.142+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:35:44.141+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:35:44.238+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:35:44.650+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:35:44.649+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:35:44.836+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:35:44.835+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:35:45.035+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.915 seconds
[2022-12-12T14:35:55.598+0000] {processor.py:154} INFO - Started process (PID=1895) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:35:55.611+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:35:55.619+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:35:55.618+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:35:55.854+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:35:56.231+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:35:56.230+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:35:56.473+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:35:56.472+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:35:56.651+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.079 seconds
[2022-12-12T14:36:06.901+0000] {processor.py:154} INFO - Started process (PID=1903) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:36:06.931+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:36:06.935+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:36:06.934+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:36:07.197+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:36:07.581+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:36:07.580+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:36:07.831+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:36:07.830+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:36:08.064+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.196 seconds
[2022-12-12T14:36:18.469+0000] {processor.py:154} INFO - Started process (PID=1913) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:36:18.507+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:36:18.516+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:36:18.515+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:36:18.772+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:36:19.243+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:36:19.242+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:36:19.514+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:36:19.513+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:36:19.648+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.212 seconds
[2022-12-12T14:36:30.111+0000] {processor.py:154} INFO - Started process (PID=1923) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:36:30.126+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:36:30.133+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:36:30.129+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:36:30.391+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:36:30.920+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:36:30.919+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:36:31.329+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:36:31.328+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:36:31.553+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.509 seconds
[2022-12-12T14:36:42.171+0000] {processor.py:154} INFO - Started process (PID=1940) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:36:42.183+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:36:42.196+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:36:42.195+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:36:42.932+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:36:43.489+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:36:43.488+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:36:43.684+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:36:43.683+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:36:43.898+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.801 seconds
[2022-12-12T14:36:54.634+0000] {processor.py:154} INFO - Started process (PID=1947) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:36:54.689+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:36:54.693+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:36:54.692+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:36:54.867+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:36:55.412+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:36:55.411+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:36:55.580+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:36:55.579+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:36:55.813+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.218 seconds
[2022-12-12T14:37:06.304+0000] {processor.py:154} INFO - Started process (PID=1957) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:37:06.379+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:37:06.384+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:37:06.383+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:37:06.509+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:37:06.947+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:37:06.946+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:37:07.089+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:37:07.088+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:37:07.210+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.915 seconds
[2022-12-12T14:37:17.716+0000] {processor.py:154} INFO - Started process (PID=1973) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:37:17.737+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:37:17.741+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:37:17.740+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:37:17.844+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:37:19.248+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T14:37:29.933+0000] {processor.py:154} INFO - Started process (PID=1984) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:37:29.959+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:37:29.963+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:37:29.962+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:37:30.170+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:37:31.041+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:37:31.040+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:37:31.183+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:37:31.182+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:37:31.294+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.390 seconds
[2022-12-12T14:37:38.385+0000] {processor.py:154} INFO - Started process (PID=2000) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:37:38.437+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:37:38.441+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:37:38.440+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:37:38.689+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:37:38.986+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:37:38.984+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:37:39.178+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:37:39.177+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:37:39.607+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.257 seconds
[2022-12-12T14:37:50.763+0000] {processor.py:154} INFO - Started process (PID=2012) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:37:50.771+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:37:50.779+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:37:50.778+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:37:51.076+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:37:51.960+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:37:51.959+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:37:52.292+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:37:52.291+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:37:52.647+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.901 seconds
[2022-12-12T14:38:03.257+0000] {processor.py:154} INFO - Started process (PID=2030) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:38:03.291+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:38:03.297+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:38:03.296+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:38:04.039+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:38:04.633+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:38:04.631+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:38:04.774+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:38:04.773+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:38:04.916+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.725 seconds
[2022-12-12T14:38:15.621+0000] {processor.py:154} INFO - Started process (PID=2037) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:38:15.626+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:38:15.632+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:38:15.630+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:38:15.875+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:38:16.957+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:38:16.949+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:38:17.276+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:38:17.275+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:38:17.614+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.015 seconds
[2022-12-12T14:38:28.284+0000] {processor.py:154} INFO - Started process (PID=2050) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:38:28.320+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:38:28.324+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:38:28.323+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:38:28.512+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:38:28.691+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:38:28.690+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:38:28.831+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:38:28.830+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:38:28.997+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.753 seconds
[2022-12-12T14:38:39.779+0000] {processor.py:154} INFO - Started process (PID=2060) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:38:39.811+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:38:39.839+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:38:39.838+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:38:40.015+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:38:40.775+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:38:40.773+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:38:41.056+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:38:41.055+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:38:41.186+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.435 seconds
[2022-12-12T14:38:52.184+0000] {processor.py:154} INFO - Started process (PID=2078) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:38:52.235+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:38:52.259+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:38:52.258+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:38:52.830+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:38:54.593+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T14:39:05.002+0000] {processor.py:154} INFO - Started process (PID=2088) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:39:05.011+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:39:05.015+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:39:05.014+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:39:05.253+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:39:06.967+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T14:39:17.634+0000] {processor.py:154} INFO - Started process (PID=2098) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:39:17.651+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:39:17.664+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:39:17.663+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:39:17.937+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:39:18.210+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:39:18.208+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:39:18.410+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:39:18.409+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:39:18.604+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.012 seconds
[2022-12-12T14:39:29.275+0000] {processor.py:154} INFO - Started process (PID=2114) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:39:29.318+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:39:29.339+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:39:29.321+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:39:29.751+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:39:30.201+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:39:30.200+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:39:30.507+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:39:30.504+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:39:30.780+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.569 seconds
[2022-12-12T14:39:41.274+0000] {processor.py:154} INFO - Started process (PID=2126) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:39:41.303+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:39:41.323+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:39:41.322+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:39:41.660+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:39:42.705+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:39:42.704+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:39:43.011+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:39:43.010+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:39:43.356+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.124 seconds
[2022-12-12T14:39:53.855+0000] {processor.py:154} INFO - Started process (PID=2138) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:39:53.900+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:39:53.905+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:39:53.904+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:39:54.062+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:39:54.324+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:39:54.322+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:39:54.564+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:39:54.563+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:39:54.741+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.912 seconds
[2022-12-12T14:40:05.213+0000] {processor.py:154} INFO - Started process (PID=2146) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:40:05.251+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:40:05.259+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:40:05.258+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:40:05.399+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:40:05.733+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:40:05.732+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:40:05.940+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:40:05.939+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:40:06.412+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.221 seconds
[2022-12-12T14:40:16.877+0000] {processor.py:154} INFO - Started process (PID=2163) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:40:16.983+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:40:16.999+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:40:16.998+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:40:17.545+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:40:18.065+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:40:18.064+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:40:18.518+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:40:18.515+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:40:18.828+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.985 seconds
[2022-12-12T14:40:29.413+0000] {processor.py:154} INFO - Started process (PID=2174) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:40:29.428+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:40:29.432+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:40:29.431+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:40:29.667+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:40:30.235+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:40:30.229+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:40:30.596+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:40:30.595+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:40:30.862+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.500 seconds
[2022-12-12T14:40:41.202+0000] {processor.py:154} INFO - Started process (PID=2184) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:40:41.229+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:40:41.233+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:40:41.232+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:40:41.324+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:40:41.471+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:40:41.470+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:40:41.611+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:40:41.610+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:40:41.773+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.584 seconds
[2022-12-12T14:40:52.353+0000] {processor.py:154} INFO - Started process (PID=2193) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:40:52.363+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:40:52.368+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:40:52.367+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:40:52.548+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:40:53.483+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:40:53.481+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:40:53.738+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:40:53.737+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:40:53.908+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.581 seconds
[2022-12-12T14:41:04.458+0000] {processor.py:154} INFO - Started process (PID=2211) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:41:04.473+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:41:04.477+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:41:04.476+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:41:04.769+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:41:05.372+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:41:05.371+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:41:05.739+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:41:05.738+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:41:06.138+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.748 seconds
[2022-12-12T14:41:16.799+0000] {processor.py:154} INFO - Started process (PID=2223) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:41:16.838+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:41:16.844+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:41:16.843+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:41:17.001+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:41:17.411+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:41:17.410+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:41:17.520+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:41:17.520+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:41:17.647+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.863 seconds
[2022-12-12T14:41:28.275+0000] {processor.py:154} INFO - Started process (PID=2233) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:41:28.328+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:41:28.333+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:41:28.332+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:41:28.586+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:41:28.906+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:41:28.904+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:41:29.043+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:41:29.042+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:41:29.163+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.934 seconds
[2022-12-12T14:41:39.775+0000] {processor.py:154} INFO - Started process (PID=2249) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:41:39.788+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:41:39.829+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:41:39.800+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:41:40.322+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:41:41.149+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:41:41.148+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:41:41.519+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:41:41.509+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:41:42.044+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.347 seconds
[2022-12-12T14:41:52.903+0000] {processor.py:154} INFO - Started process (PID=2262) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:41:52.941+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:41:52.947+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:41:52.946+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:41:53.371+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:41:55.279+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:41:55.278+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:41:55.574+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:41:55.558+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:41:55.852+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.035 seconds
[2022-12-12T14:42:06.290+0000] {processor.py:154} INFO - Started process (PID=2272) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:42:06.336+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:42:06.340+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:42:06.339+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:42:06.573+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:42:07.953+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:42:07.952+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:42:08.250+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:42:08.245+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:42:08.574+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.321 seconds
[2022-12-12T14:42:19.149+0000] {processor.py:154} INFO - Started process (PID=2282) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:42:19.177+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:42:19.185+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:42:19.184+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:42:19.420+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:42:20.415+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:42:20.414+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:42:20.682+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:42:20.677+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:42:20.899+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.789 seconds
[2022-12-12T14:42:31.578+0000] {processor.py:154} INFO - Started process (PID=2300) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:42:31.600+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:42:31.606+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:42:31.605+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:42:31.737+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:42:32.443+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:42:32.442+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:42:32.596+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:42:32.595+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:42:32.823+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.303 seconds
[2022-12-12T14:42:43.312+0000] {processor.py:154} INFO - Started process (PID=2310) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:42:43.363+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:42:43.373+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:42:43.372+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:42:43.755+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:42:45.365+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:42:45.364+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:42:46.061+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:42:46.060+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:42:46.620+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.401 seconds
[2022-12-12T14:42:57.185+0000] {processor.py:154} INFO - Started process (PID=2320) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:42:57.249+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:42:57.267+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:42:57.266+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:42:57.636+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:42:58.193+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:42:58.192+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:42:58.496+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:42:58.495+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:42:58.811+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.711 seconds
[2022-12-12T14:43:09.675+0000] {processor.py:154} INFO - Started process (PID=2337) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:43:09.706+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:43:09.724+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:43:09.723+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:43:10.584+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:43:11.957+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:43:11.956+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:43:12.289+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:43:12.288+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:43:12.573+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.975 seconds
[2022-12-12T14:43:23.354+0000] {processor.py:154} INFO - Started process (PID=2348) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:43:23.408+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:43:23.413+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:43:23.412+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:43:23.594+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:43:23.970+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:43:23.969+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:43:24.128+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:43:24.126+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:43:24.301+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.985 seconds
[2022-12-12T14:43:34.691+0000] {processor.py:154} INFO - Started process (PID=2358) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:43:34.743+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:43:34.747+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:43:34.746+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:43:34.848+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:43:36.393+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T14:43:46.868+0000] {processor.py:154} INFO - Started process (PID=2368) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:43:46.901+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:43:46.908+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:43:46.907+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:43:47.020+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:43:47.171+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:43:47.170+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:43:47.286+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:43:47.285+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:43:47.436+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.588 seconds
[2022-12-12T14:43:57.859+0000] {processor.py:154} INFO - Started process (PID=2386) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:43:57.913+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:43:57.917+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:43:57.916+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:43:58.083+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:43:58.636+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:43:58.633+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:43:58.774+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:43:58.773+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:43:59.023+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.180 seconds
[2022-12-12T14:44:09.458+0000] {processor.py:154} INFO - Started process (PID=2396) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:44:09.504+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:44:09.508+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:44:09.507+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:44:09.715+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:44:09.991+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:44:09.990+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:44:10.187+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:44:10.185+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:44:10.391+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.954 seconds
[2022-12-12T14:44:20.705+0000] {processor.py:154} INFO - Started process (PID=2406) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:44:20.784+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:44:20.789+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:44:20.788+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:44:20.876+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:44:21.014+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:44:21.013+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:44:21.141+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:44:21.140+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:44:21.296+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.610 seconds
[2022-12-12T14:44:31.719+0000] {processor.py:154} INFO - Started process (PID=2416) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:44:31.777+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:44:31.784+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:44:31.780+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:44:31.930+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:44:33.153+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:44:33.151+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:44:33.291+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:44:33.291+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:44:33.430+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.761 seconds
[2022-12-12T14:44:43.887+0000] {processor.py:154} INFO - Started process (PID=2434) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:44:43.923+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:44:43.927+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:44:43.927+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:44:44.087+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:44:44.258+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:44:44.257+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:44:44.405+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:44:44.404+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:44:44.573+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.720 seconds
[2022-12-12T14:44:54.985+0000] {processor.py:154} INFO - Started process (PID=2444) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:44:55.005+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:44:55.009+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:44:55.008+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:44:55.118+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:44:55.255+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:44:55.254+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:44:55.372+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:44:55.371+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:44:55.496+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.529 seconds
[2022-12-12T14:45:06.412+0000] {processor.py:154} INFO - Started process (PID=2454) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:45:06.442+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:45:06.447+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:45:06.446+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:45:06.602+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:45:06.751+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:45:06.750+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:45:06.883+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:45:06.881+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:45:07.083+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.687 seconds
[2022-12-12T14:45:17.768+0000] {processor.py:154} INFO - Started process (PID=2471) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:45:17.799+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:45:17.811+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:45:17.810+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:45:18.088+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:45:18.323+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:45:18.321+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:45:18.455+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:45:18.454+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:45:18.735+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.009 seconds
[2022-12-12T14:45:29.090+0000] {processor.py:154} INFO - Started process (PID=2481) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:45:29.112+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:45:29.117+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:45:29.116+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:45:29.226+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:45:30.305+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:45:30.303+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:45:30.630+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:45:30.621+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:45:30.873+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.797 seconds
[2022-12-12T14:45:41.696+0000] {processor.py:154} INFO - Started process (PID=2491) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:45:41.743+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:45:41.749+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:45:41.748+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:45:41.937+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:45:42.683+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T14:45:52.965+0000] {processor.py:154} INFO - Started process (PID=2501) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:45:52.993+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:45:52.998+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:45:52.997+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:45:53.167+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:45:53.433+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:45:53.432+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:45:53.752+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:45:53.750+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:45:53.950+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.002 seconds
[2022-12-12T14:46:04.476+0000] {processor.py:154} INFO - Started process (PID=2519) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:46:04.513+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:46:04.524+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:46:04.523+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:46:04.700+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:46:04.983+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:46:04.982+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:46:05.349+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:46:05.348+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:46:05.755+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.314 seconds
[2022-12-12T14:46:16.312+0000] {processor.py:154} INFO - Started process (PID=2529) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:46:16.325+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:46:16.329+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:46:16.328+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:46:16.450+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:46:16.846+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:46:16.845+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:46:17.147+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:46:17.141+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:46:17.349+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.064 seconds
[2022-12-12T14:46:27.630+0000] {processor.py:154} INFO - Started process (PID=2539) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:46:27.661+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:46:27.665+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:46:27.664+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:46:27.799+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:46:27.940+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:46:27.939+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:46:28.053+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:46:28.052+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:46:28.200+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.609 seconds
[2022-12-12T14:46:38.490+0000] {processor.py:154} INFO - Started process (PID=2549) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:46:38.543+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:46:38.551+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:46:38.550+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:46:38.648+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:46:39.098+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:46:39.097+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:46:39.222+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:46:39.221+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:46:39.391+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.917 seconds
[2022-12-12T14:46:49.758+0000] {processor.py:154} INFO - Started process (PID=2567) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:46:49.812+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:46:49.817+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:46:49.816+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:46:50.104+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:46:50.324+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:46:50.323+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:46:50.487+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:46:50.481+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:46:50.745+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.002 seconds
[2022-12-12T14:47:01.016+0000] {processor.py:154} INFO - Started process (PID=2577) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:47:01.060+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:47:01.065+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:47:01.064+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:47:01.168+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:47:01.832+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:47:01.831+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:47:01.969+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:47:01.968+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:47:02.189+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.188 seconds
[2022-12-12T14:47:12.587+0000] {processor.py:154} INFO - Started process (PID=2587) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:47:12.639+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:47:12.645+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:47:12.643+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:47:12.740+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:47:13.016+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:47:13.015+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:47:13.165+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:47:13.164+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:47:13.313+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.748 seconds
[2022-12-12T14:47:24.302+0000] {processor.py:154} INFO - Started process (PID=2604) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:47:24.351+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:47:24.355+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:47:24.354+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:47:24.499+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:47:24.930+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:47:24.913+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:47:25.600+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:47:25.599+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:47:25.959+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.690 seconds
[2022-12-12T14:47:36.312+0000] {processor.py:154} INFO - Started process (PID=2615) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:47:36.362+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:47:36.367+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:47:36.366+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:47:36.452+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:47:37.104+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:47:37.103+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:47:37.236+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:47:37.235+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:47:37.406+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.110 seconds
[2022-12-12T14:47:47.759+0000] {processor.py:154} INFO - Started process (PID=2625) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:47:47.775+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:47:47.780+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:47:47.779+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:47:47.885+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:47:49.278+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:47:49.277+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:47:49.391+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:47:49.390+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:47:49.532+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.788 seconds
[2022-12-12T14:47:59.886+0000] {processor.py:154} INFO - Started process (PID=2635) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:47:59.941+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:47:59.946+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:47:59.945+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:48:00.045+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:48:01.127+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:48:01.126+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:48:01.443+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:48:01.441+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:48:01.596+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.724 seconds
[2022-12-12T14:48:11.971+0000] {processor.py:154} INFO - Started process (PID=2653) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:48:12.020+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:48:12.025+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:48:12.024+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:48:12.347+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:48:12.501+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:48:12.500+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:48:12.698+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:48:12.698+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:48:12.853+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.938 seconds
[2022-12-12T14:48:23.524+0000] {processor.py:154} INFO - Started process (PID=2663) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:48:23.568+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:48:23.576+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:48:23.575+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:48:23.743+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:48:25.140+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:48:25.139+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:48:25.250+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:48:25.249+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:48:25.409+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.909 seconds
[2022-12-12T14:48:35.777+0000] {processor.py:154} INFO - Started process (PID=2673) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:48:35.824+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:48:35.828+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:48:35.827+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:48:35.914+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:48:36.365+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:48:36.364+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:48:36.476+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:48:36.475+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:48:36.617+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.854 seconds
[2022-12-12T14:48:46.766+0000] {processor.py:154} INFO - Started process (PID=2683) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:48:46.794+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:48:46.799+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:48:46.798+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:48:46.884+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:48:47.025+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:48:47.024+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:48:47.137+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:48:47.136+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:48:47.274+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.522 seconds
[2022-12-12T14:48:58.079+0000] {processor.py:154} INFO - Started process (PID=2701) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:48:58.109+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:48:58.114+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:48:58.113+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:48:58.245+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:48:59.004+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T14:49:09.370+0000] {processor.py:154} INFO - Started process (PID=2711) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:49:09.397+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:49:09.405+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:49:09.404+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:49:09.491+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:49:09.633+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:49:09.632+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:49:09.784+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:49:09.783+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:49:09.890+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.534 seconds
[2022-12-12T14:49:20.188+0000] {processor.py:154} INFO - Started process (PID=2721) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:49:20.213+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:49:20.219+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:49:20.218+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:49:20.302+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:49:21.512+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:49:21.511+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:49:21.628+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:49:21.627+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:49:21.775+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.600 seconds
[2022-12-12T14:49:32.130+0000] {processor.py:154} INFO - Started process (PID=2738) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:49:32.156+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:49:32.160+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:49:32.159+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:49:32.351+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:49:32.506+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:49:32.505+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:49:32.659+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:49:32.658+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:49:32.815+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.703 seconds
[2022-12-12T14:49:43.477+0000] {processor.py:154} INFO - Started process (PID=2748) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:49:43.517+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:49:43.524+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:49:43.523+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:49:43.685+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:49:43.827+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:49:43.826+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:49:43.947+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:49:43.946+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:49:44.186+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.727 seconds
[2022-12-12T14:49:54.842+0000] {processor.py:154} INFO - Started process (PID=2758) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:49:54.909+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:49:54.915+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:49:54.914+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:49:54.999+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:49:55.163+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:49:55.162+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:49:55.302+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:49:55.301+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:49:55.424+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.649 seconds
[2022-12-12T14:50:05.748+0000] {processor.py:154} INFO - Started process (PID=2768) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:50:05.790+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:50:05.795+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:50:05.794+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:50:05.878+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:50:06.012+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:50:06.011+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:50:06.288+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:50:06.287+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:50:06.672+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.936 seconds
[2022-12-12T14:50:17.265+0000] {processor.py:154} INFO - Started process (PID=2786) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:50:17.316+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:50:17.321+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:50:17.320+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:50:17.427+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:50:17.584+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:50:17.583+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:50:17.745+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:50:17.744+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:50:18.002+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.755 seconds
[2022-12-12T14:50:28.876+0000] {processor.py:154} INFO - Started process (PID=2796) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:50:28.925+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:50:28.929+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:50:28.928+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:50:29.036+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:50:29.203+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:50:29.202+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:50:29.381+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:50:29.380+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:50:29.534+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.681 seconds
[2022-12-12T14:50:39.923+0000] {processor.py:154} INFO - Started process (PID=2806) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:50:39.950+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:50:39.954+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:50:39.953+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:50:40.060+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:50:40.565+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:50:40.564+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:50:40.744+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:50:40.743+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:50:40.993+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.077 seconds
[2022-12-12T14:50:51.531+0000] {processor.py:154} INFO - Started process (PID=2816) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:50:51.573+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:50:51.578+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:50:51.577+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:50:51.733+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:50:52.175+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:50:52.174+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:50:52.551+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:50:52.541+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:50:52.845+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.329 seconds
[2022-12-12T14:51:03.292+0000] {processor.py:154} INFO - Started process (PID=2834) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:51:03.319+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:51:03.324+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:51:03.323+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:51:03.409+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:51:03.546+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:51:03.545+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:51:03.672+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:51:03.671+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:51:03.923+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.645 seconds
[2022-12-12T14:51:14.510+0000] {processor.py:154} INFO - Started process (PID=2844) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:51:14.523+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:51:14.527+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:51:14.526+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:51:14.626+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:51:15.342+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:51:15.341+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:51:15.456+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:51:15.455+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:51:15.622+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.126 seconds
[2022-12-12T14:51:26.017+0000] {processor.py:154} INFO - Started process (PID=2854) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:51:26.069+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:51:26.073+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:51:26.072+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:51:26.163+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:51:27.657+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:51:27.656+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:51:27.989+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:51:27.988+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:51:28.179+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.177 seconds
[2022-12-12T14:51:38.602+0000] {processor.py:154} INFO - Started process (PID=2871) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:51:38.622+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:51:38.627+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:51:38.626+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:51:38.868+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:51:40.095+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T14:51:50.281+0000] {processor.py:154} INFO - Started process (PID=2881) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:51:50.327+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:51:50.331+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:51:50.330+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:51:50.419+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:51:50.552+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:51:50.552+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:51:50.709+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:51:50.708+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:51:50.875+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.608 seconds
[2022-12-12T14:52:01.240+0000] {processor.py:154} INFO - Started process (PID=2891) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:52:01.253+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:52:01.257+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:52:01.256+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:52:01.345+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:52:01.479+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:52:01.478+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:52:01.621+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:52:01.620+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:52:01.770+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.544 seconds
[2022-12-12T14:52:12.091+0000] {processor.py:154} INFO - Started process (PID=2901) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:52:12.112+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:52:12.116+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:52:12.115+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:52:12.198+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:52:12.332+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:52:12.331+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:52:12.446+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:52:12.445+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:52:12.607+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.532 seconds
[2022-12-12T14:52:22.929+0000] {processor.py:154} INFO - Started process (PID=2919) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:52:22.950+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:52:22.954+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:52:22.953+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:52:23.061+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:52:23.404+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:52:23.402+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:52:23.762+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:52:23.761+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:52:23.917+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.003 seconds
[2022-12-12T14:52:34.232+0000] {processor.py:154} INFO - Started process (PID=2929) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:52:34.254+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:52:34.258+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:52:34.257+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:52:34.349+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:52:35.832+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:52:35.831+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:52:35.948+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:52:35.947+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:52:36.095+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.880 seconds
[2022-12-12T14:52:46.335+0000] {processor.py:154} INFO - Started process (PID=2939) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:52:46.387+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:52:46.391+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:52:46.390+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:52:46.476+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:52:46.625+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:52:46.624+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:52:46.741+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:52:46.740+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:52:46.888+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.567 seconds
[2022-12-12T14:52:57.030+0000] {processor.py:154} INFO - Started process (PID=2956) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:52:57.059+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:52:57.064+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:52:57.063+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:52:57.171+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:52:58.716+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:52:58.715+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:52:58.864+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:52:58.863+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:52:59.027+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.015 seconds
[2022-12-12T14:53:09.743+0000] {processor.py:154} INFO - Started process (PID=2967) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:53:09.789+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:53:09.793+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:53:09.792+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:53:09.885+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:53:10.088+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:53:10.087+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:53:10.205+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:53:10.205+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:53:10.333+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.604 seconds
[2022-12-12T14:53:20.585+0000] {processor.py:154} INFO - Started process (PID=2977) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:53:20.638+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:53:20.643+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:53:20.641+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:53:20.732+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:53:21.299+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T14:53:31.665+0000] {processor.py:154} INFO - Started process (PID=2987) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:53:31.691+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:53:31.695+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:53:31.694+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:53:31.779+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:53:31.911+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:53:31.910+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:53:32.027+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:53:32.026+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:53:32.164+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.513 seconds
[2022-12-12T14:53:42.340+0000] {processor.py:154} INFO - Started process (PID=3004) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:53:42.362+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:53:42.367+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:53:42.366+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:53:42.487+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:53:43.281+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:53:43.280+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:53:43.749+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:53:43.748+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:53:44.042+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.717 seconds
[2022-12-12T14:53:54.892+0000] {processor.py:154} INFO - Started process (PID=3014) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:53:54.926+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:53:54.929+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:53:54.928+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:53:55.035+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:53:55.170+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:53:55.169+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:53:55.281+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:53:55.280+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:53:55.401+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.533 seconds
[2022-12-12T14:54:05.693+0000] {processor.py:154} INFO - Started process (PID=3024) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:54:05.719+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:54:05.723+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:54:05.722+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:54:05.806+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:54:06.182+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:54:06.181+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:54:06.296+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:54:06.295+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:54:06.427+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.747 seconds
[2022-12-12T14:54:16.733+0000] {processor.py:154} INFO - Started process (PID=3041) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:54:16.789+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:54:16.798+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:54:16.797+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:54:16.903+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:54:17.081+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:54:17.080+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:54:17.208+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:54:17.207+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:54:17.419+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.701 seconds
[2022-12-12T14:54:27.697+0000] {processor.py:154} INFO - Started process (PID=3052) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:54:27.737+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:54:27.741+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:54:27.740+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:54:27.842+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:54:28.419+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:54:28.417+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:54:28.531+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:54:28.530+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:54:28.673+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.991 seconds
[2022-12-12T14:54:39.081+0000] {processor.py:154} INFO - Started process (PID=3062) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:54:39.095+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:54:39.099+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:54:39.098+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:54:39.346+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:54:39.995+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:54:39.994+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:54:40.199+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:54:40.195+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:54:40.355+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.322 seconds
[2022-12-12T14:54:50.704+0000] {processor.py:154} INFO - Started process (PID=3072) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:54:50.873+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:54:50.880+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:54:50.879+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:54:51.007+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:54:51.564+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:54:51.563+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:54:51.717+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:54:51.716+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:54:51.835+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.183 seconds
[2022-12-12T14:55:02.158+0000] {processor.py:154} INFO - Started process (PID=3090) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:55:02.188+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:55:02.193+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:55:02.192+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:55:02.300+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:55:02.471+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:55:02.471+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:55:02.605+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:55:02.604+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:55:02.797+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.654 seconds
[2022-12-12T14:55:13.019+0000] {processor.py:154} INFO - Started process (PID=3100) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:55:13.047+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:55:13.052+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:55:13.051+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:55:13.139+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:55:13.913+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T14:55:24.369+0000] {processor.py:154} INFO - Started process (PID=3110) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:55:24.423+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:55:24.430+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:55:24.429+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:55:24.670+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:55:24.943+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:55:24.942+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:55:25.120+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:55:25.119+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:55:25.297+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.957 seconds
[2022-12-12T14:55:35.625+0000] {processor.py:154} INFO - Started process (PID=3120) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:55:35.670+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:55:35.674+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:55:35.673+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:55:35.758+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:55:36.300+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:55:36.299+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:55:36.484+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:55:36.483+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:55:36.664+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.054 seconds
[2022-12-12T14:55:46.888+0000] {processor.py:154} INFO - Started process (PID=3138) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:55:46.939+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:55:46.944+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:55:46.943+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:55:47.029+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:55:47.173+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:55:47.172+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:55:47.304+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:55:47.303+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:55:47.430+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.556 seconds
[2022-12-12T14:55:57.695+0000] {processor.py:154} INFO - Started process (PID=3148) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:55:57.722+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:55:57.726+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:55:57.725+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:55:57.837+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:55:57.997+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:55:57.996+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:55:58.197+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:55:58.196+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:55:58.458+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.797 seconds
[2022-12-12T14:56:08.735+0000] {processor.py:154} INFO - Started process (PID=3158) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:56:08.791+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:56:08.796+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:56:08.795+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:56:08.884+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:56:09.038+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:56:09.037+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:56:09.149+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:56:09.148+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:56:09.299+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.576 seconds
[2022-12-12T14:56:19.896+0000] {processor.py:154} INFO - Started process (PID=3175) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:56:19.954+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:56:19.958+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:56:19.957+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:56:20.171+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:56:21.241+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:56:21.240+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:56:21.524+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:56:21.523+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:56:21.764+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.902 seconds
[2022-12-12T14:56:32.176+0000] {processor.py:154} INFO - Started process (PID=3186) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:56:32.215+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:56:32.219+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:56:32.218+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:56:32.310+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:56:32.468+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:56:32.467+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:56:32.752+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:56:32.751+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:56:32.884+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.726 seconds
[2022-12-12T14:56:43.163+0000] {processor.py:154} INFO - Started process (PID=3196) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:56:43.189+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:56:43.193+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:56:43.192+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:56:43.302+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:56:43.507+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:56:43.506+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:56:43.626+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:56:43.625+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:56:43.733+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.584 seconds
[2022-12-12T14:56:54.160+0000] {processor.py:154} INFO - Started process (PID=3206) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:56:54.168+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:56:54.172+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:56:54.171+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:56:54.475+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:56:55.160+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:56:55.159+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:56:55.324+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:56:55.323+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:56:55.491+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.364 seconds
[2022-12-12T14:57:06.100+0000] {processor.py:154} INFO - Started process (PID=3224) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:57:06.150+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:57:06.163+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:57:06.162+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:57:06.698+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:57:07.041+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:57:07.040+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:57:07.301+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:57:07.299+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:57:07.597+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.536 seconds
[2022-12-12T14:57:18.019+0000] {processor.py:154} INFO - Started process (PID=3234) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:57:18.045+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:57:18.053+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:57:18.052+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:57:18.196+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:57:18.392+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:57:18.390+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:57:18.526+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:57:18.525+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:57:18.654+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.669 seconds
[2022-12-12T14:57:28.918+0000] {processor.py:154} INFO - Started process (PID=3244) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:57:28.932+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:57:28.941+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:57:28.935+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:57:29.185+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:57:30.958+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:57:30.957+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:57:31.073+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:57:31.072+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:57:31.201+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.311 seconds
[2022-12-12T14:57:41.524+0000] {processor.py:154} INFO - Started process (PID=3254) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:57:41.552+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:57:41.557+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:57:41.556+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:57:41.690+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:57:42.515+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T14:57:53.459+0000] {processor.py:154} INFO - Started process (PID=3272) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:57:53.547+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:57:53.567+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:57:53.566+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:57:53.867+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:57:54.298+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:57:54.297+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:57:54.728+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:57:54.727+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:57:54.907+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.563 seconds
[2022-12-12T14:58:05.164+0000] {processor.py:154} INFO - Started process (PID=3282) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:58:05.226+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:58:05.235+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:58:05.234+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:58:05.327+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:58:06.772+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:58:06.771+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:58:06.922+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:58:06.921+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:58:07.069+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.942 seconds
[2022-12-12T14:58:17.376+0000] {processor.py:154} INFO - Started process (PID=3292) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:58:17.421+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:58:17.426+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:58:17.424+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:58:17.511+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:58:17.651+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:58:17.650+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:58:17.768+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:58:17.767+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:58:17.927+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.565 seconds
[2022-12-12T14:58:28.940+0000] {processor.py:154} INFO - Started process (PID=3308) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:58:28.988+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:58:28.996+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:58:28.996+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:58:29.227+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:58:29.506+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:58:29.505+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:58:29.645+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:58:29.644+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:58:29.825+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.900 seconds
[2022-12-12T14:58:40.540+0000] {processor.py:154} INFO - Started process (PID=3319) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:58:40.549+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:58:40.553+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:58:40.552+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:58:40.741+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:58:42.348+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:58:42.347+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:58:42.498+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:58:42.497+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:58:42.647+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.166 seconds
[2022-12-12T14:58:52.997+0000] {processor.py:154} INFO - Started process (PID=3329) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:58:53.023+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:58:53.029+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:58:53.028+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:58:53.109+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:58:53.289+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:58:53.288+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:58:53.440+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:58:53.439+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:58:53.648+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.665 seconds
[2022-12-12T14:59:04.085+0000] {processor.py:154} INFO - Started process (PID=3339) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:59:04.126+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:59:04.131+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:59:04.130+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:59:04.214+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:59:04.441+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:59:04.440+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:59:04.566+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:59:04.554+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:59:04.727+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.656 seconds
[2022-12-12T14:59:15.169+0000] {processor.py:154} INFO - Started process (PID=3357) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:59:15.197+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:59:15.204+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:59:15.203+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:59:15.303+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:59:16.025+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T14:59:26.602+0000] {processor.py:154} INFO - Started process (PID=3367) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:59:26.663+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:59:26.669+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:59:26.668+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:59:26.748+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:59:27.914+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:59:27.913+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:59:28.049+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:59:28.048+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:59:28.183+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.628 seconds
[2022-12-12T14:59:38.550+0000] {processor.py:154} INFO - Started process (PID=3377) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:59:38.578+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:59:38.591+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:59:38.590+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:59:38.741+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:59:38.913+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:59:38.912+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:59:39.059+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:59:39.058+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:59:39.196+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.665 seconds
[2022-12-12T14:59:49.491+0000] {processor.py:154} INFO - Started process (PID=3387) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:59:49.556+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T14:59:49.561+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:59:49.560+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:59:49.648+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T14:59:50.324+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:59:50.323+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T14:59:50.438+0000] {logging_mixin.py:137} INFO - [2022-12-12T14:59:50.437+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T14:59:50.553+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.076 seconds
[2022-12-12T15:00:00.815+0000] {processor.py:154} INFO - Started process (PID=3406) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:00:00.858+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:00:00.862+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:00:00.861+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:00:01.110+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:00:02.022+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:00:02.021+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:00:02.311+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:00:02.308+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:00:02.556+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.790 seconds
[2022-12-12T15:00:12.992+0000] {processor.py:154} INFO - Started process (PID=3416) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:00:13.035+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:00:13.040+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:00:13.039+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:00:13.140+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:00:13.722+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:00:13.721+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:00:13.855+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:00:13.854+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:00:14.006+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.037 seconds
[2022-12-12T15:00:24.477+0000] {processor.py:154} INFO - Started process (PID=3426) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:00:24.494+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:00:24.511+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:00:24.497+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:00:24.693+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:00:24.865+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:00:24.864+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:00:24.998+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:00:24.997+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:00:25.153+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.702 seconds
[2022-12-12T15:00:35.966+0000] {processor.py:154} INFO - Started process (PID=3443) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:00:35.995+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:00:36.000+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:00:35.999+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:00:36.178+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:00:36.979+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T15:00:47.255+0000] {processor.py:154} INFO - Started process (PID=3454) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:00:47.277+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:00:47.288+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:00:47.285+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:00:47.496+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:00:47.964+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:00:47.963+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:00:48.073+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:00:48.072+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:00:48.183+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.953 seconds
[2022-12-12T15:00:58.572+0000] {processor.py:154} INFO - Started process (PID=3464) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:00:58.622+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:00:58.627+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:00:58.626+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:00:58.759+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:00:59.807+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T15:01:10.195+0000] {processor.py:154} INFO - Started process (PID=3474) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:01:10.223+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:01:10.228+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:01:10.227+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:01:10.343+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:01:10.564+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:01:10.563+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:01:10.846+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:01:10.841+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:01:11.162+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.020 seconds
[2022-12-12T15:01:21.809+0000] {processor.py:154} INFO - Started process (PID=3491) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:01:21.838+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:01:21.843+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:01:21.842+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:01:22.069+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:01:22.579+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:01:22.578+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:01:22.751+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:01:22.750+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:01:22.941+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.191 seconds
[2022-12-12T15:01:33.394+0000] {processor.py:154} INFO - Started process (PID=3501) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:01:33.438+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:01:33.442+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:01:33.441+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:01:33.530+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:01:34.151+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:01:34.150+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:01:34.346+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:01:34.345+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:01:34.473+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.097 seconds
[2022-12-12T15:01:44.853+0000] {processor.py:154} INFO - Started process (PID=3511) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:01:44.895+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:01:44.899+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:01:44.898+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:01:45.003+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:01:45.163+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:01:45.162+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:01:45.327+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:01:45.326+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:01:45.659+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.840 seconds
[2022-12-12T15:01:56.189+0000] {processor.py:154} INFO - Started process (PID=3521) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:01:56.252+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:01:56.256+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:01:56.255+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:01:56.357+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:01:57.500+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T15:02:08.251+0000] {processor.py:154} INFO - Started process (PID=3539) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:02:08.271+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:02:08.277+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:02:08.276+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:02:08.415+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:02:08.801+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:02:08.800+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:02:08.919+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:02:08.918+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:02:09.029+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.830 seconds
[2022-12-12T15:02:19.615+0000] {processor.py:154} INFO - Started process (PID=3549) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:02:19.671+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:02:19.676+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:02:19.675+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:02:19.813+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:02:20.591+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:02:20.590+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:02:20.716+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:02:20.715+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:02:20.863+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.264 seconds
[2022-12-12T15:02:31.256+0000] {processor.py:154} INFO - Started process (PID=3559) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:02:31.283+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:02:31.288+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:02:31.287+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:02:31.377+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:02:32.100+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:02:32.098+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:02:32.365+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:02:32.364+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:02:32.531+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.297 seconds
[2022-12-12T15:02:43.308+0000] {processor.py:154} INFO - Started process (PID=3579) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:02:43.341+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:02:43.345+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:02:43.344+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:02:43.607+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:02:44.537+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:02:44.536+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:02:44.752+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:02:44.751+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:02:44.970+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.689 seconds
[2022-12-12T15:02:55.334+0000] {processor.py:154} INFO - Started process (PID=3589) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:02:55.360+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:02:55.365+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:02:55.364+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:02:55.447+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:02:56.819+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T15:03:07.232+0000] {processor.py:154} INFO - Started process (PID=3599) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:03:07.282+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:03:07.287+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:03:07.285+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:03:07.398+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:03:07.985+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:03:07.984+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:03:08.145+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:03:08.144+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:03:08.356+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.144 seconds
[2022-12-12T15:03:18.745+0000] {processor.py:154} INFO - Started process (PID=3609) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:03:18.794+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:03:18.802+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:03:18.797+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:03:18.937+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:03:20.041+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T15:03:30.946+0000] {processor.py:154} INFO - Started process (PID=3626) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:03:30.972+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:03:30.979+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:03:30.977+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:03:31.151+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:03:31.312+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:03:31.312+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:03:31.443+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:03:31.442+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:03:31.605+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.725 seconds
[2022-12-12T15:03:42.436+0000] {processor.py:154} INFO - Started process (PID=3636) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:03:42.485+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:03:42.489+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:03:42.488+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:03:42.577+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:03:43.656+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T15:03:54.028+0000] {processor.py:154} INFO - Started process (PID=3646) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:03:54.075+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:03:54.079+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:03:54.078+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:03:54.190+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:03:54.519+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:03:54.515+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:03:54.700+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:03:54.689+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:03:54.860+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.869 seconds
[2022-12-12T15:04:05.313+0000] {processor.py:154} INFO - Started process (PID=3661) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:04:05.334+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:04:05.340+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:04:05.337+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:04:05.530+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:04:06.892+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T15:04:17.748+0000] {processor.py:154} INFO - Started process (PID=3673) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:04:17.797+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:04:17.801+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:04:17.800+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:04:17.890+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:04:18.201+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:04:18.200+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:04:18.339+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:04:18.338+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:04:18.541+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.810 seconds
[2022-12-12T15:04:28.690+0000] {processor.py:154} INFO - Started process (PID=3683) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:04:28.712+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:04:28.718+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:04:28.717+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:04:28.815+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:04:29.056+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:04:29.056+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:04:29.169+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:04:29.168+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:04:29.306+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.633 seconds
[2022-12-12T15:04:39.610+0000] {processor.py:154} INFO - Started process (PID=3693) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:04:39.638+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:04:39.642+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:04:39.641+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:04:39.727+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:04:40.232+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:04:40.231+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:04:40.345+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:04:40.344+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:04:40.488+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.893 seconds
[2022-12-12T15:04:51.024+0000] {processor.py:154} INFO - Started process (PID=3711) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:04:51.095+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:04:51.107+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:04:51.106+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:04:51.300+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:04:51.604+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:04:51.603+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:04:51.904+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:04:51.899+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:04:52.207+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.227 seconds
[2022-12-12T15:05:02.985+0000] {processor.py:154} INFO - Started process (PID=3721) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:05:03.039+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:05:03.043+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:05:03.042+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:05:03.179+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:05:03.390+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:05:03.389+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:05:03.584+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:05:03.583+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:05:03.844+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.877 seconds
[2022-12-12T15:05:14.120+0000] {processor.py:154} INFO - Started process (PID=3731) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:05:14.144+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:05:14.148+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:05:14.147+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:05:14.236+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:05:14.364+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:05:14.363+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:05:14.480+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:05:14.479+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:05:14.617+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.510 seconds
[2022-12-12T15:05:24.933+0000] {processor.py:154} INFO - Started process (PID=3741) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:05:25.004+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:05:25.010+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:05:25.008+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:05:25.132+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:05:25.827+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:05:25.827+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:05:25.946+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:05:25.945+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:05:26.060+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.152 seconds
[2022-12-12T15:05:36.626+0000] {processor.py:154} INFO - Started process (PID=3759) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:05:36.671+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:05:36.675+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:05:36.674+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:05:37.035+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:05:37.499+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:05:37.498+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:05:37.703+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:05:37.697+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:05:38.480+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.945 seconds
[2022-12-12T15:05:49.301+0000] {processor.py:154} INFO - Started process (PID=3769) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:05:49.327+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:05:49.333+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:05:49.332+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:05:49.463+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:05:50.471+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T15:06:00.858+0000] {processor.py:154} INFO - Started process (PID=3779) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:06:00.881+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:06:00.884+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:06:00.884+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:06:00.970+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:06:01.570+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T15:06:12.601+0000] {processor.py:154} INFO - Started process (PID=3789) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:06:12.627+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:06:12.633+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:06:12.630+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:06:12.717+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:06:12.843+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:06:12.842+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:06:12.961+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:06:12.960+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:06:13.096+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.507 seconds
[2022-12-12T15:06:23.519+0000] {processor.py:154} INFO - Started process (PID=3807) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:06:23.541+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:06:23.545+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:06:23.544+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:06:23.762+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:06:24.202+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:06:24.201+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:06:24.374+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:06:24.373+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:06:24.563+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.076 seconds
[2022-12-12T15:06:34.772+0000] {processor.py:154} INFO - Started process (PID=3817) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:06:34.800+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:06:34.804+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:06:34.803+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:06:34.894+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:06:35.342+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:06:35.341+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:06:35.468+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:06:35.467+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:06:35.602+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.844 seconds
[2022-12-12T15:06:45.999+0000] {processor.py:154} INFO - Started process (PID=3827) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:06:46.012+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:06:46.023+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:06:46.022+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:06:46.148+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:06:46.536+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:06:46.535+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:06:46.870+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:06:46.869+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:06:47.079+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.096 seconds
[2022-12-12T15:06:57.441+0000] {processor.py:154} INFO - Started process (PID=3844) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:06:57.486+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:06:57.494+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:06:57.489+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:06:57.842+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:06:58.271+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:06:58.265+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:06:58.616+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:06:58.615+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:06:58.757+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.402 seconds
[2022-12-12T15:07:09.130+0000] {processor.py:154} INFO - Started process (PID=3855) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:07:09.182+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:07:09.188+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:07:09.187+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:07:09.292+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:07:09.450+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:07:09.448+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:07:09.575+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:07:09.574+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:07:09.735+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.618 seconds
[2022-12-12T15:07:20.123+0000] {processor.py:154} INFO - Started process (PID=3865) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:07:20.169+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:07:20.172+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:07:20.171+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:07:20.411+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:07:21.671+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T15:07:32.338+0000] {processor.py:154} INFO - Started process (PID=3875) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:07:32.351+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:07:32.355+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:07:32.354+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:07:32.465+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:07:33.324+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T15:07:43.964+0000] {processor.py:154} INFO - Started process (PID=3893) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:07:43.998+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:07:44.012+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:07:44.001+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:07:44.299+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:07:44.488+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:07:44.488+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:07:44.782+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:07:44.773+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:07:45.079+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.159 seconds
[2022-12-12T15:07:55.699+0000] {processor.py:154} INFO - Started process (PID=3903) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:07:55.754+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:07:55.761+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:07:55.759+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:07:55.915+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:07:56.637+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:07:56.635+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:07:56.809+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:07:56.808+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:07:56.936+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.284 seconds
[2022-12-12T15:08:07.254+0000] {processor.py:154} INFO - Started process (PID=3913) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:08:07.290+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:08:07.294+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:08:07.293+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:08:07.397+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:08:07.544+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:08:07.543+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:08:07.658+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:08:07.657+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:08:07.825+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.593 seconds
[2022-12-12T15:08:18.099+0000] {processor.py:154} INFO - Started process (PID=3923) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:08:18.127+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:08:18.133+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:08:18.132+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:08:18.276+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:08:19.216+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T15:08:29.775+0000] {processor.py:154} INFO - Started process (PID=3941) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:08:29.796+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:08:29.833+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:08:29.821+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:08:30.212+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:08:30.709+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:08:30.708+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:08:30.989+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:08:30.988+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:08:31.175+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.498 seconds
[2022-12-12T15:08:41.601+0000] {processor.py:154} INFO - Started process (PID=3951) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:08:41.627+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:08:41.640+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:08:41.639+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:08:41.967+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:08:42.135+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:08:42.134+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:08:42.252+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:08:42.251+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:08:42.364+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.798 seconds
[2022-12-12T15:08:52.745+0000] {processor.py:154} INFO - Started process (PID=3961) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:08:52.797+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:08:52.809+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:08:52.807+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:08:53.220+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:08:53.438+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:08:53.436+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:08:53.648+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:08:53.647+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:08:53.891+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.198 seconds
[2022-12-12T15:09:04.440+0000] {processor.py:154} INFO - Started process (PID=3971) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:09:04.494+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:09:04.500+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:09:04.499+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:09:04.645+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:09:05.011+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T15:09:15.875+0000] {processor.py:154} INFO - Started process (PID=3989) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:09:15.880+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:09:15.895+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:09:15.888+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:09:16.071+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:09:16.533+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:09:16.532+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:09:16.795+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:09:16.789+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:09:17.118+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.301 seconds
[2022-12-12T15:09:27.737+0000] {processor.py:154} INFO - Started process (PID=3999) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:09:27.800+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:09:27.805+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:09:27.804+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:09:27.887+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:09:28.401+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:09:28.400+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:09:28.519+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:09:28.518+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:09:28.658+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.938 seconds
[2022-12-12T15:09:38.994+0000] {processor.py:154} INFO - Started process (PID=4009) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:09:39.022+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:09:39.028+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:09:39.027+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:09:39.232+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:09:39.484+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:09:39.483+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:09:39.637+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:09:39.636+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:09:39.814+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.842 seconds
[2022-12-12T15:09:50.313+0000] {processor.py:154} INFO - Started process (PID=4024) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:09:50.340+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:09:50.344+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:09:50.344+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:09:50.488+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:09:50.944+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:09:50.944+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:09:51.314+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:09:51.303+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:09:51.550+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.294 seconds
[2022-12-12T15:10:01.968+0000] {processor.py:154} INFO - Started process (PID=4034) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:10:02.019+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:10:02.027+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:10:02.026+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:10:02.621+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:10:03.236+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:10:03.235+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:10:03.601+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:10:03.600+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:10:04.021+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.113 seconds
[2022-12-12T15:10:14.819+0000] {processor.py:154} INFO - Started process (PID=4044) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:10:14.841+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:10:14.845+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:10:14.844+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:10:15.374+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:10:15.778+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:10:15.776+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:10:16.286+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:10:16.261+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:10:16.667+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.891 seconds
[2022-12-12T15:10:27.296+0000] {processor.py:154} INFO - Started process (PID=4054) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:10:27.322+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:10:27.327+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:10:27.326+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:10:27.475+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:10:28.683+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T15:10:39.613+0000] {processor.py:154} INFO - Started process (PID=4071) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:10:39.688+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:10:39.692+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:10:39.691+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:10:40.251+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:10:41.031+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:10:41.030+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:10:41.944+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:10:41.943+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:10:42.551+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.075 seconds
[2022-12-12T15:10:53.565+0000] {processor.py:154} INFO - Started process (PID=4082) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:10:53.634+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:10:53.642+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:10:53.637+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:10:54.079+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:10:54.981+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:10:54.980+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:10:55.385+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:10:55.384+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:10:55.771+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.273 seconds
[2022-12-12T15:11:06.293+0000] {processor.py:154} INFO - Started process (PID=4092) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:11:06.301+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:11:06.323+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:11:06.322+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:11:06.717+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:11:07.964+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T15:11:18.709+0000] {processor.py:154} INFO - Started process (PID=4102) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:11:18.731+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:11:18.736+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:11:18.735+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:11:19.077+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:11:20.497+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T15:11:31.182+0000] {processor.py:154} INFO - Started process (PID=4112) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:11:31.214+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:11:31.231+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:11:31.217+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:11:31.647+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:11:32.365+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:11:32.364+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:11:32.858+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:11:32.843+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:11:33.994+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.843 seconds
[2022-12-12T15:11:44.908+0000] {processor.py:154} INFO - Started process (PID=4127) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:11:44.924+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:11:44.928+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:11:44.927+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:11:45.187+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:11:45.470+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:11:45.469+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:11:46.258+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:11:46.237+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:11:46.931+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.108 seconds
[2022-12-12T15:11:57.418+0000] {processor.py:154} INFO - Started process (PID=4137) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:11:57.440+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:11:57.445+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:11:57.444+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:11:57.548+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:11:57.893+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:11:57.892+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:11:58.032+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:11:58.031+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:11:58.288+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.889 seconds
[2022-12-12T15:12:08.785+0000] {processor.py:154} INFO - Started process (PID=4147) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:12:08.896+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:12:08.915+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:12:08.914+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:12:09.038+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:12:09.628+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:12:09.627+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:12:09.871+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:12:09.850+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:12:10.102+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.341 seconds
[2022-12-12T15:12:21.295+0000] {processor.py:154} INFO - Started process (PID=4157) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:12:21.343+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:12:21.355+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:12:21.354+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:12:21.634+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:12:22.621+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:12:22.620+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:12:22.955+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:12:22.953+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:12:23.093+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.831 seconds
[2022-12-12T15:12:33.619+0000] {processor.py:154} INFO - Started process (PID=4175) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:12:33.664+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:12:33.668+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:12:33.667+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:12:33.859+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:12:34.087+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:12:34.086+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:12:34.239+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:12:34.239+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:12:34.400+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.794 seconds
[2022-12-12T15:12:45.192+0000] {processor.py:154} INFO - Started process (PID=4185) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:12:45.287+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:12:45.306+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:12:45.294+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:12:45.444+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:12:46.059+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:12:46.058+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:12:46.223+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:12:46.222+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:12:46.363+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.240 seconds
[2022-12-12T15:12:56.651+0000] {processor.py:154} INFO - Started process (PID=4195) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:12:56.678+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:12:56.683+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:12:56.682+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:12:56.797+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:12:56.947+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:12:56.946+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:12:57.090+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:12:57.088+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:12:57.251+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.616 seconds
[2022-12-12T15:13:07.550+0000] {processor.py:154} INFO - Started process (PID=4213) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:13:07.573+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:13:07.579+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:13:07.577+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:13:07.723+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:13:09.572+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T15:13:20.217+0000] {processor.py:154} INFO - Started process (PID=4223) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:13:20.249+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:13:20.253+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:13:20.252+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:13:20.343+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:13:21.158+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T15:13:31.607+0000] {processor.py:154} INFO - Started process (PID=4236) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:13:31.630+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:13:31.635+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:13:31.634+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:13:31.748+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:13:32.922+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:13:32.921+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:13:33.034+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:13:33.033+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:13:33.165+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.574 seconds
[2022-12-12T15:13:43.601+0000] {processor.py:154} INFO - Started process (PID=4253) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:13:43.617+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:13:43.621+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:13:43.620+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:13:43.736+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:13:45.189+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T15:13:55.916+0000] {processor.py:154} INFO - Started process (PID=4266) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:13:55.919+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:13:55.923+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:13:55.922+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:13:56.006+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:13:56.401+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:13:56.400+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:13:56.517+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:13:56.517+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:13:56.662+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.762 seconds
[2022-12-12T15:14:06.939+0000] {processor.py:154} INFO - Started process (PID=4276) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:14:06.942+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:14:06.946+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:14:06.945+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:14:07.035+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:14:07.165+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:14:07.164+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:14:07.291+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:14:07.290+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:14:07.461+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.537 seconds
[2022-12-12T15:14:17.720+0000] {processor.py:154} INFO - Started process (PID=4286) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:14:17.747+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:14:17.751+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:14:17.750+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:14:17.837+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:14:17.968+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:14:17.967+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:14:18.091+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:14:18.091+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:14:18.292+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.585 seconds
[2022-12-12T15:14:28.657+0000] {processor.py:154} INFO - Started process (PID=4303) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:14:28.684+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:14:28.689+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:14:28.688+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:14:28.882+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:14:29.063+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:14:29.062+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:14:29.308+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:14:29.301+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:14:29.458+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.827 seconds
[2022-12-12T15:14:39.684+0000] {processor.py:154} INFO - Started process (PID=4311) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:14:39.690+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:14:39.695+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:14:39.694+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:14:39.821+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:14:40.107+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:14:40.106+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:14:40.242+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:14:40.241+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:14:40.367+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.713 seconds
[2022-12-12T15:14:50.673+0000] {processor.py:154} INFO - Started process (PID=4321) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:14:50.719+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:14:50.727+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:14:50.726+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:14:50.983+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:14:52.045+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:14:52.044+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:14:52.192+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:14:52.191+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:14:52.299+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.651 seconds
[2022-12-12T15:15:02.845+0000] {processor.py:154} INFO - Started process (PID=4338) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:15:02.853+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:15:02.861+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:15:02.860+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:15:03.121+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:15:04.445+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:15:04.444+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:15:04.577+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:15:04.576+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:15:04.734+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.923 seconds
[2022-12-12T15:15:14.943+0000] {processor.py:154} INFO - Started process (PID=4351) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:15:14.948+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:15:14.953+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:15:14.952+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:15:15.072+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:15:15.224+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:15:15.223+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:15:15.334+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:15:15.334+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:15:15.468+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.542 seconds
[2022-12-12T15:15:25.935+0000] {processor.py:154} INFO - Started process (PID=4359) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:15:25.949+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:15:25.968+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:15:25.967+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:15:26.177+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:15:26.374+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:15:26.373+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:15:26.506+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:15:26.505+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:15:26.648+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.759 seconds
[2022-12-12T15:15:37.212+0000] {processor.py:154} INFO - Started process (PID=4369) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:15:37.263+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:15:37.275+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:15:37.274+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:15:37.543+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:15:37.789+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:15:37.788+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:15:37.962+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:15:37.961+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:15:38.127+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.006 seconds
[2022-12-12T15:15:49.077+0000] {processor.py:154} INFO - Started process (PID=4388) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:15:49.109+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:15:49.121+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:15:49.120+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:15:49.603+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:15:50.061+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:15:50.060+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:15:50.359+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:15:50.358+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:15:50.567+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.668 seconds
[2022-12-12T15:16:01.206+0000] {processor.py:154} INFO - Started process (PID=4398) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:16:01.234+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:16:01.245+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:16:01.244+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:16:01.693+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:16:01.863+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:16:01.862+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:16:02.035+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:16:02.033+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:16:02.236+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.132 seconds
[2022-12-12T15:16:12.538+0000] {processor.py:154} INFO - Started process (PID=4408) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:16:12.551+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:16:12.556+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:16:12.555+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:16:12.854+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:16:13.713+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T15:16:24.156+0000] {processor.py:154} INFO - Started process (PID=4422) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:16:24.183+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:16:24.188+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:16:24.186+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:16:24.420+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:16:25.012+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:16:25.011+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:16:25.636+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:16:25.634+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:16:25.804+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.683 seconds
[2022-12-12T15:16:36.032+0000] {processor.py:154} INFO - Started process (PID=4433) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:16:36.061+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:16:36.065+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:16:36.064+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:16:36.185+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:16:36.705+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:16:36.704+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:16:36.859+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:16:36.858+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:16:36.973+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.955 seconds
[2022-12-12T15:16:47.195+0000] {processor.py:154} INFO - Started process (PID=4443) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:16:47.241+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:16:47.245+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:16:47.244+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:16:47.573+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:16:47.795+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:16:47.794+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:16:48.111+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:16:48.110+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:16:48.715+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.536 seconds
[2022-12-12T15:16:59.184+0000] {processor.py:154} INFO - Started process (PID=4453) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:16:59.231+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:16:59.235+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:16:59.235+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:16:59.318+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:16:59.456+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:16:59.456+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:16:59.567+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:16:59.567+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:16:59.711+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.541 seconds
[2022-12-12T15:17:10.201+0000] {processor.py:154} INFO - Started process (PID=4471) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:17:10.271+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:17:10.276+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:17:10.274+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:17:10.550+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:17:10.892+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:17:10.891+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:17:11.376+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:17:11.375+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:17:11.753+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.577 seconds
[2022-12-12T15:17:22.108+0000] {processor.py:154} INFO - Started process (PID=4481) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:17:22.156+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:17:22.161+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:17:22.160+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:17:22.328+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:17:22.478+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:17:22.477+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:17:22.656+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:17:22.652+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:17:22.851+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.759 seconds
[2022-12-12T15:17:33.077+0000] {processor.py:154} INFO - Started process (PID=4491) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:17:33.104+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:17:33.110+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:17:33.109+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:17:33.199+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:17:34.719+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.4), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-12T15:17:45.279+0000] {processor.py:154} INFO - Started process (PID=4501) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:17:45.319+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:17:45.323+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:17:45.322+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:17:45.433+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:17:46.022+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:17:46.021+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:17:46.251+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:17:46.250+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:17:46.485+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.222 seconds
[2022-12-12T15:17:57.016+0000] {processor.py:154} INFO - Started process (PID=4519) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:17:57.041+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:17:57.046+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:17:57.045+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:17:57.233+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:17:58.317+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:17:58.316+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:17:58.843+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:17:58.830+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:17:59.161+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.170 seconds
[2022-12-12T15:18:09.644+0000] {processor.py:154} INFO - Started process (PID=4529) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:18:09.649+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:18:09.655+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:18:09.654+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:18:09.870+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:18:10.150+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:18:10.145+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:18:10.696+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:18:10.694+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:18:10.842+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.246 seconds
[2022-12-12T15:18:21.981+0000] {processor.py:154} INFO - Started process (PID=4539) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:18:22.004+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:18:22.009+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:18:22.008+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:18:22.151+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:18:22.556+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:18:22.555+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:18:22.673+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:18:22.672+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:18:22.805+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.857 seconds
[2022-12-12T15:18:33.375+0000] {processor.py:154} INFO - Started process (PID=4556) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:18:33.392+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:18:33.397+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:18:33.396+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:18:33.668+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:18:33.857+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:18:33.856+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:18:33.999+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:18:33.998+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:18:34.223+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.873 seconds
[2022-12-12T15:18:44.404+0000] {processor.py:154} INFO - Started process (PID=4566) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:18:44.446+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:18:44.451+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:18:44.450+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:18:44.565+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:18:44.968+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:18:44.967+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:18:45.112+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:18:45.111+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:18:45.315+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.927 seconds
[2022-12-12T15:18:55.863+0000] {processor.py:154} INFO - Started process (PID=4576) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:18:55.915+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:18:55.919+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:18:55.918+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:18:56.001+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:18:56.145+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:18:56.145+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:18:56.464+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:18:56.463+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:18:56.831+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.982 seconds
[2022-12-12T15:19:07.617+0000] {processor.py:154} INFO - Started process (PID=4586) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:19:07.671+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-12T15:19:07.675+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:19:07.674+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:19:07.782+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-12T15:19:08.077+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:19:08.075+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-12T15:19:08.355+0000] {logging_mixin.py:137} INFO - [2022-12-12T15:19:08.354+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-12T15:19:08.664+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.061 seconds
